This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-10-16T19:21:03.070Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
data/
  train/
    _annotations.coco.json
src/
  preprocessing/
    __init__.py
    image_scaling.py
    low_res_test.py
    noise_reduction.py
    preprocess.py
    test.py
  utils/
    __init__.py
    utils.py
    visualization.py
  __init__.py
.gitignore
readme.md
requirements.txt

================================================================
Repository Files
================================================================

================
File: data/train/_annotations.coco.json
================
{"info":{"year":"2024","version":"2","description":"Exported from roboflow.com","contributor":"","url":"https://public.roboflow.com/object-detection/undefined","date_created":"2024-10-14T17:41:31+00:00"},"licenses":[{"id":1,"url":"https://creativecommons.org/licenses/by/4.0/","name":"CC BY 4.0"}],"categories":[{"id":0,"name":"plate-paper","supercategory":"none"},{"id":1,"name":"paper","supercategory":"plate-paper"},{"id":2,"name":"paper-blue","supercategory":"plate-paper"},{"id":3,"name":"paper-yellow","supercategory":"plate-paper"},{"id":4,"name":"paper-yellow-bright","supercategory":"plate-paper"},{"id":5,"name":"plate","supercategory":"plate-paper"}],"images":[{"id":0,"license":1,"file_name":"Pairtwo_png.rf.e23749dcf6644b0a2e561634554a5009.jpg","height":480,"width":640,"date_captured":"2024-10-14T17:41:31+00:00"},{"id":1,"license":1,"file_name":"Pair3_png.rf.984a166a90eb4fb2fc2ea9a4e5a882f4.jpg","height":480,"width":640,"date_captured":"2024-10-14T17:41:31+00:00"},{"id":2,"license":1,"file_name":"Pair1_png.rf.9a41eaba847f2815f37ffd3e13598fc6.jpg","height":480,"width":640,"date_captured":"2024-10-14T17:41:31+00:00"}],"annotations":[{"id":0,"image_id":0,"category_id":5,"bbox":[154,67,344,341],"area":117304,"segmentation":[[332.2,67,309,67,270,75,237,90,215,105,199,120,176,151,161,184,154,216,154,254,159,280,171,312,191,343,217,369,252,391,293,405,322,408,354,406,388,397,420,381,439,367,462,343,481,313,494,276,498,243,494,201,485,173,473,150,454,124,430,102,396,82,375,74,338,67]],"iscrowd":0},{"id":1,"image_id":0,"category_id":3,"bbox":[201,155,128,107],"area":13696,"segmentation":[[263,155,251,164,238,162,232,166,223,165,218,168,210,185,208,212,201,222,203,229,219,248,251,255,260,262,288,260,294,257,298,248,310,236,314,233,323,233,322,223,329,220,328,215,319,206,302,198,301,189,287,186,289,168,286,163,270,155]],"iscrowd":0},{"id":2,"image_id":0,"category_id":4,"bbox":[223,259,72,59],"area":4248,"segmentation":[[243,259,231,265,223,280,223,289,235,309,253,311,263,318,271,310,280,292,291,293,295,290,294,276,288,270,288,263,273,262,268,267,245,259]],"iscrowd":0},{"id":3,"image_id":0,"category_id":1,"bbox":[346,144,132,113],"area":14916,"segmentation":[[413,144,398,148,374,166,364,165,360,179,352,186,346,201,348,217,352,224,379,222,372,237,375,248,380,252,394,253,402,257,418,255,430,247,451,240,451,227,454,223,478,207,476,198,455,183,456,169,449,166,438,168,438,158,429,147,415,144]],"iscrowd":0},{"id":4,"image_id":0,"category_id":2,"bbox":[362,259,65,69],"area":4485,"segmentation":[[396,259,368,272,364,276,362,291,372,308,390,320,399,318,398,324,404,328,414,302,426,293,427,289,417,265,413,261,405,262,402,259]],"iscrowd":0},{"id":5,"image_id":1,"category_id":5,"bbox":[164,69,349,345],"area":120405,"segmentation":[[343.25,69,301,72,276,79,249,92,221,112,200,134,179,169,169,196,164,228,166,271,174,299,186,325,207,355,237,382,260,396,288,407,324,414,364,413,400,404,426,392,453,373,480,344,495,319,509,281,513,250,509,205,500,177,484,147,463,121,434,97,407,83,380,74,349,69]],"iscrowd":0},{"id":6,"image_id":1,"category_id":1,"bbox":[359,147,134,115],"area":15410,"segmentation":[[433,147,413,150,387,169,378,167,373,173,373,181,361,195,359,214,363,227,379,228,386,224,394,226,385,240,386,250,393,256,419,262,463,246,467,243,465,237,467,230,477,220,493,210,492,203,484,194,471,187,471,172,465,169,453,171,451,158,438,147]],"iscrowd":0},{"id":7,"image_id":1,"category_id":2,"bbox":[373,263,67,71],"area":4757,"segmentation":[[410,263,381,276,376,282,377,285,373,295,380,303,383,313,396,319,402,325,411,325,412,332,418,334,418,329,424,322,425,313,428,307,440,296,440,292,436,285,431,270,427,266,421,266,414,263]],"iscrowd":0},{"id":8,"image_id":1,"category_id":3,"bbox":[212,159,129,108],"area":13932,"segmentation":[[273,159,263,168,248,166,244,169,233,169,228,172,220,192,218,219,212,224,212,231,229,253,261,259,271,267,300,265,309,258,311,252,323,239,327,236,334,236,333,229,335,226,341,225,341,221,331,210,313,202,314,195,312,192,306,193,298,189,300,187,300,170,294,164,282,159]],"iscrowd":0},{"id":9,"image_id":1,"category_id":4,"bbox":[233,263,74,60],"area":4440,"segmentation":[[254,263,241,272,233,289,234,294,242,304,244,312,246,314,255,316,265,316,275,323,277,319,282,316,290,297,293,296,301,298,305,296,307,286,304,278,299,275,301,270,298,267,286,266,275,269,260,265,258,263,255,264]],"iscrowd":0},{"id":10,"image_id":2,"category_id":5,"bbox":[163,87,332,333],"area":110556,"segmentation":[[331,87,301,89,269,98,240,113,221,127,195,155,179,182,169,207,163,246,164,273,172,308,189,344,208,368,228,386,260,405,293,416,324,420,353,418,386,410,415,396,443,375,470,343,481,322,490,299,495,270,495,236,489,205,477,176,457,146,428,119,399,102,363,90,336,87]],"iscrowd":0},{"id":11,"image_id":2,"category_id":3,"bbox":[211,174,122,101],"area":12322,"segmentation":[[284,174,272,180,261,175,236,177,223,199,220,220,211,230,226,258,249,263,262,275,293,275,310,261,313,254,326,254,327,247,333,244,325,230,310,223,313,212,301,205,301,184,285,174]],"iscrowd":0},{"id":12,"image_id":2,"category_id":4,"bbox":[221,267,69,57],"area":3933,"segmentation":[[243.5,267,234,269,221,288,229,312,246,316,256,324,268,314,276,302,287,304,290,291,285,285,286,279,258,276,255,270,249,267]],"iscrowd":0},{"id":13,"image_id":2,"category_id":1,"bbox":[351,190,127,109],"area":13843,"segmentation":[[418,190,400,196,391,206,374,204,370,212,354,226,351,235,353,252,362,255,376,254,380,257,371,267,372,276,378,287,400,299,420,294,441,293,444,291,444,281,449,276,478,261,477,251,461,235,462,224,458,218,447,220,448,207,442,197,421,190]],"iscrowd":0},{"id":14,"image_id":2,"category_id":2,"bbox":[350,286,60,63],"area":3780,"segmentation":[[367,286,352,298,353,310,350,314,359,335,366,340,363,343,364,349,377,344,385,336,404,333,410,314,408,301,396,301,386,292,369,286]],"iscrowd":0}]}

================
File: src/preprocessing/__init__.py
================
from .image_scaling import upscale_depth, align_segmentation_mask
from .noise_reduction import reduce_depth_noise

================
File: src/preprocessing/image_scaling.py
================
import cv2
import numpy as np

""" 
    The goal of this file is to match the resolution of the RGB Image
    since the RGB Depth Image is in a lower resolution
"""

def upscale_depth(rgbd_image, target_shape):
    """
        
        args:
        rgbd_image (numpy.ndarray): RGBD image with depth as the 4th channel
        target_shape (tuple): Desired output shape (height, width)
        
        returns:
        numpy.ndarray: RGBD image with upscaled depth
    
    """
    if rgbd_image.shape[2] != 4:
        raise ValueError("Expected RGBD image with 4 channels")
    
    h, w = rgbd_image.shape[:2]
    target_h, target_w = target_shape

    # calculate scaling factor while maintaining aspect ratio for the image upscaling not the scaling factor for th pinhole
    scale = min(target_w / w, target_h / h)
    new_w, new_h = int(w * scale), int(h * scale)

    # depth channels 
    rgb_resized = cv2.resize(rgbd_image[:,:,:3], (new_w, new_h), interpolation=cv2.INTER_LINEAR)
    depth_resized = cv2.resize(rgbd_image[:,:,3], (new_w, new_h), interpolation=cv2.INTER_NEAREST)

    # create a new upscaled image 
    upscaled_rgbd = np.zeros((*target_shape, 4), dtype=rgbd_image.dtype)
    
    #  padding
    pad_w = (target_w - new_w) // 2
    pad_h = (target_h - new_h) // 2

    # put to center para maangas
    upscaled_rgbd[pad_h:pad_h+new_h, pad_w:pad_w+new_w, :3] = rgb_resized
    upscaled_rgbd[pad_h:pad_h+new_h, pad_w:pad_w+new_w, 3] = depth_resized

    return upscaled_rgbd
def align_segmentation_mask(mask, rgbd_shape):

    """
        args:
        mask (numpy.ndarray): Segmentation mask
        rgbd_shape (tuple): shape of rgbd image


        return:
        numpy_ndarry: Aligned Segmentation mask
    """
    
    if mask.shape[:2] != rgbd_shape[:2]:
        return cv2.resize(mask,(rgbd_shape[1],rgbd_shape[0]), interpolation=cv2.INTER_NEAREST)


    return mask

================
File: src/preprocessing/low_res_test.py
================
# src/preprocessing/test_low_res_rgbd.py

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
from src.utils.utils import load_rgbd_image, get_corresponding_rgbd_filename
from src.preprocessing.noise_reduction import reduce_depth_noise

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
def visualize_rgbd(rgbd_image, title):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
    
    #  RGB real image
    ax1.imshow(rgbd_image[:,:,:3])
    ax1.set_title("RGB")
    ax1.axis('off')
    
    # Depth factors
    depth_vis = ax2.imshow(rgbd_image[:,:,3], cmap='jet')
    ax2.set_title("Depth")
    ax2.axis('off')
    plt.colorbar(depth_vis, ax=ax2, label='Depth')
    
    plt.suptitle(title)
    plt.tight_layout()
    return fig

def process_low_res_rgbd(rgb_filename, output_dir):
    # get corresponding RGBD filename
    rgbd_filename = get_corresponding_rgbd_filename(rgb_filename)
    if rgbd_filename is None:
        raise ValueError(f"No corresponding RGBD filename found for {rgb_filename}")

    # load low-resolution RGBD image
    data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'data'))
    image_input_dir = os.path.join(data_dir, 'image_input')
    rgbd_path = os.path.join(image_input_dir, rgbd_filename)
    original_rgbd = load_rgbd_image(rgbd_path)

    print(f"Loaded RGBD image shape: {original_rgbd.shape}")
    print(f"Depth channel min: {original_rgbd[:,:,3].min()}, max: {original_rgbd[:,:,3].max()}")
    print(f"Unique depth values: {np.unique(original_rgbd[:,:,3])}")

    #vbisualize original RGBD
    fig_original = visualize_rgbd(original_rgbd, "Original Low-res RGBD")
    fig_original.savefig(os.path.join(output_dir, f"{os.path.splitext(rgbd_filename)[0]}_original.png"))
    plt.close(fig_original)

    # Apply noise reduction to depth channel
    depth = original_rgbd[:,:,3].astype(np.float32)
    noise_reduced_depth = reduce_depth_noise(depth)

    print(f"Noise reduced depth min: {noise_reduced_depth.min()}, max: {noise_reduced_depth.max()}")

    # create noise-reduced RGBD image
    noise_reduced_rgbd = original_rgbd.copy()
    noise_reduced_rgbd[:,:,3] = noise_reduced_depth

    # Visualize noise-reduced RGBD
    fig_reduced = visualize_rgbd(noise_reduced_rgbd, "Noise-reduced Low-res RGBD")
    fig_reduced.savefig(os.path.join(output_dir, f"{os.path.splitext(rgbd_filename)[0]}_noise_reduced.png"))
    plt.close(fig_reduced)

    return original_rgbd, noise_reduced_rgbd

def main():
    output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'output', 'low_res_test'))
    os.makedirs(output_dir, exist_ok=True)

    rgb_filenames = [
        'Pair1_png.rf.9a41eaba847f2815f37ffd3e13598fc6.jpg',
        'Pairtwo_png.rf.e23749dcf6644b0a2e561634554a5009.jpg',
        'Pair3_png.rf.984a166a90eb4fb2fc2ea9a4e5a882f4.jpg'
    ]

    for rgb_filename in rgb_filenames:
        print(f"Processing {rgb_filename}")
        original_rgbd, noise_reduced_rgbd = process_low_res_rgbd(rgb_filename, output_dir)
        print("---")

if __name__ == "__main__":
    main()

================
File: src/preprocessing/noise_reduction.py
================
import cv2
import numpy as np


def reduce_depth_noise(depth_image, method='bilateral'):
    """
        args:
        depth_image (numpy.ndarray): depth depth_image
        method (str) : noise reduction method ('bilateral' or 'median')

        return:
        numpy.ndarray: Noise reduced dpeth depth_image

    """
    print(f"Noise reduction input depth min: {depth_image.min()}, max: {depth_image.max()}")
    
    if method == 'bilateral':
        result = cv2.bilateralFilter(depth_image, 9, 75, 75)
    elif method == 'median':
        result = cv2.medianBlur(depth_image, 5)
    else:
        raise ValueError('Unsupported noise reduction method')
    
    print(f"Noise reduction output depth min: {result.min()}, max: {result.max()}")
    return result

================
File: src/preprocessing/preprocess.py
================
import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
from src.utils.utils import (
    load_rgbd_image,
    load_coco_data,
    create_segmentation_mask,
    get_corresponding_rgbd_filename,
    ensure_directory_exists
)
from src.preprocessing.image_scaling import align_segmentation_mask
from src.utils.visualization import visualize_preprocessing_steps

class PreprocessingPipeline:
    def __init__(self, data_dir, output_dir):
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.image_input_dir = os.path.join(data_dir, 'image_input')
        self.train_dir = os.path.join(data_dir, 'train')
        self.segmentation_file = os.path.join(data_dir, 'train', '_annotations.coco.json')
        
        ensure_directory_exists(output_dir)
        self.processed_dir = os.path.join(output_dir, 'processed')
        ensure_directory_exists(self.processed_dir)
        
        #load data
        self.coco_data = load_coco_data(self.segmentation_file)

        # creat mapping of category
        self.category_id_to_name = {cat['id']: cat['name'] for cat in self.coco_data['categories']}
        # Camera setup constants
        self.camera_height = 33  
        self.plate_height = 1.5 
        self.plate_depth = 0.7

    def color_to_depth(self, color_image):
        """
        Convert color information to estimated depth values.
        
        Args:
        color_image (numpy.ndarray): RGB image

        Returns:
        numpy.ndarray: Estimated depth values (in cm)
        """
        # Convert to grayscale
        gray = cv2.cvtColor(color_image, cv2.COLOR_RGB2GRAY)
        
        # Normalize to 0-1 range
        normalized = gray.astype(float) / 255.0
        
        # Invert so that darker colors are further away
        inverted = 1 - normalized
        
        # Scale depth values based on camera setup
        # Assume the brightest points (now 0) are at plate level, and darkest (now 1) are at max depth
        max_depth = self.camera_height - self.plate_height
        depth = inverted * max_depth + self.plate_height
        
        return depth

    def calibrate_depth(self, depth, segmentation_mask, plate_id=5):
        """
        Calibrate depth using the plate as a reference.
        
        Args:
        depth (numpy.ndarray): Estimated depth values
        segmentation_mask (numpy.ndarray): Segmentation mask
        plate_id (int): ID of the plate in the segmentation mask

        Returns:
        numpy.ndarray: Calibrated depth values
        """
        plate_mask = segmentation_mask == plate_id
        plate_depth = depth[plate_mask]
        
        if plate_depth.size == 0:
            print("Warning: No plate found in the image for calibration.")
            return depth
        
        plate_avg_depth = np.mean(plate_depth)
        expected_plate_depth = self.plate_height + self.plate_depth / 2  # Average depth of the plate
        
        # Adjust depth so that the plate has the expected depth
        calibration_factor = expected_plate_depth / plate_avg_depth
        calibrated_depth = depth * calibration_factor
        
        return calibrated_depth
    def upscale_with_padding(self, image, target_shape):
        """
        Upscale the image to target shape while maintaining aspect ratio and using padding.
        
        Args:
        image (numpy.ndarray): Input image to upscale
        target_shape (tuple): Target shape (height, width)

        Returns:
        numpy.ndarray: Upscaled and padded image
        """
        h, w = image.shape[:2]
        target_h, target_w = target_shape

        # Calculate scaling factor
        scale = min(target_h / h, target_w / w)
        
        # Calculate new dimensions
        new_h, new_w = int(h * scale), int(w * scale)

        # Resize the image
        resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)

        # Create a black canvas of the target shape
        if len(image.shape) == 3:
            padded = np.zeros((target_h, target_w, image.shape[2]), dtype=resized.dtype)
        else:
            padded = np.zeros((target_h, target_w), dtype=resized.dtype)

        # Compute padding
        pad_h = (target_h - new_h) // 2
        pad_w = (target_w - new_w) // 2

        # Place the resized image on the canvas
        padded[pad_h:pad_h+new_h, pad_w:pad_w+new_w] = resized

        return padded
    def process_image(self, rgb_filename, image_id):
        try:
            # Load high-resolution RGB image
            rgb_path = os.path.join(self.train_dir, rgb_filename)
            rgb_image = cv2.imread(rgb_path)
            if rgb_image is None:
                raise FileNotFoundError(f"Could not load RGB image at {rgb_path}")
            rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)
            print(f"Loaded RGB image shape: {rgb_image.shape}")

            # Get corresponding RGBD filename
            rgbd_filename = get_corresponding_rgbd_filename(rgb_filename)
            if rgbd_filename is None:
                raise ValueError(f"No corresponding RGBD filename found for {rgb_filename}")

            # Load RGBD image (which is actually just RGB in this case)
            rgbd_path = os.path.join(self.image_input_dir, rgbd_filename)
            rgbd_image = load_rgbd_image(rgbd_path)
            print(f"Loaded RGBD image shape: {rgbd_image.shape}")

            # Estimate depth from color
            estimated_depth = self.color_to_depth(rgbd_image[:,:,:3])
            print(f"Estimated depth shape: {estimated_depth.shape}")

            # Upscale estimated depth to match RGB resolution
            upscaled_depth = self.upscale_with_padding(estimated_depth, rgb_image.shape[:2])
            print(f"Upscaled depth shape: {upscaled_depth.shape}")


            # Load and create segmentation mask
            
            coco_data = load_coco_data(self.segmentation_file)
            segmentation_mask = create_segmentation_mask(image_id, coco_data)
            segmentation_mask = align_segmentation_mask(segmentation_mask, rgb_image.shape[:2])
            print(f"Segmentation mask shape: {segmentation_mask.shape}")
            print(f"Unique values in segmentation mask: {np.unique(segmentation_mask)}")
            # Calibrate depth using the plate
            calibrated_depth = self.calibrate_depth(upscaled_depth, segmentation_mask)

            # Extract depth for segmented objects
            segmented_data = {}
            unique_objects = np.unique(segmentation_mask)
            for obj_id in unique_objects:
                if obj_id == 0:  # Assuming 0 is background
                    continue
                object_mask = segmentation_mask == obj_id
                
                # Extract RGB data for the object
                object_rgb = rgb_image.copy()
                object_rgb[~object_mask] = 0
                
                # Extract depth data for the object
                object_depth = calibrated_depth.copy()
                object_depth[~object_mask] = np.nan
                
                # Get object name from COCO categories
                object_name = self.category_id_to_name.get(obj_id, f"unknown_object_{obj_id}")
                
                # Save object RGB and depth data as PNG
                output_filename = os.path.splitext(rgb_filename)[0]
                rgb_filename = f"{output_filename}_{object_name}_rgb.png"
                depth_filename = f"{output_filename}_{object_name}_depth.png"
                
                # Save RGB as PNG
                cv2.imwrite(os.path.join(self.processed_dir, rgb_filename), cv2.cvtColor(object_rgb, cv2.COLOR_RGB2BGR))
                
                # Save depth as PNG using matplotlib
                plt.figure(figsize=(10, 10))
                plt.imshow(object_depth, cmap='viridis')
                plt.axis('off')
                plt.title(f"{object_name} Depth")
                plt.colorbar(label='Depth (cm)')
                plt.savefig(os.path.join(self.processed_dir, depth_filename), bbox_inches='tight', pad_inches=0)
                plt.close()
                
                print(f"Saved RGB and depth data for {object_name} (ID: {obj_id})")
                
                segmented_data[obj_id] = {
                    'rgb': object_rgb, 
                    'depth': object_depth, 
                    'name': object_name
                }

            # Save full RGB, depth, and segmentation mask

            output_filename = os.path.splitext(rgb_filename)[0]
            cv2.imwrite(os.path.join(self.processed_dir, f"{output_filename}_full_rgb.png"), cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))
            plt.imsave(os.path.join(self.processed_dir, f"{output_filename}_full_depth.png"), calibrated_depth, cmap='viridis')
            plt.imsave(os.path.join(self.processed_dir, f"{output_filename}_segmentation.png"), segmentation_mask, cmap='tab10')

            # Visualize results with a unique filename for each image
            vis_filename = f"{output_filename}_visualization.png"
            visualize_preprocessing_steps(rgb_image, rgbd_image, estimated_depth, calibrated_depth, segmentation_mask, segmented_data, self.output_dir, vis_filename)
            print(f"Saved preprocessing visualization to: {os.path.join(self.output_dir, vis_filename)}")

            return rgb_image, calibrated_depth, segmentation_mask, segmented_data
        except Exception as e:
            print(f"Error processing image {rgb_filename}: {str(e)}")
            raise

    def run(self):
        rgb_filenames = [
            'Pair1_png.rf.9a41eaba847f2815f37ffd3e13598fc6.jpg',
            'Pairtwo_png.rf.e23749dcf6644b0a2e561634554a5009.jpg',
            'Pair3_png.rf.984a166a90eb4fb2fc2ea9a4e5a882f4.jpg'
        ]
        image_ids = [2, 0, 1]  # Corresponding image IDs in the COCO dataset

        for rgb_filename, image_id in zip(rgb_filenames, image_ids):
            print(f"Processing {rgb_filename}")
            rgb, depth, mask, segmented_depths = self.process_image(rgb_filename, image_id)
            print(f"Processed {rgb_filename}.")
            print(f"RGB shape: {rgb.shape}, Depth shape: {depth.shape}, Mask shape: {mask.shape}")
            print(f"Number of segmented objects: {len(segmented_depths)}")
            print(f"Saved processed data to {self.processed_dir}")
            print("---")

================
File: src/preprocessing/test.py
================
# src/preprocessing/test.py

import os
import sys
import numpy as np
import cv2

# Add the project root directory to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from src.preprocessing.preprocess import PreprocessingPipeline
from src.utils.utils import load_rgbd_image, create_segmentation_mask, load_coco_data

def test_load_rgbd_image(pipeline):
    rgbd_filename = 'Pairone.png'
    rgbd_image = load_rgbd_image(os.path.join(pipeline.image_input_dir, rgbd_filename))
    assert rgbd_image.shape[2] == 4, f"Expected 4-channel RGBD image, got {rgbd_image.shape[2]} channels"
    print(f"RGBD image shape: {rgbd_image.shape}")
    print(f"RGB min: {rgbd_image[:,:,:3].min()}, max: {rgbd_image[:,:,:3].max()}")
    print(f"Depth min: {rgbd_image[:,:,3].min()}, max: {rgbd_image[:,:,3].max()}")
    print(f"Unique depth values: {np.unique(rgbd_image[:,:,3])}")

def test_create_segmentation_mask(pipeline):
    coco_data = load_coco_data(pipeline.segmentation_file)
    mask = create_segmentation_mask(2, coco_data)  # Using image_id 2 for 'Pair1_png'
    assert mask.shape == (480, 640), f"Expected mask shape (480, 640), got {mask.shape}"
    print(f"Segmentation mask shape: {mask.shape}")
    print(f"Unique mask values: {np.unique(mask)}")

def test_preprocessing_pipeline():
    data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'data'))
    output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'output'))
    pipeline = PreprocessingPipeline(data_dir, output_dir)

    print("Running full preprocessing pipeline:")
    pipeline.run()

    # Check if output files are created
    rgb_filenames = [
        'Pair1_png.rf.9a41eaba847f2815f37ffd3e13598fc6.jpg',
        'Pairtwo_png.rf.e23749dcf6644b0a2e561634554a5009.jpg',
        'Pair3_png.rf.984a166a90eb4fb2fc2ea9a4e5a882f4.jpg'
    ]
    
    for rgb_filename in rgb_filenames:
        output_filename = os.path.splitext(rgb_filename)[0]
        depth_path = os.path.join(output_dir, 'processed', f'{output_filename}_depth.npy')
        assert os.path.exists(depth_path), f"Depth file not found: {depth_path}"
        
    visualization_path = os.path.join(output_dir, 'preprocessing_visualization.png')
    assert os.path.exists(visualization_path), f"Visualization file not found: {visualization_path}"

    print("All tests passed successfully!")

if __name__ == "__main__":
    test_preprocessing_pipeline()

================
File: src/utils/__init__.py
================
# src/utils/__init__.py

from .visualization import visualize_preprocessing_steps, overlay_segmentation_on_rgbd
from .utils import (
    load_rgbd_image,
    load_coco_data,
    create_segmentation_mask,
    get_corresponding_rgbd_filename,
    ensure_directory_exists,
    align_segmentation_mask
)

================
File: src/utils/utils.py
================
import os
import json
import cv2
import numpy as np

def load_rgbd_image(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)
    if img is None:
        raise FileNotFoundError(f"Could not load image at {image_path}")
    if img.shape[2] != 4:
        raise ValueError(f"Expected 4-channel RGBD image, got {img.shape[2]} channels")
    
    print(f"Loaded RGBD image shape: {img.shape}")
    print(f"Depth channel min: {img[:,:,3].min()}, max: {img[:,:,3].max()}")
    print(f"Unique depth values: {np.unique(img[:,:,3])}")
    
    return img

def load_coco_data(json_file):
    with open(json_file, 'r') as f:
        return json.load(f)


def create_segmentation_mask(image_id, coco_data):
    image_info = next(img for img in coco_data['images'] if img['id'] == image_id)
    mask = np.zeros((image_info['height'], image_info['width']), dtype=np.uint8)
    
    annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_id]
    
    for ann in annotations:
        category_id = ann['category_id']
        for segmentation in ann['segmentation']:
            pts = np.array(segmentation).reshape((-1, 2)).astype(np.int32)
            cv2.fillPoly(mask, [pts], color=category_id)
    
    print(f"Number of annotations for image {image_id}: {len(annotations)}")
    print(f"Unique category IDs in mask: {np.unique(mask)}")
    
    return mask
def get_corresponding_rgbd_filename(rgb_filename):
    mapping = {
        'Pair1_png.rf.9a41eaba847f2815f37ffd3e13598fc6.jpg': 'Pairone.png',
        'Pairtwo_png.rf.e23749dcf6644b0a2e561634554a5009.jpg': 'Pairtwo.png',
        'Pair3_png.rf.984a166a90eb4fb2fc2ea9a4e5a882f4.jpg': 'Pairthree.png'
    }
    return mapping.get(rgb_filename)

def ensure_directory_exists(directory):
    """Create directory if it doesn't exist."""
    os.makedirs(directory, exist_ok=True)

def align_segmentation_mask(mask, target_shape):
    """Resize segmentation mask to match target shape."""
    if mask.shape[:2] != target_shape:
        return cv2.resize(mask, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_NEAREST)
    return mask

def save_depth_image(depth_data, output_path, as_uint8=True):
    """
    Save depth data as an image file.
    
    Args:
    depth_data (numpy.ndarray): 2D array of depth values
    output_path (str): Path to save the image
    as_uint8 (bool): If True, convert depth to uint8 before saving
    """
    if as_uint8:
        # normalizedepth to 0 255 range
        depth_min = np.min(depth_data)
        depth_max = np.max(depth_data)
        if depth_min != depth_max:
            depth_normalized = ((depth_data - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)
        else:
            depth_normalized = np.zeros_like(depth_data, dtype=np.uint8)
    else:
        depth_normalized = depth_data

    cv2.imwrite(output_path, depth_normalized)

def load_depth_image(input_path, as_float=True):
    """
    Load a depth image file.
    
    Args:
    input_path (str): Path to the depth image file
    as_float (bool): If True, convert depth to float32
    
    Returns:
    numpy.ndarray: 2D array of depth values
    """
    depth_image = cv2.imread(input_path, cv2.IMREAD_UNCHANGED)
    
    if depth_image is None:
        raise FileNotFoundError(f"Could not load depth image at {input_path}")
    
    if as_float:
        depth_image = depth_image.astype(np.float32) / 255.0
    
    return depth_image

def visualize_depth(depth_image, output_path):
    """
    Create a color visualization of a depth image and save it.
    
    Args:
    depth_image (numpy.ndarray): 2D array of depth values
    output_path (str): Path to save the visualization
    """
    plt.figure(figsize=(10, 8))
    plt.imshow(depth_image, cmap='viridis')
    plt.colorbar(label='Depth')
    plt.title('Depth Visualization')
    plt.axis('off')
    plt.savefig(output_path)
    plt.close()

================
File: src/utils/visualization.py
================
# src/utils/visualization.py

import matplotlib
matplotlib.use('Agg')  # set Agg
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os

def visualize_depth(depth_image):
    depth_min, depth_max = np.min(depth_image), np.max(depth_image)
    if depth_min != depth_max:
        normalized_depth = (depth_image - depth_min) / (depth_max - depth_min)
    else:
        normalized_depth = np.zeros_like(depth_image)
    return plt.cm.viridis(normalized_depth)
def visualize_preprocessing_steps(rgb_image, rgbd_image, estimated_depth, calibrated_depth, segmentation_mask, segmented_data, output_dir, filename):
    num_objects = len(segmented_data)
    rows = 4 + num_objects
    fig, axs = plt.subplots(rows, 3, figsize=(20, 7 * rows))
    fig.suptitle('Preprocessing Pipeline Visualization', fontsize=16)

    # Ensure axs is always a 2D array
    if rows == 1:
        axs = axs.reshape(1, -1)

    # First row: RGB and RGBD
    axs[0, 0].imshow(rgb_image)
    axs[0, 0].set_title('High-res RGB')
    axs[0, 0].axis('off')

    axs[0, 1].imshow(rgbd_image[:,:,:3])
    axs[0, 1].set_title('Original RGBD (color)')
    axs[0, 1].axis('off')

    axs[0, 2].imshow(segmentation_mask, cmap='tab10')
    axs[0, 2].set_title('Segmentation Mask')
    axs[0, 2].axis('off')

    # Second row: Depth visualizations
    depth_vmin, depth_vmax = np.nanmin(estimated_depth), np.nanmax(estimated_depth)
    
    im_estimated = axs[1, 0].imshow(estimated_depth, cmap='viridis', vmin=depth_vmin, vmax=depth_vmax)
    axs[1, 0].set_title('Estimated Depth (low-res)')
    axs[1, 0].axis('off')
    plt.colorbar(im_estimated, ax=axs[1, 0], label='Depth (cm)')

    im_calibrated = axs[1, 1].imshow(calibrated_depth, cmap='viridis', vmin=depth_vmin, vmax=depth_vmax)
    axs[1, 1].set_title('Calibrated Depth (high-res)')
    axs[1, 1].axis('off')
    plt.colorbar(im_calibrated, ax=axs[1, 1], label='Depth (cm)')

    # Combined segmented depth
    combined_segmented_depth = np.zeros_like(calibrated_depth)
    for obj_data in segmented_data.values():
        combined_segmented_depth = np.nansum([combined_segmented_depth, obj_data['depth']], axis=0)
    im_segmented = axs[1, 2].imshow(combined_segmented_depth, cmap='viridis', vmin=depth_vmin, vmax=depth_vmax)
    axs[1, 2].set_title('Segmented Depths')
    axs[1, 2].axis('off')
    plt.colorbar(im_segmented, ax=axs[1, 2], label='Depth (cm)')

    # Third row: Depth histograms
    axs[2, 0].hist(estimated_depth.ravel(), bins=50, range=(depth_vmin, depth_vmax))
    axs[2, 0].set_title('Estimated Depth Histogram')
    axs[2, 0].set_xlabel('Depth (cm)')
    axs[2, 0].set_ylabel('Frequency')

    axs[2, 1].hist(calibrated_depth.ravel(), bins=50, range=(depth_vmin, depth_vmax))
    axs[2, 1].set_title('Calibrated Depth Histogram')
    axs[2, 1].set_xlabel('Depth (cm)')
    axs[2, 1].set_ylabel('Frequency')

    axs[2, 2].hist(combined_segmented_depth.ravel(), bins=50, range=(depth_vmin, depth_vmax))
    axs[2, 2].set_title('Segmented Depth Histogram')
    axs[2, 2].set_xlabel('Depth (cm)')
    axs[2, 2].set_ylabel('Frequency')

    # Fourth row: Depth difference visualizations
    depth_diff = calibrated_depth - estimated_depth
    im_diff = axs[3, 0].imshow(depth_diff, cmap='RdBu', norm=matplotlib.colors.CenteredNorm())
    axs[3, 0].set_title('Depth Difference (Calibrated - Estimated)')
    axs[3, 0].axis('off')
    plt.colorbar(im_diff, ax=axs[3, 0], label='Difference (cm)')

    axs[3, 1].hist(depth_diff.ravel(), bins=50)
    axs[3, 1].set_title('Depth Difference Histogram')
    axs[3, 1].set_xlabel('Difference (cm)')
    axs[3, 1].set_ylabel('Frequency')

    # Leave the last subplot of the fourth row empty for now
    axs[3, 2].axis('off')

    # Individual object RGB and depth
    for i, (obj_id, obj_data) in enumerate(segmented_data.items()):
        row = 4 + i
        
        # RGB part
        axs[row, 0].imshow(obj_data['rgb'])
        axs[row, 0].set_title(f'{obj_data["name"]} RGB')
        axs[row, 0].axis('off')
        
        # Depth part
        im_depth = axs[row, 1].imshow(obj_data['depth'], cmap='viridis', vmin=depth_vmin, vmax=depth_vmax)
        axs[row, 1].set_title(f'{obj_data["name"]} Depth')
        axs[row, 1].axis('off')
        plt.colorbar(im_depth, ax=axs[row, 1], label='Depth (cm)')
        
        # Segmentation mask for this object
        object_mask = (segmentation_mask == obj_id).astype(float)
        axs[row, 2].imshow(object_mask, cmap='gray')
        axs[row, 2].set_title(f'{obj_data["name"]} Mask')
        axs[row, 2].axis('off')

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')
    plt.close(fig)
def overlay_segmentation_on_rgbd(upscaled_rgbd, segmentation_mask):
    # Resize segmentation mask to match upscaled RGBD if necessary
    if upscaled_rgbd.shape[:2] != segmentation_mask.shape:
        segmentation_mask = cv2.resize(segmentation_mask, (upscaled_rgbd.shape[1], upscaled_rgbd.shape[0]), 
                                       interpolation=cv2.INTER_NEAREST)
    
    # Create a color map for segmentation
    cmap = plt.get_cmap('tab10')
    seg_colors = cmap(segmentation_mask / np.max(segmentation_mask))[:,:,:3]
    
    # Create a blended image
    alpha = 0.3  # Adjust this for segmentation transparency
    blended = upscaled_rgbd[:,:,:3] * (1-alpha) + seg_colors * alpha * 255
    
    # Add the depth channel back if it exists
    if upscaled_rgbd.shape[2] == 4:
        return np.dstack((blended, upscaled_rgbd[:,:,3]))
    else:
        return blended

================
File: src/__init__.py
================
from . import preprocessing

================
File: .gitignore
================
venv/

================
File: readme.md
================
# OKAY BITCHES TIME TO DEV THIS SHIT SORRY I'M JUST DOING THIS NOW
## date is October 14, 2023

so now first I'll be trying to develop the loading of data
- get the segmentation points 
- get the RGB depth image
  - needs to be upscaled so that the segmentation points match
- overlay the segmentation points to the depth image

### Cloud Plotting
- do the cloud Plotting

-- refer to the notes --


1. Problem Overview:
   We're working on estimating the volume of objects (in this case, crumpled paper balls) on a plate using a combination of RGB and depth imaging. The challenge is that we have a high-resolution RGB image but only a noisy, low-resolution depth image.

2. Data Sources:
   - High-resolution RGB image of a plate with crumpled paper balls
   - Low-resolution, noisy depth image of the same scene

3. Proposed Approach:
   a) Segmentation:
      - Use Mask R-CNN (or another segmentation method) on the RGB image to precisely identify the plate and paper balls.
   
   b) Depth Image Processing:
      - Scale the low-resolution depth image to match the RGB image's resolution.
      - Apply noise reduction techniques to improve depth data quality.
   
   c) Data Fusion:
      - Overlay the segmentation masks from the RGB image onto the scaled depth image.
      - Extract depth values only for pixels corresponding to the segmented objects.

   d) 3D Reconstruction:
      - Use the camera's intrinsic parameters (from the pinhole camera model) to convert 2D pixel coordinates and depth values into 3D points.
      - The known plate diameter serves as a reference for real-world scaling.

   e) Volume Estimation:
      - Use methods like Convex Hull or more advanced techniques to estimate the volume of the reconstructed 3D points for each paper ball.

4. Key Considerations:
   - Careful alignment of RGB and depth data
   - Handling noise and inaccuracies in the depth data
   - Accounting for potential loss of fine details due to initial low depth resolution
   - Calibration and real-world scaling using the plate as a reference object

5. Potential Enhancements:
   - Multi-view analysis if multiple images are available
   - Implementing more sophisticated depth refinement techniques
   - Exploring machine learning approaches for improved depth estimation

6. Validation:
   - Compare results with ground truth volumes (if available)
   - Analyze performance across different object shapes and sizes

================
File: requirements.txt
================
numpy==2.1.2
opencv-python==4.10.0.84
pandas==2.2.3
python-dateutil==2.9.0.post0
pytz==2024.2
six==1.16.0
tzdata==2024.2
