This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-10-21T12:56:32.546Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
data/
  train/
    _annotations.coco.json
src/
  preprocessing/
    __init__.py
    image_scaling.py
    low_res_test.py
    noise_reduction.py
    preprocess.py
    test.py
  utils/
    __init__.py
    utils.py
    visualization.py
  __init__.py
.gitignore
readme.md
requirements.txt

================================================================
Repository Files
================================================================

================
File: data/train/_annotations.coco.json
================
{"info":{"year":"2024","version":"2","description":"Exported from roboflow.com","contributor":"","url":"https://public.roboflow.com/object-detection/undefined","date_created":"2024-10-14T17:41:31+00:00"},"licenses":[{"id":1,"url":"https://creativecommons.org/licenses/by/4.0/","name":"CC BY 4.0"}],"categories":[{"id":0,"name":"plate-paper","supercategory":"none"},{"id":1,"name":"paper","supercategory":"plate-paper"},{"id":2,"name":"paper-blue","supercategory":"plate-paper"},{"id":3,"name":"paper-yellow","supercategory":"plate-paper"},{"id":4,"name":"paper-yellow-bright","supercategory":"plate-paper"},{"id":5,"name":"plate","supercategory":"plate-paper"}],"images":[{"id":0,"license":1,"file_name":"Pairtwo_png.rf.e23749dcf6644b0a2e561634554a5009.jpg","height":480,"width":640,"date_captured":"2024-10-14T17:41:31+00:00"},{"id":1,"license":1,"file_name":"Pair3_png.rf.984a166a90eb4fb2fc2ea9a4e5a882f4.jpg","height":480,"width":640,"date_captured":"2024-10-14T17:41:31+00:00"},{"id":2,"license":1,"file_name":"Pair1_png.rf.9a41eaba847f2815f37ffd3e13598fc6.jpg","height":480,"width":640,"date_captured":"2024-10-14T17:41:31+00:00"}],"annotations":[{"id":0,"image_id":0,"category_id":5,"bbox":[154,67,344,341],"area":117304,"segmentation":[[332.2,67,309,67,270,75,237,90,215,105,199,120,176,151,161,184,154,216,154,254,159,280,171,312,191,343,217,369,252,391,293,405,322,408,354,406,388,397,420,381,439,367,462,343,481,313,494,276,498,243,494,201,485,173,473,150,454,124,430,102,396,82,375,74,338,67]],"iscrowd":0},{"id":1,"image_id":0,"category_id":3,"bbox":[201,155,128,107],"area":13696,"segmentation":[[263,155,251,164,238,162,232,166,223,165,218,168,210,185,208,212,201,222,203,229,219,248,251,255,260,262,288,260,294,257,298,248,310,236,314,233,323,233,322,223,329,220,328,215,319,206,302,198,301,189,287,186,289,168,286,163,270,155]],"iscrowd":0},{"id":2,"image_id":0,"category_id":4,"bbox":[223,259,72,59],"area":4248,"segmentation":[[243,259,231,265,223,280,223,289,235,309,253,311,263,318,271,310,280,292,291,293,295,290,294,276,288,270,288,263,273,262,268,267,245,259]],"iscrowd":0},{"id":3,"image_id":0,"category_id":1,"bbox":[346,144,132,113],"area":14916,"segmentation":[[413,144,398,148,374,166,364,165,360,179,352,186,346,201,348,217,352,224,379,222,372,237,375,248,380,252,394,253,402,257,418,255,430,247,451,240,451,227,454,223,478,207,476,198,455,183,456,169,449,166,438,168,438,158,429,147,415,144]],"iscrowd":0},{"id":4,"image_id":0,"category_id":2,"bbox":[362,259,65,69],"area":4485,"segmentation":[[396,259,368,272,364,276,362,291,372,308,390,320,399,318,398,324,404,328,414,302,426,293,427,289,417,265,413,261,405,262,402,259]],"iscrowd":0},{"id":5,"image_id":1,"category_id":5,"bbox":[164,69,349,345],"area":120405,"segmentation":[[343.25,69,301,72,276,79,249,92,221,112,200,134,179,169,169,196,164,228,166,271,174,299,186,325,207,355,237,382,260,396,288,407,324,414,364,413,400,404,426,392,453,373,480,344,495,319,509,281,513,250,509,205,500,177,484,147,463,121,434,97,407,83,380,74,349,69]],"iscrowd":0},{"id":6,"image_id":1,"category_id":1,"bbox":[359,147,134,115],"area":15410,"segmentation":[[433,147,413,150,387,169,378,167,373,173,373,181,361,195,359,214,363,227,379,228,386,224,394,226,385,240,386,250,393,256,419,262,463,246,467,243,465,237,467,230,477,220,493,210,492,203,484,194,471,187,471,172,465,169,453,171,451,158,438,147]],"iscrowd":0},{"id":7,"image_id":1,"category_id":2,"bbox":[373,263,67,71],"area":4757,"segmentation":[[410,263,381,276,376,282,377,285,373,295,380,303,383,313,396,319,402,325,411,325,412,332,418,334,418,329,424,322,425,313,428,307,440,296,440,292,436,285,431,270,427,266,421,266,414,263]],"iscrowd":0},{"id":8,"image_id":1,"category_id":3,"bbox":[212,159,129,108],"area":13932,"segmentation":[[273,159,263,168,248,166,244,169,233,169,228,172,220,192,218,219,212,224,212,231,229,253,261,259,271,267,300,265,309,258,311,252,323,239,327,236,334,236,333,229,335,226,341,225,341,221,331,210,313,202,314,195,312,192,306,193,298,189,300,187,300,170,294,164,282,159]],"iscrowd":0},{"id":9,"image_id":1,"category_id":4,"bbox":[233,263,74,60],"area":4440,"segmentation":[[254,263,241,272,233,289,234,294,242,304,244,312,246,314,255,316,265,316,275,323,277,319,282,316,290,297,293,296,301,298,305,296,307,286,304,278,299,275,301,270,298,267,286,266,275,269,260,265,258,263,255,264]],"iscrowd":0},{"id":10,"image_id":2,"category_id":5,"bbox":[163,87,332,333],"area":110556,"segmentation":[[331,87,301,89,269,98,240,113,221,127,195,155,179,182,169,207,163,246,164,273,172,308,189,344,208,368,228,386,260,405,293,416,324,420,353,418,386,410,415,396,443,375,470,343,481,322,490,299,495,270,495,236,489,205,477,176,457,146,428,119,399,102,363,90,336,87]],"iscrowd":0},{"id":11,"image_id":2,"category_id":3,"bbox":[211,174,122,101],"area":12322,"segmentation":[[284,174,272,180,261,175,236,177,223,199,220,220,211,230,226,258,249,263,262,275,293,275,310,261,313,254,326,254,327,247,333,244,325,230,310,223,313,212,301,205,301,184,285,174]],"iscrowd":0},{"id":12,"image_id":2,"category_id":4,"bbox":[221,267,69,57],"area":3933,"segmentation":[[243.5,267,234,269,221,288,229,312,246,316,256,324,268,314,276,302,287,304,290,291,285,285,286,279,258,276,255,270,249,267]],"iscrowd":0},{"id":13,"image_id":2,"category_id":1,"bbox":[351,190,127,109],"area":13843,"segmentation":[[418,190,400,196,391,206,374,204,370,212,354,226,351,235,353,252,362,255,376,254,380,257,371,267,372,276,378,287,400,299,420,294,441,293,444,291,444,281,449,276,478,261,477,251,461,235,462,224,458,218,447,220,448,207,442,197,421,190]],"iscrowd":0},{"id":14,"image_id":2,"category_id":2,"bbox":[350,286,60,63],"area":3780,"segmentation":[[367,286,352,298,353,310,350,314,359,335,366,340,363,343,364,349,377,344,385,336,404,333,410,314,408,301,396,301,386,292,369,286]],"iscrowd":0}]}

================
File: src/preprocessing/__init__.py
================
from .image_scaling import upscale_depth, align_segmentation_mask
from .noise_reduction import reduce_depth_noise

================
File: src/preprocessing/image_scaling.py
================
import cv2
import numpy as np

""" 
    The goal of this file is to match the resolution of the RGB Image
    since the RGB Depth Image is in a lower resolution
"""

def upscale_depth(rgbd_image, target_shape):
    """
        
        args:
        rgbd_image (numpy.ndarray): RGBD image with depth as the 4th channel
        target_shape (tuple): Desired output shape (height, width)
        
        returns:
        numpy.ndarray: RGBD image with upscaled depth
    
    """
    if rgbd_image.shape[2] != 4:
        raise ValueError("Expected RGBD image with 4 channels")
    
    h, w = rgbd_image.shape[:2]
    target_h, target_w = target_shape

    # calculate scaling factor while maintaining aspect ratio for the image upscaling not the scaling factor for th pinhole
    scale = min(target_w / w, target_h / h)
    new_w, new_h = int(w * scale), int(h * scale)

    # depth channels 
    rgb_resized = cv2.resize(rgbd_image[:,:,:3], (new_w, new_h), interpolation=cv2.INTER_LINEAR)
    depth_resized = cv2.resize(rgbd_image[:,:,3], (new_w, new_h), interpolation=cv2.INTER_NEAREST)

    # create a new upscaled image 
    upscaled_rgbd = np.zeros((*target_shape, 4), dtype=rgbd_image.dtype)
    
    #  padding
    pad_w = (target_w - new_w) // 2
    pad_h = (target_h - new_h) // 2

    # put to center para maangas
    upscaled_rgbd[pad_h:pad_h+new_h, pad_w:pad_w+new_w, :3] = rgb_resized
    upscaled_rgbd[pad_h:pad_h+new_h, pad_w:pad_w+new_w, 3] = depth_resized

    return upscaled_rgbd
def align_segmentation_mask(mask, rgbd_shape):

    """
        args:
        mask (numpy.ndarray): Segmentation mask
        rgbd_shape (tuple): shape of rgbd image


        return:
        numpy_ndarry: Aligned Segmentation mask
    """
    
    if mask.shape[:2] != rgbd_shape[:2]:
        return cv2.resize(mask,(rgbd_shape[1],rgbd_shape[0]), interpolation=cv2.INTER_NEAREST)


    return mask

================
File: src/preprocessing/low_res_test.py
================
# src/preprocessing/test_low_res_rgbd.py

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
from src.utils.utils import load_rgbd_image, get_corresponding_rgbd_filename
from src.preprocessing.noise_reduction import reduce_depth_noise

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
def visualize_rgbd(rgbd_image, title):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
    
    #  RGB real image
    ax1.imshow(rgbd_image[:,:,:3])
    ax1.set_title("RGB")
    ax1.axis('off')
    
    # Depth factors
    depth_vis = ax2.imshow(rgbd_image[:,:,3], cmap='jet')
    ax2.set_title("Depth")
    ax2.axis('off')
    plt.colorbar(depth_vis, ax=ax2, label='Depth')
    
    plt.suptitle(title)
    plt.tight_layout()
    return fig

def process_low_res_rgbd(rgb_filename, output_dir):
    # get corresponding RGBD filename
    rgbd_filename = get_corresponding_rgbd_filename(rgb_filename)
    if rgbd_filename is None:
        raise ValueError(f"No corresponding RGBD filename found for {rgb_filename}")

    # load low-resolution RGBD image
    data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'data'))
    image_input_dir = os.path.join(data_dir, 'image_input')
    rgbd_path = os.path.join(image_input_dir, rgbd_filename)
    original_rgbd = load_rgbd_image(rgbd_path)

    print(f"Loaded RGBD image shape: {original_rgbd.shape}")
    print(f"Depth channel min: {original_rgbd[:,:,3].min()}, max: {original_rgbd[:,:,3].max()}")
    print(f"Unique depth values: {np.unique(original_rgbd[:,:,3])}")

    #vbisualize original RGBD
    fig_original = visualize_rgbd(original_rgbd, "Original Low-res RGBD")
    fig_original.savefig(os.path.join(output_dir, f"{os.path.splitext(rgbd_filename)[0]}_original.png"))
    plt.close(fig_original)

    # Apply noise reduction to depth channel
    depth = original_rgbd[:,:,3].astype(np.float32)
    noise_reduced_depth = reduce_depth_noise(depth)

    print(f"Noise reduced depth min: {noise_reduced_depth.min()}, max: {noise_reduced_depth.max()}")

    # create noise-reduced RGBD image
    noise_reduced_rgbd = original_rgbd.copy()
    noise_reduced_rgbd[:,:,3] = noise_reduced_depth

    # Visualize noise-reduced RGBD
    fig_reduced = visualize_rgbd(noise_reduced_rgbd, "Noise-reduced Low-res RGBD")
    fig_reduced.savefig(os.path.join(output_dir, f"{os.path.splitext(rgbd_filename)[0]}_noise_reduced.png"))
    plt.close(fig_reduced)

    return original_rgbd, noise_reduced_rgbd

def main():
    output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'output', 'low_res_test'))
    os.makedirs(output_dir, exist_ok=True)

    rgb_filenames = [
        'Pair1_png.rf.9a41eaba847f2815f37ffd3e13598fc6.jpg',
        'Pairtwo_png.rf.e23749dcf6644b0a2e561634554a5009.jpg',
        'Pair3_png.rf.984a166a90eb4fb2fc2ea9a4e5a882f4.jpg'
    ]

    for rgb_filename in rgb_filenames:
        print(f"Processing {rgb_filename}")
        original_rgbd, noise_reduced_rgbd = process_low_res_rgbd(rgb_filename, output_dir)
        print("---")

if __name__ == "__main__":
    main()

================
File: src/preprocessing/noise_reduction.py
================
import cv2
import numpy as np


def reduce_depth_noise(depth_image, method='bilateral'):
    """
        args:
        depth_image (numpy.ndarray): depth depth_image
        method (str) : noise reduction method ('bilateral' or 'median')

        return:
        numpy.ndarray: Noise reduced dpeth depth_image

    """
    print(f"Noise reduction input depth min: {depth_image.min()}, max: {depth_image.max()}")
    
    if method == 'bilateral':
        result = cv2.bilateralFilter(depth_image, 9, 75, 75)
    elif method == 'median':
        result = cv2.medianBlur(depth_image, 5)
    else:
        raise ValueError('Unsupported noise reduction method')
    
    print(f"Noise reduction output depth min: {result.min()}, max: {result.max()}")
    return result

================
File: src/preprocessing/preprocess.py
================
import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from scipy.ndimage import uniform_filter, median_filter
from mpl_toolkits.axes_grid1 import make_axes_locatable
from src.utils.utils import (
    load_rgbd_image,
    load_coco_data,
    create_segmentation_mask,
    get_corresponding_rgbd_filename,
    ensure_directory_exists
)
from src.preprocessing.image_scaling import align_segmentation_mask
from src.utils.visualization import visualize_preprocessing_steps

class PreprocessingPipeline:
    def __init__(self, data_dir, output_dir):
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.image_input_dir = os.path.join(data_dir, 'image_input')
        self.train_dir = os.path.join(data_dir, 'train')
        self.segmentation_file = os.path.join(data_dir, 'train', '_annotations.coco.json')
        
        ensure_directory_exists(output_dir)
        self.processed_dir = os.path.join(output_dir, 'processed')
        ensure_directory_exists(self.processed_dir)
        
        self.coco_data = load_coco_data(self.segmentation_file)
        self.category_id_to_name = {cat['id']: cat['name'] for cat in self.coco_data['categories']}
        
        self.camera_height = 33  # cm
        self.plate_height = 1.5  # cm
        self.plate_depth = 0.7  # cm
    def color_to_depth(self, rgbd_image, segmentation_mask, plate_id=5):
        """
        Convert color information to estimated depth values for each pixel,
        with depth relative to each object's own intensity range and smoothing of extreme values.
        """
        # Extract RGB channels
        rgb = rgbd_image[:,:,:3].astype(float)
        
        # Calculate grayscale intensity
        intensity = np.dot(rgb, [0.299, 0.587, 0.114])
        
        # Initialize depth map
        depth = np.zeros_like(intensity)
        
        # Set plate depth
        plate_mask = segmentation_mask == plate_id
        depth[plate_mask] = self.plate_height + self.plate_depth/2
        
        # Process each object separately
        for obj_id in np.unique(segmentation_mask):
            if obj_id == 0 or obj_id == plate_id:  # Skip background and plate
                continue
            
            obj_mask = segmentation_mask == obj_id
            obj_intensity = intensity[obj_mask]
            
            # Normalize intensity within the object
            obj_norm_intensity = (obj_intensity - np.min(obj_intensity)) / (np.max(obj_intensity) - np.min(obj_intensity))
            
            # Invert normalized intensity so darker colors are further away
            obj_inv_intensity = 1 - obj_norm_intensity
            
            # Scale depth values
            min_object_height = 0.5  # Minimum height above plate for the brightest part
            max_object_height = 8    # Maximum height above plate for the darkest part
            obj_depth = obj_inv_intensity * (max_object_height - min_object_height) + min_object_height
            
            # Create a 2D depth map for the object
            obj_depth_2d = np.zeros_like(depth)
            obj_depth_2d[obj_mask] = obj_depth
            
            # Apply median filter to reduce noise while preserving edges
            obj_depth_filtered = median_filter(obj_depth_2d, size=3)
            
            # Calculate local average depth
            local_avg = uniform_filter(obj_depth_filtered, size=5)
            
            # Identify extreme values
            threshold = 1.5  # Adjust this value to control sensitivity
            extreme_mask = np.abs(obj_depth_filtered - local_avg) > threshold
            
            # Adjust extreme values relative to surrounding pixels
            adjustment_factor = 0.7  # Adjust this value to control smoothing strength
            obj_depth_adjusted = obj_depth_filtered.copy()
            obj_depth_adjusted[extreme_mask] = (
                obj_depth_filtered[extreme_mask] * (1 - adjustment_factor) +
                local_avg[extreme_mask] * adjustment_factor
            )
            
            # Add plate height and depth
            obj_depth_adjusted[obj_mask] += self.plate_height + self.plate_depth
            
            # Assign depth to object pixels
            depth[obj_mask] = obj_depth_adjusted[obj_mask]
        
        # Set background to camera height
        background_mask = segmentation_mask == 0
        depth[background_mask] = self.camera_height
        
        return depth
    def analyze_object_depths(self, depth_map, segmentation_mask):
        """
        Analyze depth statistics for each object in the segmentation mask.

        Args:
        depth_map (numpy.ndarray): Estimated depth values
        segmentation_mask (numpy.ndarray): Segmentation mask

        Returns:
        dict: Object depth statistics
        """
        object_stats = {}
        unique_objects = np.unique(segmentation_mask)
        unique_objects = unique_objects[unique_objects != 0]  # Exclude background

        for obj_id in unique_objects:
            obj_mask = segmentation_mask == obj_id
            obj_depths = depth_map[obj_mask]
            
            object_stats[obj_id] = {
                'min_depth': np.min(obj_depths),
                'max_depth': np.max(obj_depths),
                'mean_depth': np.mean(obj_depths),
                'median_depth': np.median(obj_depths),
                'std_depth': np.std(obj_depths)
            }

        return object_stats
    def visualize_object_depths(self, rgb_image, depth_map, segmentation_mask, object_stats, output_path):
        num_objects = len(object_stats)
        fig, axs = plt.subplots(2, 3, figsize=(20, 10 * (num_objects // 3 + 1)))
        fig.suptitle('Object Depth Visualization', fontsize=16)

        # Original RGB Image
        axs[0, 0].imshow(rgb_image)
        axs[0, 0].set_title('Original RGB Image')
        axs[0, 0].axis('off')

        # Full Depth Map
        im = axs[0, 1].imshow(depth_map, cmap='viridis')
        axs[0, 1].set_title('Full Depth Map')
        axs[0, 1].axis('off')
        plt.colorbar(im, ax=axs[0, 1], label='Depth (cm)')

        # Segmentation Mask
        axs[0, 2].imshow(segmentation_mask, cmap='tab20')
        axs[0, 2].set_title('Segmentation Mask')
        axs[0, 2].axis('off')

        # Individual object depth maps
        for i, (obj_id, stats) in enumerate(object_stats.items()):
            row = (i + 3) // 3
            col = (i + 3) % 3
            obj_mask = segmentation_mask == obj_id
            obj_depth = np.ma.masked_where(~obj_mask, depth_map)
            im = axs[row, col].imshow(obj_depth, cmap='viridis')
            axs[row, col].set_title(f'Object {obj_id} Depth')
            axs[row, col].axis('off')
            plt.colorbar(im, ax=axs[row, col], label='Depth (cm)')

        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    def analyze_object_depths(self, depth_map, segmentation_mask):
        object_stats = {}
        unique_objects = np.unique(segmentation_mask)
        unique_objects = unique_objects[unique_objects != 0]  # Exclude background

        for obj_id in unique_objects:
            obj_mask = segmentation_mask == obj_id
            obj_depths = depth_map[obj_mask]
            
            object_stats[obj_id] = {
                'min_depth': np.min(obj_depths),
                'max_depth': np.max(obj_depths),
                'mean_depth': np.mean(obj_depths),
                'median_depth': np.median(obj_depths),
                'std_depth': np.std(obj_depths)
            }

        return object_stats
    def upscale_with_padding(self, image, target_shape):
        h, w = image.shape[:2]
        target_h, target_w = target_shape
        scale = min(target_h / h, target_w / w)
        new_h, new_w = int(h * scale), int(w * scale)
        resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        
        if len(image.shape) == 3:
            padded = np.zeros((target_h, target_w, image.shape[2]), dtype=resized.dtype)
        else:
            padded = np.zeros((target_h, target_w), dtype=resized.dtype)
        
        pad_h, pad_w = (target_h - new_h) // 2, (target_w - new_w) // 2
        padded[pad_h:pad_h+new_h, pad_w:pad_w+new_w] = resized
        return padded

    def normalize_plate_color(self, upscaled_rgbd, segmentation_mask, plate_id=5):
        plate_mask = segmentation_mask == plate_id
        plate_colors = upscaled_rgbd[plate_mask][:, :3]
        
        kmeans = KMeans(n_clusters=3, random_state=42)
        kmeans.fit(plate_colors)
        
        cluster_sizes = np.bincount(kmeans.labels_)
        sorted_clusters = sorted(zip(cluster_sizes, kmeans.cluster_centers_), reverse=True)
        dominant_color = sorted_clusters[0][1]
        
        color_std = np.std(plate_colors, axis=0)
        distance_threshold = np.mean(color_std) * 2
        
        close_to_dominant = np.linalg.norm(plate_colors - dominant_color, axis=1) < distance_threshold
        normalized_plate_color = np.mean(plate_colors[close_to_dominant], axis=0)
        
        normalized_rgbd = upscaled_rgbd.copy().astype(float)
        for i in range(3):
            normalized_rgbd[:,:,i] = (normalized_rgbd[:,:,i] - normalized_plate_color[i]) / normalized_plate_color[i] * 0.7 + 0.7
        
        return normalized_rgbd, normalized_plate_color


    def process_image(self, rgb_filename, image_id):
        try:
            rgb_path = os.path.join(self.train_dir, rgb_filename)
            rgb_image = cv2.imread(rgb_path)
            if rgb_image is None:
                raise FileNotFoundError(f"Could not load RGB image at {rgb_path}")
            rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)

            rgbd_filename = get_corresponding_rgbd_filename(rgb_filename)
            if rgbd_filename is None:
                raise ValueError(f"No corresponding RGBD filename found for {rgb_filename}")

            rgbd_path = os.path.join(self.image_input_dir, rgbd_filename)
            rgbd_image = load_rgbd_image(rgbd_path)

            upscaled_rgbd = self.upscale_with_padding(rgbd_image, rgb_image.shape[:2])

            segmentation_mask = create_segmentation_mask(image_id, self.coco_data)
            segmentation_mask = align_segmentation_mask(segmentation_mask, rgb_image.shape[:2])
            
            # Extract and calibrate depth from RGBD
            calibrated_depth = self.color_to_depth(upscaled_rgbd, segmentation_mask)
            
            # Analyze object depths
            object_stats = self.analyze_object_depths(calibrated_depth, segmentation_mask)

            # Normalize RGB channels (if needed)
            normalized_rgbd, normalized_plate_color = self.normalize_plate_color(upscaled_rgbd, segmentation_mask)

            # Prepare segmented data
            segmented_data = {}
            for obj_id in np.unique(segmentation_mask):
                if obj_id == 0:  # Assuming 0 is background
                    continue
                object_mask = segmentation_mask == obj_id
                
                segmented_data[obj_id] = {
                    'rgb': rgb_image.copy() * object_mask[:,:,np.newaxis],
                    'rgbd': normalized_rgbd.copy() * object_mask[:,:,np.newaxis],
                    'depth': calibrated_depth.copy() * object_mask,
                    'colors': normalized_rgbd[object_mask],
                    'name': self.category_id_to_name.get(obj_id, f"unknown_object_{obj_id}")
                }

            # Visualize preprocessing steps including depth
            vis_filename = f"{os.path.splitext(rgb_filename)[0]}_visualization.png"
            visualize_preprocessing_steps(
                rgb_image, rgbd_image, normalized_rgbd, segmentation_mask, 
                segmented_data, self.output_dir, vis_filename, 
                normalized_plate_color, calibrated_depth
            )

            return rgb_image, normalized_rgbd, calibrated_depth, segmentation_mask, segmented_data, normalized_plate_color, object_stats
        except Exception as e:
            print(f"Error processing image {rgb_filename}: {str(e)}")
            raise
    def run(self):
        rgb_filenames = [
            'Pair1_png.rf.9a41eaba847f2815f37ffd3e13598fc6.jpg',
            'Pairtwo_png.rf.e23749dcf6644b0a2e561634554a5009.jpg',
            'Pair3_png.rf.984a166a90eb4fb2fc2ea9a4e5a882f4.jpg'
        ]
        image_ids = [2, 0, 1]  # Corresponding image IDs in the COCO dataset

        for rgb_filename, image_id in zip(rgb_filenames, image_ids):
            print(f"Processing {rgb_filename}")
            rgb, normalized_rgbd, depth, mask, segmented_data, normalized_plate_color, object_stats = self.process_image(rgb_filename, image_id)
            print(f"Processed {rgb_filename}.")
            print(f"RGB shape: {rgb.shape}, Normalized RGBD shape: {normalized_rgbd.shape}, Depth shape: {depth.shape}, Mask shape: {mask.shape}")
            print(f"Number of segmented objects: {len(segmented_data)}")
            print(f"Normalized plate color: {normalized_plate_color}")
            print("Object depth statistics:")
            for obj_id, stats in object_stats.items():
                print(f"  Object {obj_id} ({self.category_id_to_name.get(obj_id, 'Unknown')}):")
                for stat_name, stat_value in stats.items():
                    print(f"    {stat_name}: {stat_value:.2f} cm")
            print(f"Saved processed data to {self.processed_dir}")
            print("---")

================
File: src/preprocessing/test.py
================
# src/preprocessing/test.py

import os
import sys
import numpy as np
import cv2

# Add the project root directory to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from src.preprocessing.preprocess import PreprocessingPipeline
from src.utils.utils import load_rgbd_image, create_segmentation_mask, load_coco_data

def test_load_rgbd_image(pipeline):
    rgbd_filename = 'Pairone.png'
    rgbd_image = load_rgbd_image(os.path.join(pipeline.image_input_dir, rgbd_filename))
    assert rgbd_image.shape[2] == 4, f"Expected 4-channel RGBD image, got {rgbd_image.shape[2]} channels"
    print(f"RGBD image shape: {rgbd_image.shape}")
    print(f"RGB min: {rgbd_image[:,:,:3].min()}, max: {rgbd_image[:,:,:3].max()}")
    print(f"Depth min: {rgbd_image[:,:,3].min()}, max: {rgbd_image[:,:,3].max()}")
    print(f"Unique depth values: {np.unique(rgbd_image[:,:,3])}")

def test_create_segmentation_mask(pipeline):
    coco_data = load_coco_data(pipeline.segmentation_file)
    mask = create_segmentation_mask(2, coco_data)  # Using image_id 2 for 'Pair1_png'
    assert mask.shape == (480, 640), f"Expected mask shape (480, 640), got {mask.shape}"
    print(f"Segmentation mask shape: {mask.shape}")
    print(f"Unique mask values: {np.unique(mask)}")

def test_preprocessing_pipeline():
    data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'data'))
    output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'output'))
    pipeline = PreprocessingPipeline(data_dir, output_dir)

    print("Running full preprocessing pipeline:")
    pipeline.run()

    # Check if output files are created
    rgb_filenames = [
        'Pair1_png.rf.9a41eaba847f2815f37ffd3e13598fc6.jpg',
        'Pairtwo_png.rf.e23749dcf6644b0a2e561634554a5009.jpg',
        'Pair3_png.rf.984a166a90eb4fb2fc2ea9a4e5a882f4.jpg'
    ]
    
    for rgb_filename in rgb_filenames:
        output_filename = os.path.splitext(rgb_filename)[0]
        depth_path = os.path.join(output_dir, 'processed', f'{output_filename}_depth.npy')
        assert os.path.exists(depth_path), f"Depth file not found: {depth_path}"
        
    visualization_path = os.path.join(output_dir, 'preprocessing_visualization.png')
    assert os.path.exists(visualization_path), f"Visualization file not found: {visualization_path}"

    print("All tests passed successfully!")

if __name__ == "__main__":
    test_preprocessing_pipeline()

================
File: src/utils/__init__.py
================
# src/utils/__init__.py

from .visualization import visualize_preprocessing_steps, overlay_segmentation_on_rgbd
from .utils import (
    load_rgbd_image,
    load_coco_data,
    create_segmentation_mask,
    get_corresponding_rgbd_filename,
    ensure_directory_exists,
    align_segmentation_mask
)

================
File: src/utils/utils.py
================
import os
import json
import cv2
import numpy as np

def load_rgbd_image(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)
    if img is None:
        raise FileNotFoundError(f"Could not load image at {image_path}")
    if img.shape[2] != 4:
        raise ValueError(f"Expected 4-channel RGBD image, got {img.shape[2]} channels")
    
    print(f"Loaded RGBD image shape: {img.shape}")
    print(f"Depth channel min: {img[:,:,3].min()}, max: {img[:,:,3].max()}")
    print(f"Unique depth values: {np.unique(img[:,:,3])}")
    
    return img

def load_coco_data(json_file):
    with open(json_file, 'r') as f:
        return json.load(f)


def create_segmentation_mask(image_id, coco_data):
    image_info = next(img for img in coco_data['images'] if img['id'] == image_id)
    mask = np.zeros((image_info['height'], image_info['width']), dtype=np.uint8)
    
    annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_id]
    
    for ann in annotations:
        category_id = ann['category_id']
        for segmentation in ann['segmentation']:
            pts = np.array(segmentation).reshape((-1, 2)).astype(np.int32)
            cv2.fillPoly(mask, [pts], color=category_id)
    
    print(f"Number of annotations for image {image_id}: {len(annotations)}")
    print(f"Unique category IDs in mask: {np.unique(mask)}")
    
    return mask
def get_corresponding_rgbd_filename(rgb_filename):
    mapping = {
        'Pair1_png.rf.9a41eaba847f2815f37ffd3e13598fc6.jpg': 'Pairone.png',
        'Pairtwo_png.rf.e23749dcf6644b0a2e561634554a5009.jpg': 'Pairtwo.png',
        'Pair3_png.rf.984a166a90eb4fb2fc2ea9a4e5a882f4.jpg': 'Pairthree.png'
    }
    return mapping.get(rgb_filename)

def ensure_directory_exists(directory):
    """Create directory if it doesn't exist."""
    os.makedirs(directory, exist_ok=True)

def align_segmentation_mask(mask, target_shape):
    """Resize segmentation mask to match target shape."""
    if mask.shape[:2] != target_shape:
        return cv2.resize(mask, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_NEAREST)
    return mask

def save_depth_image(depth_data, output_path, as_uint8=True):
    """
    Save depth data as an image file.
    
    Args:
    depth_data (numpy.ndarray): 2D array of depth values
    output_path (str): Path to save the image
    as_uint8 (bool): If True, convert depth to uint8 before saving
    """
    if as_uint8:
        # normalizedepth to 0 255 range
        depth_min = np.min(depth_data)
        depth_max = np.max(depth_data)
        if depth_min != depth_max:
            depth_normalized = ((depth_data - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)
        else:
            depth_normalized = np.zeros_like(depth_data, dtype=np.uint8)
    else:
        depth_normalized = depth_data

    cv2.imwrite(output_path, depth_normalized)

def load_depth_image(input_path, as_float=True):
    """
    Load a depth image file.
    
    Args:
    input_path (str): Path to the depth image file
    as_float (bool): If True, convert depth to float32
    
    Returns:
    numpy.ndarray: 2D array of depth values
    """
    depth_image = cv2.imread(input_path, cv2.IMREAD_UNCHANGED)
    
    if depth_image is None:
        raise FileNotFoundError(f"Could not load depth image at {input_path}")
    
    if as_float:
        depth_image = depth_image.astype(np.float32) / 255.0
    
    return depth_image

def visualize_depth(depth_image, output_path):
    """
    Create a color visualization of a depth image and save it.
    
    Args:
    depth_image (numpy.ndarray): 2D array of depth values
    output_path (str): Path to save the visualization
    """
    plt.figure(figsize=(10, 8))
    plt.imshow(depth_image, cmap='viridis')
    plt.colorbar(label='Depth')
    plt.title('Depth Visualization')
    plt.axis('off')
    plt.savefig(output_path)
    plt.close()

================
File: src/utils/visualization.py
================
import matplotlib
matplotlib.use('Agg')  # set Agg
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os

def visualize_depth(depth_image):
    depth_min, depth_max = np.nanmin(depth_image), np.nanmax(depth_image)
    if depth_min != depth_max:
        normalized_depth = (depth_image - depth_min) / (depth_max - depth_min)
    else:
        normalized_depth = np.zeros_like(depth_image)
    return plt.cm.viridis(normalized_depth)

def visualize_preprocessing_steps(rgb_image, rgbd_image, normalized_rgbd, segmentation_mask, segmented_data, output_dir, filename, normalized_plate_color, calibrated_depth):
    fig, axs = plt.subplots(2, 3, figsize=(20, 15))
    fig.suptitle('Preprocessing Pipeline Visualization', fontsize=16)

    # Original RGB Image
    axs[0, 0].imshow(rgb_image)
    axs[0, 0].set_title('Original RGB Image')
    axs[0, 0].axis('off')

    # RGBD Image (showing RGB part)
    axs[0, 1].imshow(rgbd_image[:,:,:3])
    axs[0, 1].set_title('RGBD Image (RGB part)')
    axs[0, 1].axis('off')

    # Segmentation Mask
    axs[0, 2].imshow(segmentation_mask, cmap='tab20')
    axs[0, 2].set_title('Segmentation Mask')
    axs[0, 2].axis('off')

    # Normalized RGBD (showing RGB part)
    axs[1, 0].imshow(normalized_rgbd[:,:,:3])
    axs[1, 0].set_title('Normalized RGBD (RGB part)')
    axs[1, 0].axis('off')

    # Calibrated Depth
    depth_plot = axs[1, 1].imshow(calibrated_depth, cmap='viridis')
    axs[1, 1].set_title('Calibrated Depth')
    axs[1, 1].axis('off')
    plt.colorbar(depth_plot, ax=axs[1, 1], label='Depth (cm)')

    # Segmented Objects
    axs[1, 2].imshow(rgb_image)
    for obj_id, obj_data in segmented_data.items():
        obj_mask = obj_data['depth'] > 0
        axs[1, 2].imshow(np.ma.masked_where(~obj_mask, obj_data['depth']), cmap='jet', alpha=0.5)
    axs[1, 2].set_title('Segmented Objects with Depth')
    axs[1, 2].axis('off')

    plt.tight_layout()
    plt.savefig(f"{output_dir}/{filename}")
    plt.close()
def overlay_segmentation_on_rgbd(upscaled_rgbd, segmentation_mask):
    # Resize segmentation mask to match upscaled RGBD if necessary
    if upscaled_rgbd.shape[:2] != segmentation_mask.shape:
        segmentation_mask = cv2.resize(segmentation_mask, (upscaled_rgbd.shape[1], upscaled_rgbd.shape[0]), 
                                       interpolation=cv2.INTER_NEAREST)
    
    # Create a color map for segmentation
    cmap = plt.get_cmap('tab10')
    seg_colors = cmap(segmentation_mask / np.max(segmentation_mask))[:,:,:3]
    
    # Create a blended image
    alpha = 0.3  # Adjust this for segmentation transparency
    blended = upscaled_rgbd[:,:,:3] * (1-alpha) + seg_colors * alpha * 255
    
    # Add the depth channel back if it exists
    if upscaled_rgbd.shape[2] == 4:
        return np.dstack((blended, upscaled_rgbd[:,:,3]))
    else:
        return blended

================
File: src/__init__.py
================
from . import preprocessing

================
File: .gitignore
================
venv/

================
File: readme.md
================
# OKAY BITCHES TIME TO DEV THIS SHIT SORRY I'M JUST DOING THIS NOW
## date is October 14, 2023

so now first I'll be trying to develop the loading of data
- get the segmentation points 
- get the RGB depth image
  - needs to be upscaled so that the segmentation points match
- overlay the segmentation points to the depth image

### Cloud Plotting
- do the cloud Plotting

-- refer to the notes --


1. Problem Overview:
   We're working on estimating the volume of objects (in this case, crumpled paper balls) on a plate using a combination of RGB and depth imaging. The challenge is that we have a high-resolution RGB image but only a noisy, low-resolution depth image.

2. Data Sources:
   - High-resolution RGB image of a plate with crumpled paper balls
   - Low-resolution, noisy depth image of the same scene

3. Proposed Approach:
   a) Segmentation:
      - Use Mask R-CNN (or another segmentation method) on the RGB image to precisely identify the plate and paper balls.
   
   b) Depth Image Processing:
      - Scale the low-resolution depth image to match the RGB image's resolution.
      - Apply noise reduction techniques to improve depth data quality.
   
   c) Data Fusion:
      - Overlay the segmentation masks from the RGB image onto the scaled depth image.
      - Extract depth values only for pixels corresponding to the segmented objects.

   d) 3D Reconstruction:
      - Use the camera's intrinsic parameters (from the pinhole camera model) to convert 2D pixel coordinates and depth values into 3D points.
      - The known plate diameter serves as a reference for real-world scaling.

   e) Volume Estimation:
      - Use methods like Convex Hull or more advanced techniques to estimate the volume of the reconstructed 3D points for each paper ball.

4. Key Considerations:
   - Careful alignment of RGB and depth data
   - Handling noise and inaccuracies in the depth data
   - Accounting for potential loss of fine details due to initial low depth resolution
   - Calibration and real-world scaling using the plate as a reference object

5. Potential Enhancements:
   - Multi-view analysis if multiple images are available
   - Implementing more sophisticated depth refinement techniques
   - Exploring machine learning approaches for improved depth estimation

6. Validation:
   - Compare results with ground truth volumes (if available)
   - Analyze performance across different object shapes and sizes

================
File: requirements.txt
================
numpy==2.1.2
opencv-python==4.10.0.84
pandas==2.2.3
python-dateutil==2.9.0.post0
pytz==2024.2
six==1.16.0
tzdata==2024.2
