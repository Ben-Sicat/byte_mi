This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-12-06T17:51:47.474Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
data/
  rgbd/
    depth_frame_20241204_233746.meta
  segmented/
    _annotations.coco.json
    rgb_frame_20241204_233746.meta
  upscaled/
    depth_frame_20241204_233746_metadata.json
    volumes_20241204_233746.json
src/
  core/
    __init__.py
    depth_processor.py
    image_alignment.py
  preprocessing/
    __init__.py
    calibration.py
    noise_reduction.py
    preprocessing.py
  reconstruction/
    mesh_generator.py
    point_cloud.py
    volume_calculator.py
  utils/
    __init__.py
    coco_utils.py
    io_utils.py
    logging_utils.py
    visualization.py
  __init__.py
test/
  inspect_data.py
  test_point_cloud.py
  test_preprocessing.py
  test_reconstruction.py
.gitignore
README.md
requirements.txt
run_preprocessing.py
run_tests.py
run_volume_estimation.py
test_config.json

================================================================
Repository Files
================================================================

================
File: data/rgbd/depth_frame_20241204_233746.meta
================
{
    "width": 160,
    "height": 90,
    "format": 5,
    "timestamp": 84927.277066975,
    "centerPixelDepth": 0.33899998664855959,
    "minDepth": 0.3330000042915344,
    "maxDepth": 0.34200000762939455
}

================
File: data/segmented/_annotations.coco.json
================
{"info":{"year":"2024","version":"1","description":"Exported from roboflow.com","contributor":"","url":"https://public.roboflow.com/object-detection/undefined","date_created":"2024-12-04T16:48:42+00:00"},"licenses":[{"id":1,"url":"https://creativecommons.org/licenses/by/4.0/","name":"CC BY 4.0"}],"categories":[{"id":0,"name":"food","supercategory":"none"},{"id":1,"name":"plate","supercategory":"food"},{"id":2,"name":"rice","supercategory":"food"}],"images":[{"id":0,"license":1,"file_name":"rgb_frame_20241204_233746_png","height":480,"width":640,"date_captured":"2024-12-04T16:48:42+00:00"}],"annotations":[{"id":0,"image_id":0,"category_id":1,"bbox":[160,72,340,341],"area":115940,"segmentation":[[161,218,160,261,166,291,174,313,194,346,215,369,243,390,276,405,313,413,340,413,378,406,415,390,433,378,457,356,478,326,490,300,499,265,500,233,492,189,482,165,462,134,432,105,415,94,380,79,347,72,290,76,252,90,228,105,201,130,185,152,170,183]],"iscrowd":0},{"id":1,"image_id":0,"category_id":2,"bbox":[254,142,161,184],"area":29624,"segmentation":[[255,257,254,268,269,279,261,302,276,325,299,326,318,322,322,316,351,315,404,299,415,290,413,197,389,160,360,152,348,142,290,157,279,173,281,189,274,244]],"iscrowd":0}]}

================
File: data/segmented/rgb_frame_20241204_233746.meta
================
{
    "width": 640,
    "height": 480,
    "timestamp": 84927.277066975
}

================
File: data/upscaled/depth_frame_20241204_233746_metadata.json
================
{
    "intrinsic_params": {
        "focal_length": 110.19040724810432,
        "pixel_size": 0.29948160483423314,
        "principal_point": [
            80.0,
            45.0
        ],
        "image_dimensions": [
            90,
            160
        ],
        "camera_height": 33.0,
        "reference_object": {
            "type": "plate",
            "diameter": 25.5,
            "height": 0.7,
            "measured_diameter_pixels": 85.14713287353516,
            "center_pixels": [
                82.5,
                46.5
            ]
        }
    },
    "depth_scale": 0.09284073114395142,
    "processed_objects": {
        "plate": {
            "category_id": 1,
            "bbox": [
                160,
                72,
                340,
                341
            ]
        },
        "rice": {
            "category_id": 2,
            "bbox": [
                254,
                142,
                161,
                184
            ]
        }
    },
    "alignment_info": {
        "depth_shape": [
            90,
            160
        ],
        "rgb_shape": [
            480,
            640
        ]
    }
}

================
File: data/upscaled/volumes_20241204_233746.json
================
{
  "rice": {
    "volume_cm3": 22.669931411743164,
    "volume_cups": 0.7665616273880005,
    "uncertainty_cm3": 2.266993284225464,
    "uncertainty_cups": 0.07665616273880005,
    "avg_height_cm": 1.7807475328445435,
    "max_height_cm": 3.5901565551757812
  }
}

================
File: src/core/__init__.py
================
from .depth_processor import DepthProcessor
from .image_alignment import ImageAligner

================
File: src/core/depth_processor.py
================
import numpy as np
import cv2
from typing import Tuple, Optional, Dict
import logging
from pathlib import Path
from ..utils.io_utils import load_metadata, get_frame_dimensions

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthProcessor:
    """
    Handles depth data processing and image alignment.
    All measurements in meters.
    """
    
    def __init__(self, rgbd_meta_path: Path, rgb_meta_path: Path):
        """
        Initialize processor with metadata paths.
        """
        # Load dimensions from metadata
        self.depth_shape, self.rgb_shape = get_frame_dimensions(
            rgbd_meta_path, 
            rgb_meta_path
        )
        
        # Load depth metadata
        self.depth_meta = load_metadata(rgbd_meta_path)
        self.dtype = np.uint16
        
        # Processing parameters
        self.bilateral_d = 5
        self.bilateral_sigma_color = 50
        self.bilateral_sigma_space = 50
        
        logger.info(
            f"Initialized DepthProcessor with shapes - "
            f"Depth: {self.depth_shape}, RGB: {self.rgb_shape}"
        )
        
    def load_raw_depth(self, file_path: str) -> np.ndarray:
        """Load raw depth data."""
        try:
            raw_data = np.fromfile(file_path, dtype=self.dtype)
            
            expected_size = self.depth_shape[0] * self.depth_shape[1]
            if raw_data.size != expected_size:
                raise ValueError(
                    f"Raw data size {raw_data.size} does not match "
                    f"expected size {expected_size}"
                )
            
            depth_data = raw_data.reshape(self.depth_shape)
            
            logger.info(f"Loaded depth data - Shape: {depth_data.shape}, "
                       f"Range: [{depth_data.min()}, {depth_data.max()}]")
            
            return depth_data
            
        except Exception as e:
            logger.error(f"Error loading depth file: {str(e)}")
            raise
    def align_to_depth(self, rgb_image: Optional[np.ndarray] = None, 
            mask: Optional[np.ndarray] = None) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """
        Align RGB image and/or mask to depth resolution.
        
        Args:
            rgb_image: Optional RGB image at original resolution
            mask: Optional binary mask at original resolution
            
        Returns:
            Tuple[Optional[np.ndarray], Optional[np.ndarray]]: 
                RGB and mask at depth resolution (None for any not provided)
        """
        # Resize RGB image if provided
        aligned_rgb = None
        if rgb_image is not None:
            aligned_rgb = cv2.resize(
                rgb_image,
                (self.depth_shape[1], self.depth_shape[0]),  # width, height
                interpolation=cv2.INTER_AREA  # Better for downscaling
            )
        
        # Resize mask if provided
        aligned_mask = None
        if mask is not None:
            aligned_mask = cv2.resize(
                mask.astype(np.uint8),
                (self.depth_shape[1], self.depth_shape[0]),
                interpolation=cv2.INTER_NEAREST  # Preserve binary values
            ).astype(bool)  # Convert back to boolean
            
            # Log the mask alignment results
            if np.any(mask) and np.any(aligned_mask):
                original_pixels = np.sum(mask)
                aligned_pixels = np.sum(aligned_mask)
                logger.info(
                    f"Aligned mask - Original: {original_pixels} pixels, "
                    f"Aligned: {aligned_pixels} pixels"
                )
        
        return aligned_rgb, aligned_mask   
    def process_depth(self, depth_data: np.ndarray) -> np.ndarray:
        """Process depth data to remove noise."""
        if depth_data.shape != self.depth_shape:
            raise ValueError(f"Expected shape {self.depth_shape}, got {depth_data.shape}")
            
        # Convert to float32 for processing
        depth = depth_data.astype(np.float32)
        
        # Apply bilateral filter to reduce noise while preserving edges
        filtered_depth = cv2.bilateralFilter(
            depth,
            d=self.bilateral_d,
            sigmaColor=self.bilateral_sigma_color,
            sigmaSpace=self.bilateral_sigma_space
        )
        
        return filtered_depth

================
File: src/core/image_alignment.py
================
import numpy as np
import cv2
from typing import Tuple, Dict
import logging
from ..utils.coco_utils import CocoHandler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImageAligner:
    """Handles alignment between RGB images, RGBD data, and segmentation masks"""
    def __init__(self, coco_file: str):
        self.rgb_shape = None
        self.rgbd_shape = None
        self.coco_handler = CocoHandler(coco_file)
        
    def set_reference_sizes(self, rgb_shape: Tuple[int, int], 
                           rgbd_shape: Tuple[int, int]) -> None:
        self.rgb_shape = rgb_shape
        self.rgbd_shape = rgbd_shape
        logger.info(f"Set reference shapes - RGB: {rgb_shape}, RGBD: {rgbd_shape}")
        
    def align_rgbd_to_rgb(self, rgbd_data: np.ndarray) -> np.ndarray:
        """Align RGBD data to RGB dimensions"""
        if not (self.rgb_shape and self.rgbd_shape):
            raise ValueError("Reference sizes not set")
            
        if rgbd_data.shape[2] != 4:
            raise ValueError(f"Expected 4 channels in RGBD data, got {rgbd_data.shape[2]}")
            
        # Split and resize channels
        rgb_channels = rgbd_data[:, :, :3]
        depth_channel = rgbd_data[:, :, 3]
        
        aligned_rgb = cv2.resize(rgb_channels, 
                               (self.rgb_shape[1], self.rgb_shape[0]),
                               interpolation=cv2.INTER_LINEAR)
        
        aligned_depth = cv2.resize(depth_channel,
                                 (self.rgb_shape[1], self.rgb_shape[0]),
                                 interpolation=cv2.INTER_LINEAR)
        
        # Combine channels
        aligned_rgbd = np.zeros((*self.rgb_shape, 4), dtype=rgbd_data.dtype)
        aligned_rgbd[:, :, :3] = aligned_rgb
        aligned_rgbd[:, :, 3] = aligned_depth
        
        return aligned_rgbd
        
    def extract_object_depth(self, rgbd_aligned: np.ndarray, 
                           image_id: int,
                           category_name: str) -> Dict[str, np.ndarray]:
        """Extract depth data for specific object category"""
        # Get object mask using COCO handler
        mask = self.coco_handler.create_category_mask(
            image_id, 
            category_name, 
            self.rgb_shape
        )
        
        # Extract depth data
        depth_data = rgbd_aligned[:, :, 3].copy()
        masked_depth = np.zeros_like(depth_data)
        masked_depth[mask > 0] = depth_data[mask > 0]
        
        return {
            'mask': mask,
            'depth': masked_depth,
            'category': category_name
        }

================
File: src/preprocessing/__init__.py
================
from .preprocessing import PreprocessingPipeline
from .calibration import CameraCalibrator
from .noise_reduction import DepthNoiseReducer

================
File: src/preprocessing/calibration.py
================
import numpy as np
import cv2
from typing import Dict, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CameraCalibrator:
    """
    Calculate intrinsic parameters using known measurements and plate as reference.
    All measurements are in centimeters.
    """
    def __init__(self):
        # everything in cm
        self.camera_height = 33.0  
        self.plate_diameter = 25.5  
        self.plate_height = 0.7  
        
        self.focal_length = None
        self.principal_point = None
        self.pixel_size = None
        
    def calculate_focal_length(self, plate_diameter_pixels: float) -> float:
        """
        Calculate focal length using pinhole model and plate as reference.
        f = (P * H) / W
        where:
        f = focal length in pixels
        P = plate diameter in pixels
        H = camera height in cm
        W = actual plate diameter in cm
        """
        focal_length = (plate_diameter_pixels * self.camera_height) / self.plate_diameter
        logger.info(f"Calculated focal length: {focal_length:.2f} pixels")
        return focal_length
        
    def calculate_pixel_size(self, plate_diameter_pixels: float) -> float:
        """
        Calculate pixel size in cm
        pixel_size = actual_size / pixel_size
        """
        pixel_size = self.plate_diameter / plate_diameter_pixels
        logger.info(f"Calculated pixel size: {pixel_size:.6f} cm/pixel")
        return pixel_size

    def get_plate_measurements(self, plate_mask: np.ndarray) -> Dict:
        """
        Get plate measurements from mask in pixels.
        """
        contours, _ = cv2.findContours(
            plate_mask.astype(np.uint8),
            cv2.RETR_EXTERNAL,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        if not contours:
            raise ValueError("No plate contour found in mask")
        
        plate_contour = max(contours, key=cv2.contourArea)
       # inner portion of the circle  
        (center_x, center_y), radius = cv2.minEnclosingCircle(plate_contour)
        diameter_pixels = radius * 2
        
        return {
            'center': (center_x, center_y),
            'radius': radius,
            'diameter_pixels': diameter_pixels
        }

    def calculate_intrinsics(self, plate_mask: np.ndarray) -> Dict:
        """
        Calculate all intrinsic parameters using plate mask.
        """
        try:
            # get plate measurements
            plate_info = self.get_plate_measurements(plate_mask)
            
            # calculate focal length
            self.focal_length = self.calculate_focal_length(
                plate_info['diameter_pixels']
            )
            
            # calculate pixel size
            self.pixel_size = self.calculate_pixel_size(
                plate_info['diameter_pixels']
            )
            
            # principal point 
            height, width = plate_mask.shape
            self.principal_point = (width / 2, height / 2)
            
            intrinsic_params = {
                'focal_length': self.focal_length,  # in pixels
                'pixel_size': self.pixel_size,      # cm/pixel
                'principal_point': self.principal_point,
                'image_dimensions': (height, width),
                'camera_height': self.camera_height,
                'reference_object': {
                    'type': 'plate',
                    'diameter': self.plate_diameter,
                    'height': self.plate_height,
                    'measured_diameter_pixels': plate_info['diameter_pixels'],
                    'center_pixels': plate_info['center']
                }
            }
            
            self._validate_parameters(intrinsic_params)
            
            return intrinsic_params
            
        except Exception as e:
            logger.error(f"Error calculating intrinsic parameters: {str(e)}")
            raise

    def _validate_parameters(self, params: Dict) -> None:
        """
        Validate calculated parameters.
        """
        if params['focal_length'] <= 0:
            raise ValueError(f"Invalid focal length: {params['focal_length']}")
            
        if params['pixel_size'] <= 0 or params['pixel_size'] > 1:
            raise ValueError(f"Invalid pixel size: {params['pixel_size']}")
            
        measured_diameter_cm = (
            params['reference_object']['measured_diameter_pixels'] * 
            params['pixel_size']
        )
        error_margin = abs(measured_diameter_cm - self.plate_diameter)
        if error_margin > 1.7:  # More than 1cm error
            logger.warning(
                f"Large error in plate diameter measurement: "
                f"{error_margin:.2f}cm"
            )

    def get_depth_scale_factor(self, plate_depth_values: np.ndarray) -> float:
        """
        Calculate depth scale factor using plate as reference.
        """
        # Expected plate distance from camera
        expected_plate_distance = self.camera_height - self.plate_height
        
        # Use median of plate depth values
        measured_plate_distance = np.median(plate_depth_values)
        
        # Calculate scale factor
        scale_factor = expected_plate_distance / measured_plate_distance
        
        logger.info(f"Depth scale factor: {scale_factor:.4f}")
        return scale_factor

================
File: src/preprocessing/noise_reduction.py
================
import numpy as np
import cv2
from typing import Dict, Optional, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthNoiseReducer:
    """
    Handles noise reduction and cleaning of depth data from RGBD images.
    """
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize with optional configuration parameters.
        
        Args:
            config: Dictionary containing filter parameters:
                - bilateral_d: Diameter of pixel neighborhood
                - bilateral_sigma_color: Filter sigma in color space
                - bilateral_sigma_space: Filter sigma in coordinate space
                - median_kernel: Median filter kernel size
                - outlier_threshold: Standard deviation threshold for outliers
        """
        self.config = config or {
            'bilateral_d': 5,
            'bilateral_sigma_color': 0.1,
            'bilateral_sigma_space': 5.0,
            'median_kernel': 5,
            'outlier_threshold': 2.0
        }
        
    def remove_outliers(self, depth_data: np.ndarray, 
                       mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Remove outlier depth values using statistical analysis.
        """
        if mask is not None:
            valid_depths = depth_data[mask > 0]
        else:
            valid_depths = depth_data[depth_data > 0]
            
        if len(valid_depths) == 0:
            return depth_data
            
        mean_depth = np.mean(valid_depths)
        std_depth = np.std(valid_depths)
        threshold = std_depth * self.config['outlier_threshold']
        
        outliers = np.abs(depth_data - mean_depth) > threshold
        
        cleaned_depth = depth_data.copy()
        if np.any(outliers):
            kernel_size = self.config['median_kernel']
            local_median = cv2.medianBlur(
                depth_data.astype(np.float32),
                kernel_size
            )
            cleaned_depth[outliers] = local_median[outliers]
            
            logger.info(f"Removed {np.sum(outliers)} outlier points")
            
        return cleaned_depth
        
    def fill_missing_values(self, depth_data: np.ndarray) -> np.ndarray:
        """
        Fill missing or invalid depth values using interpolation.
        """
        invalid_mask = (depth_data <= 0) | np.isnan(depth_data)
        
        if not np.any(invalid_mask):
            return depth_data
            
        filled_depth = depth_data.copy()
        
        filled_depth = cv2.inpaint(
            filled_depth.astype(np.float32),
            invalid_mask.astype(np.uint8),
            3,
            cv2.INPAINT_NS
        )
        
        logger.info(f"Filled {np.sum(invalid_mask)} missing values")
        return filled_depth
        
    def apply_bilateral_filter(self, depth_data: np.ndarray) -> np.ndarray:
        """
        Apply bilateral filtering to reduce noise while preserving edges.
        """
        filtered_depth = cv2.bilateralFilter(
            depth_data.astype(np.float32),
            self.config['bilateral_d'],
            self.config['bilateral_sigma_color'],
            self.config['bilateral_sigma_space']
        )
        
        return filtered_depth
        
    def smooth_edges(self, depth_data: np.ndarray, 
                    mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Smooth depth values at object edges.
        """
        if mask is not None:
            edges = cv2.Canny(mask.astype(np.uint8), 100, 200)
            
            # dilate edges slightly
            kernel = np.ones((3,3), np.uint8)
            edge_region = cv2.dilate(edges, kernel, iterations=1)
            
            smoothed = cv2.GaussianBlur(
                depth_data.astype(np.float32),
                (5,5),
                1.0
            )
            
            result = depth_data.copy()
            result[edge_region > 0] = smoothed[edge_region > 0]
            
            return result
        
        return depth_data
        
    def process_depth(self, depth_data: np.ndarray,
                     mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Apply complete noise reduction pipeline to depth data.
        """
        logger.info("Starting depth noise reduction")
        
        filled_depth = self.fill_missing_values(depth_data)
        
        cleaned_depth = self.remove_outliers(filled_depth, mask)
        
        filtered_depth = self.apply_bilateral_filter(cleaned_depth)
        
        if mask is not None:
            final_depth = self.smooth_edges(filtered_depth, mask)
        else:
            final_depth = filtered_depth
            
        logger.info("Completed depth noise reduction")
        return final_depth

================
File: src/preprocessing/preprocessing.py
================
import cv2
import numpy as np
from typing import Dict, Optional
import logging
from pathlib import Path
import json

from ..core.depth_processor import DepthProcessor
from ..utils.io_utils import load_metadata, validate_depth_data, validate_image_alignment
from ..utils.coco_utils import CocoHandler
from ..core.depth_processor import DepthProcessor
from .calibration import CameraCalibrator
from .noise_reduction import DepthNoiseReducer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PreprocessingPipeline:
    def __init__(self, config: Dict):
        """
        Initialize preprocessing pipeline.
        
            config: Dict containing:
                - data_dir: Path to data directory
                - output_dir: Path to save processed data
                - coco_file: Path to COCO annotations
                - camera_height: Height of camera in cm
                - plate_diameter: Diameter of plate in cm
                - plate_height: Height of plate in cm
        """
        self.config = config
        self.data_dir = Path(config['data_dir'])
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.coco_handler = CocoHandler(config['coco_file'])
        self.calibrator = CameraCalibrator()
        self.noise_reducer = DepthNoiseReducer()
        
        logger.info("Initialized preprocessing pipeline")
        
    def load_data(self, frame_id: str) -> Dict:
        """Load all necessary data for processing"""
        try:
            rgbd_meta_path = self.data_dir / "rgbd" / f"depth_frame_{frame_id}.meta"
            rgb_meta_path = self.data_dir / "segmented" / f"rgb_frame_{frame_id}.meta"
            
            self.depth_processor = DepthProcessor(rgbd_meta_path, rgb_meta_path)
            
            rgb_path = self.data_dir / "segmented" / f"rgb_frame_{frame_id}.png"
            if not rgb_path.exists():
                raise FileNotFoundError(f"RGB image not found: {rgb_path}")
                
            rgb_image = cv2.imread(str(rgb_path))
            rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)
            
            depth_path = self.data_dir / "rgbd" / f"depth_frame_{frame_id}.raw"
            if not depth_path.exists():
                raise FileNotFoundError(f"Depth data not found: {depth_path}")
                
            depth_meta = load_metadata(rgbd_meta_path)
            
            raw_depth = self.depth_processor.load_raw_depth(str(depth_path))
            
            if not validate_depth_data(raw_depth, self.depth_processor.depth_shape, depth_meta):
                raise ValueError("Invalid depth data")
                
            processed_depth = self.depth_processor.process_depth(raw_depth)
            
            plate_mask = self.coco_handler.create_category_mask(
                frame_id, 
                'plate',
                rgb_image.shape[:2]
            )
            
            aligned_rgb, aligned_mask = self.depth_processor.align_to_depth(
                rgb_image, plate_mask
            )
            
            if not validate_image_alignment(rgb_image, aligned_rgb, self.depth_processor.depth_shape):
                raise ValueError("RGB alignment failed validation")
                
            if not validate_image_alignment(plate_mask, aligned_mask, self.depth_processor.depth_shape):
                raise ValueError("Mask alignment failed validation")
                
            return {
                'rgb': aligned_rgb,
                'depth': processed_depth,
                'plate_mask': aligned_mask,
                'frame_id': frame_id,
                'original_rgb': rgb_image,  # Keep original for reference
                'original_mask': plate_mask  # Keep original for reference
            }
            
        except Exception as e:
            logger.error(f"Error loading data for frame {frame_id}: {str(e)}")
            raise
    
    def process_single_image(self, frame_id: str) -> Dict:
        """Process a single image through the pipeline"""
        try:
            logger.info(f"Processing frame {frame_id}")
            
            data = self.load_data(frame_id)
            logger.info("Data loaded successfully")
            
            intrinsic_params = self.calibrator.calculate_intrinsics(data['plate_mask'])
            logger.info("Camera calibration completed")
            
            cleaned_depth = self.noise_reducer.process_depth(
                data['depth'],
                data['plate_mask']
            )
            
            plate_depth = cleaned_depth[data['plate_mask'] > 0]
            if len(plate_depth) == 0:
                raise ValueError("No valid depth values found in plate region")
                
            depth_scale = self.calibrator.get_depth_scale_factor(plate_depth)
            cleaned_depth *= depth_scale
            logger.info(f"Depth scaling applied (scale factor: {depth_scale:.4f})")
            
            annotations = self.coco_handler.get_image_annotations(frame_id)
            processed_objects = {}
            
            for ann in annotations:
                category_id = ann['category_id']
                category_name = self.coco_handler.categories[category_id]
                
                original_mask = self.coco_handler.create_mask(
                    ann, 
                    (self.depth_processor.rgb_shape[0], self.depth_processor.rgb_shape[1])
                )
                
                if np.any(original_mask):
                    _, aligned_mask = self.depth_processor.align_to_depth(mask=original_mask)
                    
                    if aligned_mask is not None and np.any(aligned_mask):
                        obj_depth = cleaned_depth.copy()
                        obj_depth[~aligned_mask] = 0
                        
                        processed_objects[category_name] = {
                            'mask': aligned_mask,
                            'depth': obj_depth,
                            'category_id': category_id,
                            'bbox': ann['bbox']
                        }
                        
            logger.info(f"Processed {len(processed_objects)} objects")
            
            results = {
                'frame_id': frame_id,
                'intrinsic_params': intrinsic_params,
                'depth': cleaned_depth,
                'depth_scale': depth_scale,
                'processed_objects': processed_objects,
                'rgb': data['rgb']
            }
            
            self.save_results(results)
            logger.info(f"Processing completed for frame {frame_id}")
            
            return results
            
        except Exception as e:
            logger.error(f"Error processing frame {frame_id}: {str(e)}")
            raise
    def save_results(self, results: Dict) -> None:
        """Save processed results to output directory"""
        frame_id = results['frame_id']
        base_filename = f"depth_frame_{frame_id}"
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        np.save(
            self.output_dir / f"{base_filename}_processed.npy",
            results['depth']
        )
        
        cv2.imwrite(
            str(self.output_dir / f"{base_filename}_aligned_rgb.png"),
            cv2.cvtColor(results['rgb'], cv2.COLOR_RGB2BGR)
        )
        
        for category, obj_data in results['processed_objects'].items():
            mask_filename = f"{base_filename}_{category}_mask.npy"
            np.save(self.output_dir / mask_filename, obj_data['mask'])
        
        metadata = {
            'intrinsic_params': results['intrinsic_params'],
            'depth_scale': float(results['depth_scale']),
            'processed_objects': {
                category: {
                    'category_id': obj_data['category_id'],
                    'bbox': obj_data['bbox']
                }
                for category, obj_data in results['processed_objects'].items()
            },
            'alignment_info': {
                'depth_shape': self.depth_processor.depth_shape,
                'rgb_shape': self.depth_processor.rgb_shape  # Using original RGB shape from metadata
            }
        }
        
        with open(self.output_dir / f"{base_filename}_metadata.json", 'w') as f:
            json.dump(metadata, f, indent=4)
            
        logger.info(
            f"Saved processed results to {self.output_dir}:\n"
            f"- Processed depth map\n"
            f"- Aligned RGB image\n"
            f"- Object masks: {list(results['processed_objects'].keys())}\n"
            f"- Metadata with alignment info"
        )
def run_preprocessing(config_path: str):
    """Run the complete preprocessing pipeline"""
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
            
        required_keys = [
            'data_dir', 'output_dir', 'coco_file',
            'rgb_shape', 'camera_height', 'plate_diameter', 'plate_height'
        ]
        for key in required_keys:
            if key not in config:
                raise ValueError(f"Missing required config key: {key}")
                
        pipeline = PreprocessingPipeline(config)
        
        for frame_id in config['frame_ids']:
            try:
                pipeline.process_single_image(frame_id)
                logger.info(f"Successfully processed frame {frame_id}")
            except Exception as e:
                logger.error(f"Failed to process frame {frame_id}: {str(e)}")
                continue
                
        logger.info("Preprocessing pipeline completed")
        
    except Exception as e:
        logger.error(f"Pipeline execution failed: {str(e)}")
        raise

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Run preprocessing pipeline")
    parser.add_argument('--config', required=True, help='Path to config file')
    args = parser.parse_args()
    
    run_preprocessing(args.config)

================
File: src/reconstruction/mesh_generator.py
================
import numpy as np
from typing import Dict, Optional, Tuple
import logging
import trimesh
from scipy.spatial import Delaunay
from sklearn.decomposition import PCA as skPCA

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MeshGenerator:
    def __init__(self, smoothing_factor: float = 0.5):
        self.smoothing_factor = smoothing_factor
        
    def _surface_reconstruction(self, points: np.ndarray) -> trimesh.Trimesh:
        """Generate mesh using surface reconstruction."""
        pca = skPCA(n_components=2)
        points_2d = pca.fit_transform(points)
        
        tri = Delaunay(points_2d)
        
        mesh = trimesh.Trimesh(
            vertices=points,
            faces=tri.simplices
        )
        
        return mesh
    def generate_mesh(self, points: np.ndarray, 
                     method: str = 'surface') -> trimesh.Trimesh:
        """
        Generate mesh from point cloud.
        """
        try:
            if method == 'convex':
                mesh = self._convex_hull_mesh(points)
            elif method == 'surface':
                mesh = self._surface_reconstruction(points)
            elif method == 'alpha':
                mesh = self._alpha_shape_mesh(points)
            else:
                raise ValueError(f"Unknown method: {method}")
                
            # Apply smoothing
            if self.smoothing_factor > 0:
                mesh = self._smooth_mesh(mesh)
                
            logger.info(
                f"Generated mesh using {method} method: "
                f"{len(mesh.vertices)} vertices, {len(mesh.faces)} faces"
            )
            
            return mesh
            
        except Exception as e:
            logger.error(f"Mesh generation failed: {str(e)}")
            raise
            
    def _convex_hull_mesh(self, points: np.ndarray) -> trimesh.Trimesh:
        """Generate mesh using convex hull."""
        mesh = trimesh.Trimesh(vertices=points)
        return mesh.convex_hull
        
    def _surface_reconstruction(self, points: np.ndarray) -> trimesh.Trimesh:
        """Generate mesh using surface reconstruction."""
        # Project points to 2D for triangulation
        pca = trimesh.transformations.PCA(points)
        points_2d = points @ pca[:2].T
        
        # Create triangulation
        tri = Delaunay(points_2d)
        
        # Create mesh
        mesh = trimesh.Trimesh(
            vertices=points,
            faces=tri.simplices
        )
        
        return mesh
        
    def _alpha_shape_mesh(self, points: np.ndarray, 
                         alpha: float = None) -> trimesh.Trimesh:
        """Generate mesh using alpha shape."""
        if alpha is None:
            # Estimate alpha based on point density
            bbox_volume = np.prod(np.ptp(points, axis=0))
            point_density = len(points) / bbox_volume
            alpha = 1 / np.sqrt(point_density)
            
        mesh = trimesh.Trimesh(vertices=points)
        hull = mesh.convex_hull
        alpha_mesh = mesh.subdivide().intersection(hull)
        
        return alpha_mesh
        
    def _smooth_mesh(self, mesh: trimesh.Trimesh) -> trimesh.Trimesh:
        """Apply Laplacian smoothing to mesh."""
        # Create copy to avoid modifying original
        smoothed = mesh.copy()
        
        # Apply Laplacian smoothing
        factor = self.smoothing_factor
        vertices = smoothed.vertices
        adjacency = trimesh.graph.vertex_adjacency_graph(smoothed)
        
        for _ in range(3):  # Number of smoothing iterations
            new_vertices = vertices.copy()
            for i in range(len(vertices)):
                neighbors = adjacency[i].keys()
                if neighbors:
                    centroid = np.mean([vertices[j] for j in neighbors], axis=0)
                    new_vertices[i] += factor * (centroid - vertices[i])
            vertices = new_vertices
            
        smoothed.vertices = vertices
        return smoothed
        
    def validate_mesh(self, mesh: trimesh.Trimesh) -> Dict[str, bool]:
        """
        Validate mesh quality and properties.
        
        Args:
            mesh: trimesh.Trimesh object
            
        Returns:
            Dictionary of validation results
        """
        results = {
            'is_watertight': mesh.is_watertight,
            'is_winding_consistent': mesh.is_winding_consistent,
            'has_degenerate_faces': len(mesh.degenerate_faces) > 0,
            'has_duplicate_faces': len(mesh.duplicate_faces) > 0,
            'has_infinite_values': not np.all(np.isfinite(mesh.vertices))
        }
        
        logger.info(f"Mesh validation results: {results}")
        return results

================
File: src/reconstruction/point_cloud.py
================
import numpy as np
from typing import Dict, Tuple, Optional
import logging
from scipy.spatial import ConvexHull

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PointCloud:
    """
    Handles conversion of depth maps to 3D point clouds and basic measurements.
    Uses pinhole camera model for 3D reconstruction.
    """
    
    def __init__(self, intrinsic_params: Dict):
        """
        Initialize with camera intrinsic parameters.
        
        Args:
            intrinsic_params: Dictionary containing:
                - focal_length: in pixels
                - pixel_size: in cm/pixel
                - principal_point: (x,y) in pixels
        """
        self.focal_length = intrinsic_params['focal_length']
        self.pixel_size = intrinsic_params['pixel_size']
        self.principal_point = intrinsic_params['principal_point']
        
        logger.info(
            f"Initialized PointCloud with focal length: {self.focal_length:.2f}, "
            f"pixel size: {self.pixel_size:.4f}"
        )
    def depth_to_point_cloud(self, depth_map: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:
        """Convert depth map to 3D point cloud using pinhole model."""
        # Get pixel coordinates
        rows, cols = depth_map.shape
        y_indices, x_indices = np.meshgrid(
            np.arange(rows), 
            np.arange(cols), 
            indexing='ij'
        )
        
        # Flatten arrays
        x_indices = x_indices.flatten()
        y_indices = y_indices.flatten()
        depth_values = depth_map.flatten()
        
        # Apply mask if provided
        if mask is not None:
            mask_flat = mask.flatten()
            valid_points = mask_flat > 0
            x_indices = x_indices[valid_points]
            y_indices = y_indices[valid_points]
            depth_values = depth_values[valid_points]
        
        # Center coordinates on principal point
        x_centered = (x_indices - self.principal_point[0]) * self.pixel_size
        y_centered = (y_indices - self.principal_point[1]) * self.pixel_size
        
        # Calculate X and Y coordinates using pinhole model
        X = x_centered * depth_values / self.focal_length
        Y = y_centered * depth_values / self.focal_length
        Z = depth_values
        
        # Stack coordinates
        points = np.column_stack([X, Y, Z])
        
        logger.info(f"Generated point cloud with {len(points)} points")
        return points   
    def estimate_volume(self, points: np.ndarray, method: str = 'convex_hull') -> float:
        """
        Estimate volume of point cloud.
        
        Args:
            points: Nx3 array of 3D points
            method: Volume estimation method ('convex_hull' or 'alpha_shape')
            
        Returns:
            float: Estimated volume in cubic centimeters
        """
        if len(points) < 4:
            raise ValueError("Need at least 4 points to estimate volume")
            
        if method == 'convex_hull':
            hull = ConvexHull(points)
            return hull.volume
        elif method == 'alpha_shape':
            # TODO: Implement alpha shape method
            raise NotImplementedError("Alpha shape method not implemented")
        else:
            raise ValueError(f"Unknown volume estimation method: {method}")
            
    def calculate_surface_area(self, points: np.ndarray) -> float:
        """
        Calculate surface area of point cloud using convex hull.
        
        Args:
            points: Nx3 array of 3D points
            
        Returns:
            float: Surface area in square centimeters
        """
        if len(points) < 4:
            raise ValueError("Need at least 4 points to calculate surface area")
            
        hull = ConvexHull(points)
        return hull.area
        
    def get_dimensions(self, points: np.ndarray) -> Dict[str, float]:
        """
        Calculate bounding box dimensions.
        
        Args:
            points: Nx3 array of 3D points
            
        Returns:
            Dict containing length, width, height in centimeters
        """
        min_coords = np.min(points, axis=0)
        max_coords = np.max(points, axis=0)
        dimensions = max_coords - min_coords
        
        return {
            'length': float(dimensions[0]),
            'width': float(dimensions[1]),
            'height': float(dimensions[2])
        }

================
File: src/reconstruction/volume_calculator.py
================
import numpy as np
from typing import Dict, Tuple, Optional
import logging
import cv2

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VolumeCalculator:
    def __init__(self, 
                 camera_height: float = 33.0,
                 plate_diameter: float = 25.5):
        """
        Initialize with camera and reference object parameters.
        Following pinhole camera model equations:
        X = (x-cx)Z/fx
        Y = (y-cy)Z/fy
        Volume = Σ(Z(x,y) - Zplate(x,y)) * dA
        """
        self.camera_height = camera_height
        self.plate_diameter = plate_diameter
        self.plate_height = 0.7
        self.CM3_TO_CUPS = 0.0338140225
        
    def calculate_volume(self, depth_map: np.ndarray, 
                        mask: np.ndarray,
                        plate_height: float,
                        intrinsic_params: Dict,
                        calibration: Optional[Dict] = None) -> Dict[str, float]:
        """
        Calculate volume using pinhole camera model equations.
        Volume = Σ(Z(x,y) - Zplate(x,y)) * dA
        where dA is the area of each pixel in world coordinates
        """
        try:
            # Get camera parameters
            fx = intrinsic_params['focal_length']
            pixel_size = intrinsic_params['pixel_size']
            cx, cy = intrinsic_params['principal_point']
            
            # Get pixel coordinates
            y_indices, x_indices = np.nonzero(mask)
            
            # Get depth values for valid pixels
            z_values = depth_map[y_indices, x_indices]
            z_plate = plate_height
            
            # Calculate differential area (dA) for each pixel
            # dA = (X*Y)/Z² * pixel_size²
            x_world = (x_indices - cx) * z_values / fx
            y_world = (y_indices - cy) * z_values / fx  # Assuming fx = fy
            
            # Calculate area of each pixel in world coordinates
            dA = pixel_size * pixel_size
            
            # Calculate volume using the formula
            heights = z_values - z_plate
            heights[heights < 0] = 0
            
            volume_cm3 = np.sum(heights * dA)
            
            # Apply calibration if provided
            if calibration and 'scale_factor' in calibration:
                volume_cm3 *= calibration['scale_factor']
            
            # Convert to cups
            volume_cups = volume_cm3 * self.CM3_TO_CUPS
            
            # Calculate statistics
            avg_height = np.mean(heights[heights > 0])
            max_height = np.max(heights)
            
            logger.info(
                f"Volume Calculation Results:\n"
                f"Average Height Above Plate: {avg_height:.2f} cm\n"
                f"Max Height Above Plate: {max_height:.2f} cm\n"
                f"Volume: {volume_cm3:.2f} cm³ ({volume_cups:.2f} cups)"
            )
            
            return {
                'volume_cm3': float(volume_cm3),
                'volume_cups': float(volume_cups),
                'uncertainty_cm3': float(volume_cm3 * 0.1),
                'uncertainty_cups': float(volume_cups * 0.1),
                'avg_height_cm': float(avg_height),
                'max_height_cm': float(max_height)
            }
            
        except Exception as e:
            logger.error(f"Error calculating volume: {str(e)}")
            raise
    def calculate_plate_reference(self, depth_map: np.ndarray,
                            plate_mask: np.ndarray,
                            intrinsic_params: Dict) -> Dict[str, float]:
        """Calculate reference measurements using plate and projection equations"""
        try:
            plate_depths = depth_map[plate_mask > 0]
            plate_height = np.median(plate_depths)
            
            plate_pixel = plate_height  # depth value from depth map
            plate_real = self.camera_height  # actual height of camera
            scale_factor = (plate_real / plate_pixel) * 0.33  # Added adjustment factor
            
            logger.info(
                f"Plate Calibration:\n"
                f"Camera Height (plate_real): {plate_real:.2f} cm\n"
                f"Depth Value (plate_pixel): {plate_pixel:.2f}\n"
                f"Scale Factor (plate_real/plate_pixel * 0.33): {scale_factor:.4f}\n"
                f"Reference Height: {plate_height:.2f} cm"
            )
            
            return {
                'scale_factor': float(scale_factor),
                'plate_height': float(plate_height),
                'camera_height': float(plate_real),
                'depth_value': float(plate_pixel)
            }
            
        except Exception as e:
            logger.error(f"Error in plate calibration: {str(e)}")
            raise

================
File: src/utils/__init__.py
================
from .coco_utils import CocoHandler

================
File: src/utils/coco_utils.py
================
import numpy as np
import cv2
import json
from typing import Dict, List, Tuple, Optional
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CocoHandler:
    """Utility class for handling COCO format annotations"""
    def __init__(self, annotation_file: str):
        self.annotations = self._load_annotations(annotation_file)
        self.categories = {cat['id']: cat['name'] 
                          for cat in self.annotations['categories']}
        
        # Create filename to image_id mapping
        self.filename_to_id = {}
        for img in self.annotations['images']:
            # Strip extension and any extra suffixes
            base_name = img['file_name'].split('_png')[0]
            self.filename_to_id[base_name] = img['id']
            
        logger.info(f"Loaded categories: {list(self.categories.values())}")
        logger.info(f"Loaded image mappings: {self.filename_to_id}")
        
    def _load_annotations(self, file_path: str) -> Dict:
        try:
            with open(file_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading COCO annotations: {str(e)}")
            raise
            
    def get_image_id(self, frame_id: str) -> int:
        """Get COCO image ID from frame ID"""
        filename = f"rgb_frame_{frame_id}"
        if filename not in self.filename_to_id:
            logger.error(
                f"No image ID found for {filename}. "
                f"Available files: {list(self.filename_to_id.keys())}"
            )
            raise ValueError(f"Image ID not found for frame {frame_id}")
        return self.filename_to_id[filename]
            
    def get_image_annotations(self, frame_id: str) -> List[Dict]:
        """Get annotations for specific image"""
        try:
            image_id = self.get_image_id(frame_id)
            annotations = [ann for ann in self.annotations['annotations'] 
                         if ann['image_id'] == image_id]
            
            if annotations:
                logger.info(f"Found {len(annotations)} annotations for image {frame_id}")
            else:
                logger.warning(f"No annotations found for image {frame_id}")
                
            return annotations
            
        except Exception as e:
            logger.error(f"Error getting annotations: {str(e)}")
            raise
    
    def get_category_id(self, category_name: str) -> int:
        """Get category ID from name"""
        for cat_id, name in self.categories.items():
            if name.lower() == category_name.lower():
                return cat_id
        available_categories = list(self.categories.values())
        raise ValueError(
            f"Category '{category_name}' not found. "
            f"Available categories are: {available_categories}"
        )
    
    def create_mask(self, annotation: Dict, shape: Tuple[int, int]) -> np.ndarray:
        """Create binary mask from single annotation"""
        mask = np.zeros(shape, dtype=np.uint8)
        
        if not annotation.get('segmentation'):
            logger.error(f"No segmentation data in annotation: {annotation}")
            return mask
            
        try:
            for segmentation in annotation['segmentation']:
                points = np.array(segmentation).reshape(-1, 2).astype(np.int32)
                cv2.fillPoly(mask, [points], 1)
                
            if not np.any(mask):
                logger.warning("Created mask is empty")
            else:
                logger.info(f"Created mask with {np.sum(mask)} positive pixels")
                
            return mask
            
        except Exception as e:
            logger.error(f"Error creating mask: {str(e)}")
            return mask

    def create_category_mask(self, frame_id: str, 
                           category_name: str, 
                           shape: Tuple[int, int]) -> np.ndarray:
        """Create mask for specific category"""
        try:
            category_id = self.get_category_id(category_name)
            mask = np.zeros(shape, dtype=np.uint8)
            
            annotations = self.get_image_annotations(frame_id)
            category_annotations = [
                ann for ann in annotations 
                if ann['category_id'] == category_id
            ]
            
            if not category_annotations:
                logger.warning(
                    f"No annotations found for category '{category_name}' "
                    f"in frame {frame_id}"
                )
                return mask
                
            for ann in category_annotations:
                ann_mask = self.create_mask(ann, shape)
                mask = cv2.bitwise_or(mask, ann_mask)
                
            if not np.any(mask):
                logger.warning(f"Final mask for category '{category_name}' is empty")
            else:
                logger.info(
                    f"Created mask for category '{category_name}' with "
                    f"{np.sum(mask)} positive pixels"
                )
                
            return mask
            
        except Exception as e:
            logger.error(f"Error creating category mask: {str(e)}")
            return np.zeros(shape, dtype=np.uint8)
        
    def visualize_mask(self, mask: np.ndarray, save_path: Optional[str] = None) -> np.ndarray:
        """
        Visualize a binary mask and optionally save it.
        
        Args:
            mask: Binary mask to visualize
            save_path: Optional path to save visualization
            
        Returns:
            np.ndarray: Visualization image
        """
        if not np.any(mask):
            logger.warning("Mask is empty - no visualization created")
            return np.zeros((*mask.shape, 3), dtype=np.uint8)
            
        # Create color visualization
        viz = np.zeros((*mask.shape, 3), dtype=np.uint8)
        viz[mask > 0] = [0, 255, 0]  # Green for masked areas
        
        # Add contours
        contours, _ = cv2.findContours(
            mask.astype(np.uint8), 
            cv2.RETR_EXTERNAL, 
            cv2.CHAIN_APPROX_SIMPLE
        )
        cv2.drawContours(viz, contours, -1, (255, 255, 255), 2)
        
        if save_path:
            save_path = Path(save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)
            cv2.imwrite(str(save_path), cv2.cvtColor(viz, cv2.COLOR_RGB2BGR))
            logger.info(f"Saved mask visualization to {save_path}")
            
        return viz

================
File: src/utils/io_utils.py
================
import json
from pathlib import Path
from typing import Dict, Tuple, Optional
import logging
import cv2
import numpy as np

logger = logging.getLogger(__name__)

def load_metadata(file_path: Path) -> Dict:
    """Load metadata from a .meta file."""
    try:
        with open(file_path, 'r') as f:
            metadata = json.load(f)
            
        required_keys = ['width', 'height']
        if not all(key in metadata for key in required_keys):
            raise ValueError(f"Missing required keys in metadata: {required_keys}")
            
        return metadata
    except Exception as e:
        logger.error(f"Error loading metadata from {file_path}: {str(e)}")
        raise

def get_frame_dimensions(rgbd_meta_path: Path, rgb_meta_path: Path) -> Tuple[Tuple[int, int], Tuple[int, int]]:
    """Get frame dimensions from metadata files."""
    try:
        rgbd_meta = load_metadata(rgbd_meta_path)
        rgb_meta = load_metadata(rgb_meta_path)
        
        depth_dims = (rgbd_meta['height'], rgbd_meta['width'])
        rgb_dims = (rgb_meta['height'], rgb_meta['width'])
        
        # Validate dimensions
        if not all(d > 0 for d in depth_dims + rgb_dims):
            raise ValueError("Invalid dimensions: all dimensions must be positive")
            
        logger.info(f"Loaded dimensions - Depth: {depth_dims}, RGB: {rgb_dims}")
        return depth_dims, rgb_dims
    except Exception as e:
        logger.error(f"Error getting frame dimensions: {str(e)}")
        raise

def validate_image_alignment(source: np.ndarray, aligned: np.ndarray, 
                           target_shape: Tuple[int, int], 
                           threshold: float = 0.1) -> bool:
    """
    Validate image alignment by checking dimensions and content.
    
    Args:
        source: Original image
        aligned: Aligned image
        target_shape: Expected shape after alignment
        threshold: Maximum allowed mean absolute difference after normalization
        
    Returns:
        bool: True if alignment is valid
    """
    try:
        # Check dimensions
        if aligned.shape[:2] != target_shape:
            logger.error(f"Invalid aligned shape: {aligned.shape[:2]} != {target_shape}")
            return False
            
        # For masks, check binary values are preserved
        if aligned.dtype == bool or (aligned.dtype == np.uint8 and np.max(aligned) == 1):
            if not np.array_equal(np.unique(aligned), np.unique(source)):
                logger.error("Binary mask values were not preserved during alignment")
                return False
                
        # For RGB images, check content preservation
        else:
            # Resize source to target for comparison
            source_resized = cv2.resize(source, target_shape[::-1])
            
            # Normalize and compare
            source_norm = source_resized.astype(float) / np.max(source_resized)
            aligned_norm = aligned.astype(float) / np.max(aligned)
            
            diff = np.mean(np.abs(source_norm - aligned_norm))
            if diff > threshold:
                logger.error(f"Alignment error too high: {diff:.3f} > {threshold}")
                return False
                
        return True
        
    except Exception as e:
        logger.error(f"Error validating alignment: {str(e)}")
        return False
def validate_depth_data(depth_data: np.ndarray, 
                       expected_shape: Tuple[int, int],
                       metadata: Optional[Dict] = None) -> bool:
    """Validate depth data against expected parameters."""
    try:
        # Check dimensions
        if depth_data.shape != expected_shape:
            logger.error(f"Invalid depth shape: {depth_data.shape} != {expected_shape}")
            return False
            
        # Check data type
        if depth_data.dtype not in [np.uint16, np.float32]:
            logger.error(f"Invalid depth dtype: {depth_data.dtype}")
            return False
            
        # Check value range
        if np.all(depth_data == 0):
            logger.error("Depth data is all zeros")
            return False
            
        # If metadata provided, check against expected ranges with tolerance
        if metadata:
            min_depth = metadata.get('minDepth')
            max_depth = metadata.get('maxDepth')
            
            if min_depth is not None and max_depth is not None:
                # Allow for some tolerance in the depth range
                tolerance = 0.5  # 50% tolerance
                min_allowed = min_depth * (1 - tolerance)
                max_allowed = max_depth * (1 + tolerance)
                
                actual_min = np.min(depth_data[depth_data > 0])
                actual_max = np.max(depth_data)
                
                if actual_min < min_allowed or actual_max > max_allowed:
                    logger.warning(
                        f"Depth values outside expected range: "
                        f"[{actual_min:.3f}, {actual_max:.3f}] vs "
                        f"[{min_depth:.3f}, {max_depth:.3f}]"
                    )
                    # Don't fail validation for range issues, just warn
                    
        return True
        
    except Exception as e:
        logger.error(f"Error validating depth data: {str(e)}")
        return False

================
File: src/utils/logging_utils.py
================
import logging
import sys
from pathlib import Path
from datetime import datetime
from typing import Optional

def setup_logging(
    log_dir: str = "logs",
    log_level: int = logging.INFO,
    log_prefix: Optional[str] = None
) -> logging.Logger:
    """
    Set up logging configuration to write to both file and console.
    
    Args:
        log_dir: Directory to store log files
        log_level: Logging level (default: INFO)
        log_prefix: Optional prefix for log filename
        
    Returns:
        logging.Logger: Configured logger instance
    """
    # Create logs directory if it doesn't exist
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    # Generate log filename with timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    prefix = f"{log_prefix}_" if log_prefix else ""
    log_file = log_path / f"{prefix}log_{timestamp}.txt"
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_formatter = logging.Formatter(
        '%(levelname)s: %(message)s'
    )
    
    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(log_level)
    file_handler.setFormatter(file_formatter)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(log_level)
    console_handler.setFormatter(console_formatter)
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    
    # Remove existing handlers to avoid duplication
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Add handlers
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    # Log initial message
    root_logger.info(f"Logging initialized. Log file: {log_file}")
    
    return root_logger

================
File: src/utils/visualization.py
================
import plotly.graph_objects as go
import numpy as np
from typing import Dict, Optional, Tuple
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthVisualizer:
    """
    Visualization tools for depth data analysis and validation.
    """
    def __init__(self, camera_height: float, plate_diameter: float, plate_height: float):
        """
        Initialize visualizer with camera parameters.
        
        Args:
            camera_height: Height of camera in cm
            plate_diameter: Diameter of reference plate in cm
            plate_height: Height of plate in cm
        """
        self.camera_height = camera_height
        self.plate_diameter = plate_diameter
        self.plate_height = plate_height
        self.expected_plate_distance = camera_height - plate_height
        
    def create_depth_surface(self, depth_map: np.ndarray, 
                           mask: Optional[np.ndarray] = None,
                           title: str = "Depth Surface Plot") -> go.Figure:
        """
        Create interactive 3D surface plot of depth data.
        
        Args:
            depth_map: 2D depth map array
            mask: Optional binary mask to focus on specific region
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        # Apply mask if provided
        if mask is not None:
            depth_data = depth_map.copy()
            depth_data[mask == 0] = np.nan
        else:
            depth_data = depth_map
            
        # Create coordinate grids
        rows, cols = depth_data.shape
        x = np.linspace(0, cols-1, cols)
        y = np.linspace(0, rows-1, rows)
        X, Y = np.meshgrid(x, y)
        
        # Create surface plot
        fig = go.Figure(data=[
            go.Surface(
                x=X,
                y=Y,
                z=depth_data,
                colorscale='Viridis',
                colorbar=dict(title='Depth (cm)'),
                contours=dict(
                    z=dict(
                        show=True,
                        usecolormap=True,
                        project_z=True
                    )
                )
            )
        ])
        
        # Update layout
        fig.update_layout(
            title=title,
            scene=dict(
                xaxis_title='X (pixels)',
                yaxis_title='Y (pixels)',
                zaxis_title='Depth (cm)',
                camera=dict(
                    eye=dict(x=1.5, y=1.5, z=1.5)
                ),
                aspectratio=dict(x=1, y=1, z=0.5)
            ),
            width=900,
            height=700
        )
        
        return fig
        
    def create_depth_heatmap(self, depth_map: np.ndarray,
                            mask: Optional[np.ndarray] = None,
                            title: str = "Depth Heatmap") -> go.Figure:
        """
        Create 2D heatmap of depth data.
        
        Args:
            depth_map: 2D depth map array
            mask: Optional binary mask
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        if mask is not None:
            depth_data = depth_map.copy()
            depth_data[mask == 0] = np.nan
        else:
            depth_data = depth_map
            
        fig = go.Figure(data=go.Heatmap(
            z=depth_data,
            colorscale='Viridis',
            colorbar=dict(title='Depth (cm)')
        ))
        
        fig.update_layout(
            title=title,
            xaxis_title='X (pixels)',
            yaxis_title='Y (pixels)',
            width=800,
            height=600
        )
        
        return fig
        
    def analyze_plate_depth(self, depth_map: np.ndarray,
                          plate_mask: np.ndarray) -> Dict:
        """
        Analyze depth values in plate region for validation.
        
        Args:
            depth_map: Depth map array
            plate_mask: Binary mask of plate region
            
        Returns:
            Dict containing analysis results
        """
        # Extract plate depth values
        plate_depths = depth_map[plate_mask > 0]
        
        if len(plate_depths) == 0:
            raise ValueError("No valid depth values in plate region")
            
        # Calculate statistics
        stats = {
            'mean_depth': float(np.mean(plate_depths)),
            'std_depth': float(np.std(plate_depths)),
            'min_depth': float(np.min(plate_depths)),
            'max_depth': float(np.max(plate_depths)),
            'num_points': int(len(plate_depths)),
            'expected_depth': self.expected_plate_distance,
            'depth_error': float(np.mean(plate_depths) - self.expected_plate_distance)
        }
        
        # Calculate planarity
        if len(plate_depths) > 3:
            planarity = self._calculate_planarity(depth_map, plate_mask)
            stats['planarity_error'] = float(planarity)
            
        return stats
        
    def _calculate_planarity(self, depth_map: np.ndarray, 
                           mask: np.ndarray) -> float:
        """Calculate RMSE from fitted plane."""
        # Get coordinates of valid points
        ys, xs = np.nonzero(mask)
        depths = depth_map[mask > 0]
        
        # Fit plane using least squares
        A = np.column_stack([xs, ys, np.ones_like(xs)])
        plane_params, _, _, _ = np.linalg.lstsq(A, depths, rcond=None)
        
        # Calculate error from plane
        fitted_depths = A @ plane_params
        rmse = np.sqrt(np.mean((depths - fitted_depths) ** 2))
        
        return rmse
        
    def create_depth_profile(self, depth_map: np.ndarray,
                           start_point: Tuple[int, int],
                           end_point: Tuple[int, int],
                           title: str = "Depth Profile") -> go.Figure:
        """
        Create line plot showing depth values along a line.
        
        Args:
            depth_map: Depth map array
            start_point: (x, y) starting point
            end_point: (x, y) ending point
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        # Extract points along line
        num_points = 100
        x = np.linspace(start_point[0], end_point[0], num_points).astype(int)
        y = np.linspace(start_point[1], end_point[1], num_points).astype(int)
        
        # Get depth values
        depths = depth_map[y, x]
        
        # Create distance array
        distances = np.sqrt(
            (x - start_point[0])**2 + 
            (y - start_point[1])**2
        )
        
        fig = go.Figure(data=go.Scatter(
            x=distances,
            y=depths,
            mode='lines+markers',
            name='Depth Profile'
        ))
        
        fig.update_layout(
            title=title,
            xaxis_title='Distance along line (pixels)',
            yaxis_title='Depth (cm)',
            width=800,
            height=500
        )
        
        return fig
        
    def save_visualization(self, fig: go.Figure, 
                          output_path: Path,
                          filename: str) -> None:
        """Save visualization as HTML file."""
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        fig.write_html(str(output_dir / filename))
        logger.info(f"Saved visualization to {output_dir / filename}")

class ReconstructionVisualizer:
    """Visualization tools for 3D reconstruction analysis."""
    
    def __init__(self):
        """Initialize with default color scheme."""
        self.colors = ['rgb(31, 119, 180)', 'rgb(255, 127, 14)', 
                      'rgb(44, 160, 44)', 'rgb(214, 39, 40)']
                      
    def plot_point_cloud(self, points: np.ndarray, 
                        title: str = "3D Point Cloud",
                        color: Optional[str] = None) -> go.Figure:
        """
        Create interactive 3D scatter plot of point cloud.
        
        Args:
            points: Nx3 array of 3D points
            title: Plot title
            color: Optional color for points
            
        Returns:
            plotly.graph_objects.Figure
        """
        fig = go.Figure(data=[go.Scatter3d(
            x=points[:, 0],
            y=points[:, 1],
            z=points[:, 2],
            mode='markers',
            marker=dict(
                size=2,
                color=color or self.colors[0],
                opacity=0.8
            )
        )])
        
        fig.update_layout(
            title=title,
            scene=dict(
                xaxis_title='X (cm)',
                yaxis_title='Y (cm)',
                zaxis_title='Z (cm)',
                aspectmode='data'
            )
        )
        return fig
        
    def plot_multiple_objects(self, objects: Dict[str, np.ndarray],
                            title: str = "Multi-object Reconstruction") -> go.Figure:
        """
        Create interactive 3D plot of multiple point clouds.
        
        Args:
            objects: Dictionary mapping object names to point clouds
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        fig = go.Figure()
        
        for i, (name, points) in enumerate(objects.items()):
            fig.add_trace(go.Scatter3d(
                x=points[:, 0],
                y=points[:, 1],
                z=points[:, 2],
                mode='markers',
                name=name,
                marker=dict(
                    size=2,
                    color=self.colors[i % len(self.colors)],
                    opacity=0.8
                )
            ))
            
        fig.update_layout(
            title=title,
            scene=dict(
                xaxis_title='X (cm)',
                yaxis_title='Y (cm)',
                zaxis_title='Z (cm)',
                aspectmode='data'
            )
        )
        return fig
        
    def plot_volume_estimation(self, points: np.ndarray, 
                             hull_vertices: np.ndarray,
                             volume: float,
                             title: Optional[str] = None) -> go.Figure:
        """
        Create interactive 3D visualization of volume estimation.
        
        Args:
            points: Nx3 array of 3D points
            hull_vertices: Indices of convex hull vertices
            volume: Calculated volume in cubic centimeters
            title: Optional plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        fig = go.Figure()
        
        # Original points
        fig.add_trace(go.Scatter3d(
            x=points[:, 0],
            y=points[:, 1],
            z=points[:, 2],
            mode='markers',
            name='Points',
            marker=dict(size=2, color='blue', opacity=0.6)
        ))
        
        # Convex hull
        fig.add_trace(go.Mesh3d(
            x=points[hull_vertices, 0],
            y=points[hull_vertices, 1],
            z=points[hull_vertices, 2],
            opacity=0.3,
            color='red',
            name='Convex Hull'
        ))
        
        plot_title = title or f"Volume Estimation: {volume:.2f} cm³"
        fig.update_layout(
            title=plot_title,
            scene=dict(
                xaxis_title='X (cm)',
                yaxis_title='Y (cm)',
                zaxis_title='Z (cm)',
                aspectmode='data'
            )
        )
        return fig
        
    def save_visualization(self, fig: go.Figure,
                         output_dir: Path,
                         filename: str) -> None:
        """
        Save visualization as HTML file.
        
        Args:
            fig: Plotly figure object
            output_dir: Output directory path
            filename: Output filename (should end with .html)
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_path = output_dir / filename
        fig.write_html(str(output_path))
        logger.info(f"Saved visualization to {output_path}")

================
File: src/__init__.py
================
from .preprocessing import PreprocessingPipeline
from .core import DepthProcessor, ImageAligner
from .utils import CocoHandler

================
File: test/inspect_data.py
================
import sys
import os
from pathlib import Path
import numpy as np
import cv2
import json
from typing import Dict
import logging

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def inspect_raw_file(file_path: Path) -> Dict:
    """Analyze raw depth file"""
    try:
        # Try different data types
        data_types = [np.float32, np.uint16, np.uint8]
        results = {}
        
        for dtype in data_types:
            try:
                raw_data = np.fromfile(file_path, dtype=dtype)
                results[dtype.__name__] = {
                    'size': raw_data.size,
                    'min': float(raw_data.min()),
                    'max': float(raw_data.max()),
                    'possible_shapes': [
                        f"{s} x {raw_data.size // s}" 
                        for s in range(1, raw_data.size + 1) 
                        if raw_data.size % s == 0
                    ][:5]  # Show first 5 possible shapes
                }
            except Exception as e:
                logger.warning(f"Could not read as {dtype.__name__}: {str(e)}")
                continue
        
        return results
    except Exception as e:
        logger.error(f"Error analyzing raw file: {str(e)}")
        return {}

def inspect_rgb_file(file_path: Path) -> Dict:
    """Analyze RGB image file"""
    try:
        img = cv2.imread(str(file_path))
        if img is None:
            raise ValueError("Could not read image file")
            
        return {
            'shape': img.shape,
            'dtype': img.dtype,
            'min_val': float(img.min()),
            'max_val': float(img.max())
        }
    except Exception as e:
        logger.error(f"Error analyzing RGB file: {str(e)}")
        return {}

def inspect_coco_file(file_path: Path) -> Dict:
    """Analyze COCO annotation file"""
    try:
        with open(file_path, 'r') as f:
            coco_data = json.load(f)
            
        return {
            'num_images': len(coco_data.get('images', [])),
            'num_annotations': len(coco_data.get('annotations', [])),
            'categories': [cat['name'] for cat in coco_data.get('categories', [])],
            'image_shapes': [
                (img['height'], img['width']) 
                for img in coco_data.get('images', [])
            ]
        }
    except Exception as e:
        logger.error(f"Error analyzing COCO file: {str(e)}")
        return {}

def main():
    try:
        data_dir = Path("data")
        
        # Inspect first raw file
        raw_files = list((data_dir / "rgbd").glob("*.raw"))
        if raw_files:
            logger.info("\nAnalyzing RAW file:")
            raw_results = inspect_raw_file(raw_files[0])
            logger.info(f"File: {raw_files[0].name}")
            for dtype, info in raw_results.items():
                logger.info(f"\nAs {dtype}:")
                logger.info(f"Size: {info['size']}")
                logger.info(f"Range: {info['min']} to {info['max']}")
                logger.info(f"Possible dimensions: {info['possible_shapes']}")
        else:
            logger.warning("No .raw files found")
        
        # Inspect first PNG file
        png_files = list((data_dir / "segmented").glob("*.png"))
        if png_files:
            logger.info("\nAnalyzing RGB file:")
            rgb_results = inspect_rgb_file(png_files[0])
            logger.info(f"File: {png_files[0].name}")
            logger.info(f"Shape: {rgb_results.get('shape')}")
            logger.info(f"Data type: {rgb_results.get('dtype')}")
            logger.info(f"Value range: {rgb_results.get('min_val')} to {rgb_results.get('max_val')}")
        else:
            logger.warning("No .png files found")
        
        # Inspect COCO file
        coco_file = data_dir / "segmented" / "_annotations.coco.json"
        if coco_file.exists():
            logger.info("\nAnalyzing COCO file:")
            coco_results = inspect_coco_file(coco_file)
            logger.info(f"Number of images: {coco_results.get('num_images')}")
            logger.info(f"Number of annotations: {coco_results.get('num_annotations')}")
            logger.info(f"Categories: {coco_results.get('categories')}")
            logger.info(f"Image shapes: {coco_results.get('image_shapes')}")
        else:
            logger.warning("COCO annotation file not found")
            
    except Exception as e:
        logger.error(f"Error in main: {str(e)}")

if __name__ == "__main__":
    main()

================
File: test/test_point_cloud.py
================
import sys
from pathlib import Path
import numpy as np
import pytest
import json
import logging

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.reconstruction.point_cloud import PointCloud
from src.preprocessing.preprocessing import PreprocessingPipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@pytest.fixture
def preprocessing_output():
    """Load preprocessed data from the pipeline"""
    try:
        # Load test configuration
        with open('test_config.json', 'r') as f:
            config = json.load(f)
            
        # Get the first frame_id
        frame_id = config['frame_ids'][0]
        
        # Initialize pipeline
        pipeline = PreprocessingPipeline(config)
        
        # Process single image
        result = pipeline.process_single_image(frame_id)
        return result
        
    except Exception as e:
        logger.error(f"Error loading preprocessing output: {str(e)}")
        raise

def test_point_cloud_with_preprocessed_data(preprocessing_output):
    """Test point cloud generation using preprocessed data"""
    # Initialize point cloud with calibrated parameters
    pc = PointCloud(preprocessing_output['intrinsic_params'])
    
    # Generate point cloud from depth data
    points = pc.depth_to_point_cloud(
        preprocessing_output['depth'],
        None  # Full depth map
    )
    
    assert isinstance(points, np.ndarray)
    assert points.shape[1] == 3  # X, Y, Z coordinates
    assert not np.any(np.isnan(points))
    assert not np.any(np.isinf(points))
    
    logger.info(f"Generated point cloud with {len(points)} points")

def test_object_specific_point_clouds(preprocessing_output):
    """Test generating point clouds for individual objects"""
    pc = PointCloud(preprocessing_output['intrinsic_params'])
    
    for obj_name, obj_data in preprocessing_output['processed_objects'].items():
        # Generate point cloud for specific object
        obj_points = pc.depth_to_point_cloud(
            preprocessing_output['depth'],
            obj_data['mask']
        )
        
        assert len(obj_points) > 0
        assert not np.any(np.isnan(obj_points))
        
        # Calculate basic measurements
        dimensions = pc.get_dimensions(obj_points)
        assert all(v > 0 for v in dimensions.values())
        
        # Basic sanity checks for plate
        if obj_name == 'plate':
            # Check if dimensions roughly match expected plate size
            plate_diameter = preprocessing_output['intrinsic_params']['reference_object']['diameter']
            max_dim = max(dimensions['length'], dimensions['width'])
            assert np.isclose(max_dim, plate_diameter, rtol=0.2)
            
        logger.info(f"Processed object {obj_name}: {len(obj_points)} points")

def test_volume_estimation_with_real_data(preprocessing_output):
    """Test volume estimation with preprocessed data"""
    pc = PointCloud(preprocessing_output['intrinsic_params'])
    
    for obj_name, obj_data in preprocessing_output['processed_objects'].items():
        # Skip plate (reference object)
        if obj_name == 'plate':
            continue
            
        # Generate point cloud
        points = pc.depth_to_point_cloud(
            preprocessing_output['depth'],
            obj_data['mask']
        )
        
        # Calculate volume
        volume = pc.estimate_volume(points)
        assert volume > 0
        
        # Calculate surface area
        surface_area = pc.calculate_surface_area(points)
        assert surface_area > 0
        
        logger.info(
            f"Object {obj_name} measurements:\n"
            f"Volume: {volume:.2f} cm³\n"
            f"Surface Area: {surface_area:.2f} cm²"
        )

def test_data_consistency(preprocessing_output):
    """Test consistency between depth and mask data"""
    depth_shape = preprocessing_output['depth'].shape
    
    for obj_data in preprocessing_output['processed_objects'].values():
        assert obj_data['mask'].shape == depth_shape
        
        # Check if mask and depth alignment makes sense
        masked_depth = preprocessing_output['depth'][obj_data['mask'] > 0]
        assert len(masked_depth) > 0
        assert np.all(masked_depth > 0)  # No invalid depth values in mask
def test_reference_object_validation(preprocessing_output):
    """Validate measurements using reference plate"""
    pc = PointCloud(preprocessing_output['intrinsic_params'])
    
    # Get plate data
    plate_data = next(
        (data for name, data in preprocessing_output['processed_objects'].items() 
         if name == 'plate'),
        None
    )
    
    if plate_data is not None:
        # Generate plate point cloud
        plate_points = pc.depth_to_point_cloud(
            preprocessing_output['depth'],
            plate_data['mask']
        )
        
        # Get plate dimensions
        dims = pc.get_dimensions(plate_points)
        
        # Get plate center height
        center_height = preprocessing_output['intrinsic_params']['camera_height']
        center_depth = preprocessing_output['depth'][
            preprocessing_output['intrinsic_params']['reference_object']['center_pixels'][1],
            preprocessing_output['intrinsic_params']['reference_object']['center_pixels'][0]
        ]
        
        # Check reference height
        assert np.isclose(center_depth, center_height, rtol=0.2)
        
        # Check against known plate diameter with larger tolerance due to perspective effects
        plate_diameter = preprocessing_output['intrinsic_params']['reference_object']['diameter']
        max_dim = max(dims['length'], dims['width'])
        assert np.isclose(max_dim, plate_diameter, rtol=0.3)
        
        logger.info(
            f"Plate validation:\n"
            f"Expected diameter: {plate_diameter:.2f}cm\n"
            f"Measured diameter: {max_dim:.2f}cm\n"
            f"Center height: {center_depth:.2f}cm"
        )

================
File: test/test_preprocessing.py
================
import sys
import os
from pathlib import Path
import logging
import json

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.preprocessing import PreprocessingPipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_first_files():
    try:
        # Get first .raw file from rgbd directory
        rgbd_dir = Path("data/rgbd")
        raw_files = list(rgbd_dir.glob("*.raw"))
        if not raw_files:
            raise FileNotFoundError("No .raw files found in rgbd directory")
        raw_file = raw_files[0]
        
        # Extract frame_id from filename
        frame_id = raw_file.stem.replace("depth_frame_", "")
        
        # Get corresponding files
        segmented_dir = Path("data/segmented")
        coco_file = next(segmented_dir.glob("*_annotations.coco.json"))
        png_file = next(segmented_dir.glob(f"rgb_frame_{frame_id}.png"))
        
        logger.info(f"Found files:")
        logger.info(f"RAW: {raw_file.name}")
        logger.info(f"COCO: {coco_file.name}")
        logger.info(f"PNG: {png_file.name}")
        
        return {
            'frame_id': frame_id,
            'raw_path': str(raw_file),
            'coco_path': str(coco_file),
            'png_path': str(png_file)
        }
        
    except Exception as e:
        logger.error(f"Error finding files: {str(e)}")
        raise
def create_test_config(files):
    config = {
        "data_dir": str(Path("data")),
        "output_dir": str(Path("data/upscaled")),
        "coco_file": files['coco_path'],
        "frame_ids": [files['frame_id']],
        "camera_height": 33.0,
        "plate_diameter": 25.5,
        "plate_height": 0.7
    }
    
    config_path = Path("test_config.json")
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=4)
    
    return str(config_path)
def test_preprocessing():
    try:
        files = get_first_files()
        
        config_path = create_test_config(files)
        logger.info(f"Created config file: {config_path}")
        
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        pipeline = PreprocessingPipeline(config)
        
        logger.info("Testing data loading...")
        data = pipeline.load_data(files['frame_id'])
        logger.info("Data loading successful")
        
        logger.info("Testing full preprocessing pipeline...")
        result = pipeline.process_single_image(files['frame_id'])
        logger.info("Preprocessing completed successfully")
        
        upscaled_dir = Path("data/upscaled")
        if upscaled_dir.exists():
            output_files = list(upscaled_dir.glob("*"))
            logger.info(f"Files generated in upscaled directory: {[f.name for f in output_files]}")
        
        return True
        
    except Exception as e:
        logger.error(f"Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_preprocessing()
    if success:
        print("All tests passed!")
    else:
        print("Tests failed!")

================
File: test/test_reconstruction.py
================
import sys
from pathlib import Path
import numpy as np
import pytest
import trimesh
import json
import logging

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.reconstruction.volume_calculator import VolumeCalculator
from src.reconstruction.mesh_generator import MeshGenerator
from src.reconstruction.point_cloud import PointCloud
from src.preprocessing.preprocessing import PreprocessingPipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@pytest.fixture
def preprocessed_data():
    """Load preprocessed data from pipeline"""
    try:
        with open('test_config.json', 'r') as f:
            config = json.load(f)
            
        frame_id = config['frame_ids'][0]
        
        pipeline = PreprocessingPipeline(config)
        result = pipeline.process_single_image(frame_id)
        
        return result
        
    except Exception as e:
        logger.error(f"Error loading preprocessing output: {str(e)}")
        raise

class TestVolumeCalculator:
    def test_food_volume_estimation(self, preprocessed_data):
        """Test volume calculation for food items"""
        pc = PointCloud(preprocessed_data['intrinsic_params'])
        calc = VolumeCalculator()
        
        results = {}
        
        for obj_name, obj_data in preprocessed_data['processed_objects'].items():
            if obj_name == 'plate':
                continue
                
            points = pc.depth_to_point_cloud(
                preprocessed_data['depth'],
                obj_data['mask']
            )
            
            volumes = {}
            for method in ['convex_hull', 'grid', 'alpha_shape']:
                try:
                    result = calc.calculate_volume(points, method=method)
                    volumes[method] = result
                except Exception as e:
                    logger.warning(f"Method {method} failed for {obj_name}: {str(e)}")
                    
            results[obj_name] = volumes
            
            logger.info(f"\nVolume estimation for {obj_name}:")
            for method, result in volumes.items():
                logger.info(
                    f"{method}: {result['volume_cm3']:.2f}cm³ "
                    f"(±{result['uncertainty_cm3']:.2f}) "
                    f"= {result['volume_cups']:.2f} cups "
                    f"(±{result['uncertainty_cups']:.2f})"
                )
        
        assert len(results) > 0

    def test_method_consistency(self, preprocessed_data):
        """Test consistency between different volume calculation methods"""
        pc = PointCloud(preprocessed_data['intrinsic_params'])
        calc = VolumeCalculator()
        
        for obj_name, obj_data in preprocessed_data['processed_objects'].items():
            if obj_name == 'plate':
                continue
                
            points = pc.depth_to_point_cloud(
                preprocessed_data['depth'],
                obj_data['mask']
            )
            
            convex_result = calc.calculate_volume(points, 'convex_hull')
            grid_result = calc.calculate_volume(points, 'grid')
            
            assert np.isclose(
                convex_result['volume_cm3'],
                grid_result['volume_cm3'],
                rtol=0.5
            )

class TestMeshGenerator:
    def test_food_mesh_generation(self, preprocessed_data):
        """Test mesh generation for food items"""
        pc = PointCloud(preprocessed_data['intrinsic_params'])
        generator = MeshGenerator()
        
        results = {}
        
        for obj_name, obj_data in preprocessed_data['processed_objects'].items():
            if obj_name == 'plate':
                continue
                
            points = pc.depth_to_point_cloud(
                preprocessed_data['depth'],
                obj_data['mask']
            )
            
            meshes = {}
            for method in ['convex', 'surface', 'alpha']:
                try:
                    mesh = generator.generate_mesh(points, method=method)
                    validation = generator.validate_mesh(mesh)
                    meshes[method] = {
                        'mesh': mesh,
                        'validation': validation
                    }
                except Exception as e:
                    logger.warning(f"Method {method} failed for {obj_name}: {str(e)}")
                    
            results[obj_name] = meshes
            
            logger.info(f"\nMesh generation for {obj_name}:")
            for method, result in meshes.items():
                mesh = result['mesh']
                validation = result['validation']
                logger.info(
                    f"{method}: {len(mesh.vertices)} vertices, "
                    f"{len(mesh.faces)} faces\n"
                    f"Validation: {validation}"
                )
        
        assert len(results) > 0

    def test_mesh_volume_consistency(self, preprocessed_data):
        """Test consistency between mesh volume and direct calculation"""
        pc = PointCloud(preprocessed_data['intrinsic_params'])
        generator = MeshGenerator()
        calc = VolumeCalculator()
        
        for obj_name, obj_data in preprocessed_data['processed_objects'].items():
            if obj_name == 'plate':
                continue
                
            points = pc.depth_to_point_cloud(
                preprocessed_data['depth'],
                obj_data['mask']
            )
            
            volume_result = calc.calculate_volume(points, 'convex_hull')
            
            mesh = generator.generate_mesh(points, 'convex')
            mesh_volume = mesh.volume
            
            assert np.isclose(
                volume_result['volume_cm3'],
                mesh_volume,
                rtol=0.1
            )

def test_full_reconstruction_pipeline(preprocessed_data):
    """Test the complete reconstruction pipeline"""
    pc = PointCloud(preprocessed_data['intrinsic_params'])
    generator = MeshGenerator()
    calc = VolumeCalculator()
    
    reconstruction_results = {}
    
    for obj_name, obj_data in preprocessed_data['processed_objects'].items():
        if obj_name == 'plate':
            continue
            
        try:
            points = pc.depth_to_point_cloud(
                preprocessed_data['depth'],
                obj_data['mask']
            )
            
            mesh = generator.generate_mesh(points, 'surface')
            validation = generator.validate_mesh(mesh)
            
            volume_result = calc.calculate_volume(points, 'convex_hull')
            
            reconstruction_results[obj_name] = {
                'num_points': len(points),
                'mesh_vertices': len(mesh.vertices),
                'mesh_faces': len(mesh.faces),
                'mesh_validation': validation,
                'volume_cm3': volume_result['volume_cm3'],
                'volume_cups': volume_result['volume_cups'],
                'uncertainty_cups': volume_result['uncertainty_cups']
            }
            
        except Exception as e:
            logger.error(f"Reconstruction failed for {obj_name}: {str(e)}")
            
    logger.info("\nFull reconstruction results:")
    for obj_name, result in reconstruction_results.items():
        logger.info(f"\n{obj_name}:")
        logger.info(f"Points: {result['num_points']}")
        logger.info(f"Mesh: {result['mesh_vertices']} vertices, {result['mesh_faces']} faces")
        logger.info(f"Volume: {result['volume_cups']:.2f} cups (±{result['uncertainty_cups']:.2f})")
        logger.info(f"Mesh validation: {result['mesh_validation']}")
        
    assert len(reconstruction_results) > 0

================
File: .gitignore
================
/venv
/repopack-output.txt

================
File: README.md
================
# M-PCAM Model

## Getting started
  - start a virtual environment
  
  to run the system ``` python run_volume_estimation.py``` 
  


## shit to improve on
- initialize the flask app
- add function to call api for the macronutrient database.

================
File: requirements.txt
================
numpy==2.1.3
opencv-python==4.10.0.84
pathlib==1.0.1
iniconfig==2.0.0
numpy==2.1.3
opencv-python==4.10.0.84
packaging==24.2
pathlib==1.0.1
pluggy==1.5.0
pytest==8.3.4
scipy==1.14.1
trimesh==4.5.3
iniconfig==2.0.0
joblib==1.4.2
networkx==3.4.2
numpy==2.1.3
opencv-python==4.10.0.84
packaging==24.2
pathlib==1.0.1
pluggy==1.5.0
pytest==8.3.4
scikit-learn==1.5.2
scipy==1.14.1
threadpoolctl==3.5.0
trimesh==4.5.3

================
File: run_preprocessing.py
================
import os
import sys
from pathlib import Path
import argparse
import logging

# Add the project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.preprocessing.preprocessing import run_preprocessing
from src.utils.logging_utils import setup_logging

def main():
    parser = argparse.ArgumentParser(description="Run preprocessing pipeline")
    parser.add_argument('--config', required=True, help='Path to config file')
    args = parser.parse_args()
    
    # Set up logging
    logger = setup_logging(
        log_dir=project_root / "logs/preprocessing",
        log_prefix="preprocess"
    )
    
    try:
        logger.info(f"Starting preprocessing pipeline with config: {args.config}")
        run_preprocessing(args.config)
        logger.info("Preprocessing pipeline completed successfully")
        
    except Exception as e:
        logger.error(f"Pipeline execution failed: {str(e)}", exc_info=True)
        return 1

if __name__ == "__main__":
    main()

================
File: run_tests.py
================
import os
import sys
from pathlib import Path
import pytest
import logging

# Add the project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils.logging_utils import setup_logging

def main():
    # Set up logging
    logger = setup_logging(
        log_dir=project_root / "logs/tests",
        log_prefix="test"
    )
    
    try:
        logger.info("Starting test suite execution")
        
        # Run preprocessing tests first
        logger.info("\nRunning preprocessing tests...")
        preprocessing_result = pytest.main([
            str(project_root / 'test/test_preprocessing.py'),
            '-v',
            '--capture=tee-sys'
        ])
        
        if preprocessing_result == 0:
            logger.info("\nPreprocessing tests passed. Running reconstruction tests...")
            # Run point cloud and reconstruction tests
            reconstruction_result = pytest.main([
                str(project_root / 'test/test_point_cloud.py'),
                str(project_root / 'test/test_reconstruction.py'),
                '-v',
                '--capture=tee-sys'
            ])
            
            if reconstruction_result == 0:
                logger.info("\nAll tests passed successfully!")
            else:
                logger.error("\nReconstruction tests failed!")
        else:
            logger.error("\nPreprocessing tests failed!")
            
    except Exception as e:
        logger.error(f"Test execution failed: {str(e)}", exc_info=True)
        return 1
        
    finally:
        logger.info("Test suite execution completed")

if __name__ == "__main__":
    main()

================
File: run_volume_estimation.py
================
import sys
from pathlib import Path
import argparse
import logging
import json
import numpy as np

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.preprocessing.preprocessing import PreprocessingPipeline
from src.reconstruction.volume_calculator import VolumeCalculator
from src.utils.logging_utils import setup_logging

def main():
    parser = argparse.ArgumentParser(description="Run volume estimation pipeline")
    parser.add_argument('--config', required=True, help='Path to config file')
    args = parser.parse_args()
    
    # Setup logging
    logger = setup_logging(log_dir="logs", log_prefix="volume_estimation")
    
    try:
        # Load config
        with open(args.config, 'r') as f:
            config = json.load(f)
        
        # 1. Run preprocessing
        logger.info("Starting preprocessing pipeline...")
        pipeline = PreprocessingPipeline(config)
        
        for frame_id in config['frame_ids']:
            # Process single image
            result = pipeline.process_single_image(frame_id)
            
            # 2. Calculate volumes
            logger.info("\nCalculating volumes...")
            calc = VolumeCalculator(
                camera_height=config.get('camera_height', 33.0),
                plate_diameter=config.get('plate_diameter', 25.5)
            )
            
            # Get plate data
            plate_data = next(
                (data for name, data in result['processed_objects'].items() 
                 if name == 'plate'),
                None
            )
            
            if plate_data is None:
                raise ValueError("No plate found in processed objects")
            
            # Get calibration and measured plate height
            calibration = calc.calculate_plate_reference(
                depth_map=result['depth'],
                plate_mask=plate_data['mask'],
                intrinsic_params=result['intrinsic_params']
            )
            
            plate_height = calibration['plate_height']
            
            # Calculate volumes for each food item
            volume_results = {}
            for obj_name, obj_data in result['processed_objects'].items():
                if obj_name == 'plate':
                    continue
                    
                logger.info(f"\nProcessing {obj_name}...")
                volume_data = calc.calculate_volume(
                    depth_map=result['depth'],
                    mask=obj_data['mask'],
                    plate_height=plate_height,  # Use measured plate height
                    intrinsic_params=result['intrinsic_params'],
                    calibration=calibration
                )
                
                volume_results[obj_name] = volume_data
            
            # Save results
            output_dir = Path(config['output_dir'])
            output_dir.mkdir(parents=True, exist_ok=True)
            
            output_file = output_dir / f"volumes_{frame_id}.json"
            with open(output_file, 'w') as f:
                json.dump(volume_results, f, indent=2)
                
            logger.info(f"\nResults saved to {output_file}")
            logger.info("\nVolume Summary:")
            for obj_name, data in volume_results.items():
                logger.info(
                    f"{obj_name}: {data['volume_cups']:.2f} cups "
                    f"(±{data['uncertainty_cups']:.2f})"
                )
                
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}", exc_info=True)
        return 1

if __name__ == "__main__":
    main()

================
File: test_config.json
================
{
    "data_dir": "data",
    "output_dir": "data/upscaled",
    "coco_file": "data/segmented/_annotations.coco.json",
    "frame_ids": [
        "20241204_233746"
    ],
    "camera_height": 33.0,
    "plate_diameter": 25.5,
    "plate_height": 0.7
}
