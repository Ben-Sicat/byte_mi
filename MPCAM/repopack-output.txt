This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-12-02T20:59:06.698Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
data/
  rgbd/
    depth_frame_20241127_181653.meta
  segmented/
    _annotations.coco.json
    rgb_frame_20241127_181653.meta
src/
  core/
    __init__.py
    depth_processor.py
    image_alignment.py
  preprocessing/
    __init__.py
    calibration.py
    noise_reduction.py
    preprocessing.py
  reconstruction/
    mesh_generator.py
    point_cloud.py
    volume_calculator.py
  utils/
    __init__.py
    coco_utils.py
    io_utils.py
    visualization.py
  __init__.py
test/
  test_preprocessing.py
test_config.json

================================================================
Repository Files
================================================================

================
File: data/rgbd/depth_frame_20241127_181653.meta
================
{
    "width": 160,
    "height": 90,
    "format": 5,
    "timestamp": 1096946.873597841,
    "centerPixelDepth": 0.42500001192092898,
    "minDepth": 0.42399999499320986,
    "maxDepth": 0.4259999990463257
}

================
File: data/segmented/_annotations.coco.json
================
{"info":{"year":"2024","version":"1","description":"Exported from roboflow.com","contributor":"","url":"https://public.roboflow.com/object-detection/undefined","date_created":"2024-12-01T05:29:15+00:00"},"licenses":[{"id":1,"url":"https://creativecommons.org/licenses/by/4.0/","name":"CC BY 4.0"}],"categories":[{"id":0,"name":"food","supercategory":"none"},{"id":1,"name":"case","supercategory":"food"}],"images":[{"id":0,"license":1,"file_name":"rgb_frame_20241127_181653_png.rf.5b5ef19c841d0b9b98ee1d0d032584cf.jpg","height":480,"width":640,"date_captured":"2024-12-01T05:29:15+00:00"}],"annotations":[{"id":0,"image_id":0,"category_id":1,"bbox":[149,172,295,139],"area":41005,"segmentation":[[150,203,149,295,157,309,430,311,443,306,444,184,437,175,425,172,161,175,152,182]],"iscrowd":0}]}

================
File: data/segmented/rgb_frame_20241127_181653.meta
================
{
    "width": 640,
    "height": 480,
    "timestamp": 1096946.873597841
}

================
File: src/core/__init__.py
================
from .depth_processor import DepthProcessor
from .image_alignment import ImageAligner

================
File: src/core/depth_processor.py
================
import numpy as np
import cv2
from typing import Tuple, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthProcessor:
    def __init__(self):
        """Initialize the depth processor"""
        self.rgbd_data = None
        self.original_shape = None
    
    def load_raw_rgbd(self, file_path: str, shape: Tuple[int, int]) -> np.ndarray:
        """
        Load RGBD data from .raw file
        """
        try:
            # load raw data (expects float32 format)
            raw_data = np.fromfile(file_path, dtype=np.float32)
            
            # calculate expected size (height * width * 4 channels)
            expected_size = shape[0] * shape[1] * 4
            
            if raw_data.size != expected_size:
                raise ValueError(
                    f"Raw data size {raw_data.size} does not match expected size {expected_size}"
                )
            
            # reshape to rgbd format
            rgbd_image = raw_data.reshape((*shape, 4))
            
            self.rgbd_data = rgbd_image
            self.original_shape = shape
            
            logger.info(f"Loaded RGBD image with shape {rgbd_image.shape}")
            return rgbd_image
            
        except Exception as e:
            logger.error(f"Error loading raw RGBD file: {str(e)}")
            raise

    def upscale_rgbd(self, target_shape: Tuple[int, int]) -> np.ndarray:
        """
        upscale rgbd image while properly handling depth data
        """
        if self.rgbd_data is None:
            raise ValueError("No RGBD data loaded. Call load_raw_rgbd first.")
            
        original_h, original_w = self.original_shape
        target_h, target_w = target_shape
        
        # calculate scaling factors
        scale_h = target_h / original_h
        scale_w = target_w / original_w
        
        # split rgb and depth channels
        rgb_channels = self.rgbd_data[:, :, :3]
        depth_channel = self.rgbd_data[:, :, 3]
        
        # upscale rgb using bilinear interpolation
        rgb_upscaled = cv2.resize(
            rgb_channels,
            (target_w, target_h),
            interpolation=cv2.INTER_LINEAR
        )
        
        depth_upscaled = self._interpolate_depth(
            depth_channel,
            (target_h, target_w)
        )
        
        # combine channels
        rgbd_upscaled = np.zeros((target_h, target_w, 4), dtype=np.float32)
        rgbd_upscaled[:, :, :3] = rgb_upscaled
        rgbd_upscaled[:, :, 3] = depth_upscaled
        
        return rgbd_upscaled
    
    def _interpolate_depth(self, depth_data: np.ndarray, 
                          target_shape: Tuple[int, int]) -> np.ndarray:
        """
        interpolate depth values for upscaling
        """
        h, w = target_shape
        
        # initial bilinear interpolation
        depth_upscaled = cv2.resize(
            depth_data,
            (w, h),
            interpolation=cv2.INTER_LINEAR
        )
        
        # create validity mask for original depth values
        valid_mask = depth_data > 0
        valid_mask_upscaled = cv2.resize(
            valid_mask.astype(np.uint8),
            (w, h),
            interpolation=cv2.INTER_NEAREST
        ).astype(bool)
        
        # for invalid regions use nearest neighbor 
        depth_nearest = cv2.resize(
            depth_data,
            (w, h),
            interpolation=cv2.INTER_NEAREST
        )
        
        # combine the results
        depth_upscaled[~valid_mask_upscaled] = depth_nearest[~valid_mask_upscaled]
        
        return depth_upscaled

    def get_depth_for_mask(self, depth_data: np.ndarray, 
                          mask: np.ndarray) -> np.ndarray:
        """
        extract depth data for a specific segmentation mask
        """
        if depth_data.shape != mask.shape:
            raise ValueError("Depth data and mask shapes must match")
            
        masked_depth = np.zeros_like(depth_data)
        masked_depth[mask > 0] = depth_data[mask > 0]
        
        return masked_depth

    def validate_depth_values(self, depth_data: np.ndarray, 
                            min_depth: float = 0.0, 
                            max_depth: float = float('inf')) -> np.ndarray:
        """
        validate and clean depth values
        """
        validated_depth = depth_data.copy()
        
        # create mask for invalid values
        invalid_mask = (
            (depth_data < min_depth) |
            (depth_data > max_depth) |
            np.isnan(depth_data)
        )
        
        if np.any(invalid_mask):
            # replace invalid values with nearest valid neighbors
            valid_depths = depth_data[~invalid_mask]
            if len(valid_depths) > 0:
                validated_depth[invalid_mask] = np.median(valid_depths)
            else:
                validated_depth[invalid_mask] = min_depth
                
            logger.warning(f"Found and corrected {np.sum(invalid_mask)} invalid depth values")
            
        return validated_depth

================
File: src/core/image_alignment.py
================
import numpy as np
import cv2
from typing import Tuple, Dict
import logging
from ..utils.coco_utils import CocoHandler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImageAligner:
    """Handles alignment between RGB images, RGBD data, and segmentation masks"""
    def __init__(self, coco_file: str):
        self.rgb_shape = None
        self.rgbd_shape = None
        self.coco_handler = CocoHandler(coco_file)
        
    def set_reference_sizes(self, rgb_shape: Tuple[int, int], 
                           rgbd_shape: Tuple[int, int]) -> None:
        self.rgb_shape = rgb_shape
        self.rgbd_shape = rgbd_shape
        logger.info(f"Set reference shapes - RGB: {rgb_shape}, RGBD: {rgbd_shape}")
        
    def align_rgbd_to_rgb(self, rgbd_data: np.ndarray) -> np.ndarray:
        """Align RGBD data to RGB dimensions"""
        if not (self.rgb_shape and self.rgbd_shape):
            raise ValueError("Reference sizes not set")
            
        if rgbd_data.shape[2] != 4:
            raise ValueError(f"Expected 4 channels in RGBD data, got {rgbd_data.shape[2]}")
            
        # Split and resize channels
        rgb_channels = rgbd_data[:, :, :3]
        depth_channel = rgbd_data[:, :, 3]
        
        aligned_rgb = cv2.resize(rgb_channels, 
                               (self.rgb_shape[1], self.rgb_shape[0]),
                               interpolation=cv2.INTER_LINEAR)
        
        aligned_depth = cv2.resize(depth_channel,
                                 (self.rgb_shape[1], self.rgb_shape[0]),
                                 interpolation=cv2.INTER_LINEAR)
        
        # Combine channels
        aligned_rgbd = np.zeros((*self.rgb_shape, 4), dtype=rgbd_data.dtype)
        aligned_rgbd[:, :, :3] = aligned_rgb
        aligned_rgbd[:, :, 3] = aligned_depth
        
        return aligned_rgbd
        
    def extract_object_depth(self, rgbd_aligned: np.ndarray, 
                           image_id: int,
                           category_name: str) -> Dict[str, np.ndarray]:
        """Extract depth data for specific object category"""
        # Get object mask using COCO handler
        mask = self.coco_handler.create_category_mask(
            image_id, 
            category_name, 
            self.rgb_shape
        )
        
        # Extract depth data
        depth_data = rgbd_aligned[:, :, 3].copy()
        masked_depth = np.zeros_like(depth_data)
        masked_depth[mask > 0] = depth_data[mask > 0]
        
        return {
            'mask': mask,
            'depth': masked_depth,
            'category': category_name
        }

================
File: src/preprocessing/__init__.py
================
from .preprocessing import PreprocessingPipeline
from .calibration import CameraCalibrator
from .noise_reduction import DepthNoiseReducer

================
File: src/preprocessing/calibration.py
================
import numpy as np
import cv2
from typing import Dict, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CameraCalibrator:
    """
    Calculate intrinsic parameters using known measurements and plate as reference.
    All measurements are in centimeters.
    """
    def __init__(self):
        # Known measurements in cm
        self.camera_height = 33.0  
        self.plate_diameter = 25.5  
        self.plate_height = 0.7  
        
        # Will be calculated
        self.focal_length = None
        self.principal_point = None
        self.pixel_size = None
        
    def calculate_focal_length(self, plate_diameter_pixels: float) -> float:
        """
        Calculate focal length using pinhole model and plate as reference.
        f = (P * H) / W
        where:
        f = focal length in pixels
        P = plate diameter in pixels
        H = camera height in cm
        W = actual plate diameter in cm
        """
        focal_length = (plate_diameter_pixels * self.camera_height) / self.plate_diameter
        logger.info(f"Calculated focal length: {focal_length:.2f} pixels")
        return focal_length
        
    def calculate_pixel_size(self, plate_diameter_pixels: float) -> float:
        """
        Calculate pixel size in cm
        pixel_size = actual_size / pixel_size
        """
        pixel_size = self.plate_diameter / plate_diameter_pixels
        logger.info(f"Calculated pixel size: {pixel_size:.6f} cm/pixel")
        return pixel_size

    def get_plate_measurements(self, plate_mask: np.ndarray) -> Dict:
        """
        Get plate measurements from mask in pixels.
        """
        # Find contours of the plate
        contours, _ = cv2.findContours(
            plate_mask.astype(np.uint8),
            cv2.RETR_EXTERNAL,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        if not contours:
            raise ValueError("No plate contour found in mask")
        
        # Get largest contour (plate)
        plate_contour = max(contours, key=cv2.contourArea)
        
        # Find the minimum enclosing circle
        (center_x, center_y), radius = cv2.minEnclosingCircle(plate_contour)
        diameter_pixels = radius * 2
        
        return {
            'center': (center_x, center_y),
            'radius': radius,
            'diameter_pixels': diameter_pixels
        }

    def calculate_intrinsics(self, plate_mask: np.ndarray) -> Dict:
        """
        Calculate all intrinsic parameters using plate mask.
        """
        try:
            # get plate measurements
            plate_info = self.get_plate_measurements(plate_mask)
            
            # calculate focal length
            self.focal_length = self.calculate_focal_length(
                plate_info['diameter_pixels']
            )
            
            # calculate pixel size
            self.pixel_size = self.calculate_pixel_size(
                plate_info['diameter_pixels']
            )
            
            # principal point 
            height, width = plate_mask.shape
            self.principal_point = (width / 2, height / 2)
            
            intrinsic_params = {
                'focal_length': self.focal_length,  # in pixels
                'pixel_size': self.pixel_size,      # cm/pixel
                'principal_point': self.principal_point,
                'image_dimensions': (height, width),
                'camera_height': self.camera_height,
                'reference_object': {
                    'type': 'plate',
                    'diameter': self.plate_diameter,
                    'height': self.plate_height,
                    'measured_diameter_pixels': plate_info['diameter_pixels'],
                    'center_pixels': plate_info['center']
                }
            }
            
            self._validate_parameters(intrinsic_params)
            
            return intrinsic_params
            
        except Exception as e:
            logger.error(f"Error calculating intrinsic parameters: {str(e)}")
            raise

    def _validate_parameters(self, params: Dict) -> None:
        """
        Validate calculated parameters.
        """
        if params['focal_length'] <= 0:
            raise ValueError(f"Invalid focal length: {params['focal_length']}")
            
        if params['pixel_size'] <= 0 or params['pixel_size'] > 1:
            raise ValueError(f"Invalid pixel size: {params['pixel_size']}")
            
        measured_diameter_cm = (
            params['reference_object']['measured_diameter_pixels'] * 
            params['pixel_size']
        )
        error_margin = abs(measured_diameter_cm - self.plate_diameter)
        if error_margin > 1.7:  # More than 1cm error
            logger.warning(
                f"Large error in plate diameter measurement: "
                f"{error_margin:.2f}cm"
            )

    def get_depth_scale_factor(self, plate_depth_values: np.ndarray) -> float:
        """
        Calculate depth scale factor using plate as reference.
        """
        # Expected plate distance from camera
        expected_plate_distance = self.camera_height - self.plate_height
        
        # Use median of plate depth values
        measured_plate_distance = np.median(plate_depth_values)
        
        # Calculate scale factor
        scale_factor = expected_plate_distance / measured_plate_distance
        
        logger.info(f"Depth scale factor: {scale_factor:.4f}")
        return scale_factor

================
File: src/preprocessing/noise_reduction.py
================
import numpy as np
import cv2
from typing import Dict, Optional, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthNoiseReducer:
    """
    Handles noise reduction and cleaning of depth data from RGBD images.
    """
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize with optional configuration parameters.
        
        Args:
            config: Dictionary containing filter parameters:
                - bilateral_d: Diameter of pixel neighborhood
                - bilateral_sigma_color: Filter sigma in color space
                - bilateral_sigma_space: Filter sigma in coordinate space
                - median_kernel: Median filter kernel size
                - outlier_threshold: Standard deviation threshold for outliers
        """
        self.config = config or {
            'bilateral_d': 5,
            'bilateral_sigma_color': 0.1,
            'bilateral_sigma_space': 5.0,
            'median_kernel': 5,
            'outlier_threshold': 2.0
        }
        
    def remove_outliers(self, depth_data: np.ndarray, 
                       mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Remove outlier depth values using statistical analysis.
        
        Args:
            depth_data: Depth channel data
            mask: Optional mask to process specific regions
            
        Returns:
            np.ndarray: Depth data with outliers removed
        """
        if mask is not None:
            valid_depths = depth_data[mask > 0]
        else:
            valid_depths = depth_data[depth_data > 0]
            
        if len(valid_depths) == 0:
            return depth_data
            
        # Calculate statistics
        mean_depth = np.mean(valid_depths)
        std_depth = np.std(valid_depths)
        threshold = std_depth * self.config['outlier_threshold']
        
        # Create outlier mask
        outliers = np.abs(depth_data - mean_depth) > threshold
        
        # Replace outliers with local median
        cleaned_depth = depth_data.copy()
        if np.any(outliers):
            kernel_size = self.config['median_kernel']
            local_median = cv2.medianBlur(
                depth_data.astype(np.float32),
                kernel_size
            )
            cleaned_depth[outliers] = local_median[outliers]
            
            logger.info(f"Removed {np.sum(outliers)} outlier points")
            
        return cleaned_depth
        
    def fill_missing_values(self, depth_data: np.ndarray) -> np.ndarray:
        """
        Fill missing or invalid depth values using interpolation.
        
        Args:
            depth_data: Depth channel data
            
        Returns:
            np.ndarray: Depth data with filled values
        """
        # Create mask of invalid values
        invalid_mask = (depth_data <= 0) | np.isnan(depth_data)
        
        if not np.any(invalid_mask):
            return depth_data
            
        filled_depth = depth_data.copy()
        
        # Use inpainting to fill holes
        filled_depth = cv2.inpaint(
            filled_depth.astype(np.float32),
            invalid_mask.astype(np.uint8),
            3,
            cv2.INPAINT_NS
        )
        
        logger.info(f"Filled {np.sum(invalid_mask)} missing values")
        return filled_depth
        
    def apply_bilateral_filter(self, depth_data: np.ndarray) -> np.ndarray:
        """
        Apply bilateral filtering to reduce noise while preserving edges.
        
        Args:
            depth_data: Depth channel data
            
        Returns:
            np.ndarray: Filtered depth data
        """
        filtered_depth = cv2.bilateralFilter(
            depth_data.astype(np.float32),
            self.config['bilateral_d'],
            self.config['bilateral_sigma_color'],
            self.config['bilateral_sigma_space']
        )
        
        return filtered_depth
        
    def smooth_edges(self, depth_data: np.ndarray, 
                    mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Smooth depth values at object edges.
        
        Args:
            depth_data: Depth channel data
            mask: Optional segmentation mask
            
        Returns:
            np.ndarray: Depth data with smoothed edges
        """
        if mask is not None:
            # Find edges in mask
            edges = cv2.Canny(mask.astype(np.uint8), 100, 200)
            
            # Dilate edges slightly
            kernel = np.ones((3,3), np.uint8)
            edge_region = cv2.dilate(edges, kernel, iterations=1)
            
            # Apply stronger smoothing only to edge regions
            smoothed = cv2.GaussianBlur(
                depth_data.astype(np.float32),
                (5,5),
                1.0
            )
            
            result = depth_data.copy()
            result[edge_region > 0] = smoothed[edge_region > 0]
            
            return result
        
        return depth_data
        
    def process_depth(self, depth_data: np.ndarray,
                     mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Apply complete noise reduction pipeline to depth data.
        
        Args:
            depth_data: Depth channel data
            mask: Optional segmentation mask
            
        Returns:
            np.ndarray: Processed depth data
        """
        logger.info("Starting depth noise reduction")
        
        # Fill missing values first
        filled_depth = self.fill_missing_values(depth_data)
        
        # Remove statistical outliers
        cleaned_depth = self.remove_outliers(filled_depth, mask)
        
        # Apply bilateral filtering
        filtered_depth = self.apply_bilateral_filter(cleaned_depth)
        
        # Smooth edges if mask provided
        if mask is not None:
            final_depth = self.smooth_edges(filtered_depth, mask)
        else:
            final_depth = filtered_depth
            
        logger.info("Completed depth noise reduction")
        return final_depth

================
File: src/preprocessing/preprocessing.py
================
import cv2
import numpy as np
from typing import Dict, Optional
import logging
from pathlib import Path
import json

from ..core.depth_processor import DepthProcessor
from ..core.image_alignment import ImageAligner
from .calibration import CameraCalibrator
from .noise_reduction import DepthNoiseReducer
from ..utils.coco_utils import CocoHandler

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PreprocessingPipeline:
    def __init__(self, config: Dict):
        """
        Initialize preprocessing pipeline with configuration.
        
        Args:
            config: Dict containing:
                - data_dir: Path to data directory
                - coco_file: Path to COCO annotations
                - output_dir: Path to save processed data
                - rgbd_shape: Original RGBD shape (height, width)
                - rgb_shape: RGB reference shape (height, width)
        """
        self.config = config
        self.data_dir = Path(config['data_dir'])
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.depth_processor = DepthProcessor()
        self.image_aligner = ImageAligner(config['coco_file'])
        self.calibrator = CameraCalibrator()
        self.noise_reducer = DepthNoiseReducer()
        self.coco_handler = CocoHandler(config['coco_file'])
        
        self.image_aligner.set_reference_sizes(
            rgb_shape=config['rgb_shape'],
            rgbd_shape=config['rgbd_shape']
        )
        
        logger.info("Initialized preprocessing pipeline")
        
    def load_data(self, frame_id: str) -> Dict:
        """
        Load all necessary data for processing.
        """
        try:
            rgb_path = self.data_dir / 'segmented' / f"rgb_frame_{frame_id}.png"
            if not rgb_path.exists():
                raise FileNotFoundError(f"RGB image not found: {rgb_path}")
                
            rgb_image = cv2.imread(str(rgb_path))
            rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)
            
            rgbd_path = self.data_dir / 'rgbd' / f"depth_frame_{frame_id}.raw"
            if not rgbd_path.exists():
                raise FileNotFoundError(f"RGBD data not found: {rgbd_path}")
                
            rgbd_data = self.depth_processor.load_raw_rgbd(
                str(rgbd_path), 
                self.config['rgbd_shape']
            )
            
            annotations_path = self.data_dir / 'segmented' / '_annotations.coco.json'
            plate_mask = self.coco_handler.create_category_mask(
                frame_id, 
                'plate',
                rgb_image.shape[:2]
            )
            
            return {
                'rgb': rgb_image,
                'rgbd': rgbd_data,
                'plate_mask': plate_mask,
                'frame_id': frame_id
            }
            
        except Exception as e:
            logger.error(f"Error loading data for frame {frame_id}: {str(e)}")
            raise   
    def preprocess_depth(self, rgbd_data: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Apply noise reduction to depth channel.
        
        Args:
            rgbd_data: RGBD image data
            mask: Optional mask for targeted processing
            
        Returns:
            np.ndarray: RGBD data with cleaned depth
        """
        cleaned_rgbd = rgbd_data.copy()
        cleaned_rgbd[:,:,3] = self.noise_reducer.process_depth(
            rgbd_data[:,:,3],
            mask
        )
        return cleaned_rgbd
        
    def process_single_image(self, image_id: int) -> Dict:
        """
        Process a single image through the complete pipeline.
        
        Args:
            image_id: Image identifier
            
        Returns:
            Dict containing processed results
        """
        try:
            logger.info(f"Starting processing for image {image_id}")
            
            # Step 1: Load all necessary data
            data = self.load_data(image_id)
            logger.info("Data loaded successfully")
            
            # Step 2: Clean RGBD depth data
            cleaned_rgbd = self.preprocess_depth(data['rgbd'])
            logger.info("Initial depth cleaning completed")
            
            # Step 3: Calculate camera intrinsics using plate
            intrinsic_params = self.calibrator.calculate_intrinsics(data['plate_mask'])
            logger.info("Camera calibration completed")
            
            # Step 4: Align RGBD to RGB resolution
            aligned_rgbd = self.image_aligner.align_rgbd_to_rgb(cleaned_rgbd)
            logger.info("RGBD alignment completed")
            
            # Step 5: Get depth scale factor using plate
            plate_depth = aligned_rgbd[:,:,3][data['plate_mask'] > 0]
            depth_scale = self.calibrator.get_depth_scale_factor(plate_depth)
            aligned_rgbd[:,:,3] *= depth_scale
            logger.info(f"Depth scaling applied (scale factor: {depth_scale:.4f})")
            
            # Step 6: Process each object
            annotations = self.coco_handler.get_image_annotations(image_id)
            processed_objects = {}
            
            for ann in annotations:
                category_id = ann['category_id']
                category_name = self.coco_handler.categories[category_id]
                
                # Create object mask
                obj_mask = self.coco_handler.create_mask(
                    ann, 
                    aligned_rgbd.shape[:2]
                )
                
                # Extract and clean object depth
                obj_depth = aligned_rgbd[:,:,3].copy()
                obj_depth[obj_mask == 0] = 0
                cleaned_obj_depth = self.noise_reducer.process_depth(
                    obj_depth,
                    obj_mask
                )
                
                processed_objects[category_name] = {
                    'mask': obj_mask,
                    'depth': cleaned_obj_depth,
                    'category_id': category_id,
                    'bbox': ann['bbox']
                }
                
            logger.info(f"Processed {len(processed_objects)} objects")
            
            # Prepare results
            results = {
                'image_id': image_id,
                'intrinsic_params': intrinsic_params,
                'aligned_rgbd': aligned_rgbd,
                'depth_scale': depth_scale,
                'processed_objects': processed_objects
            }
            
            # Save results
            self.save_results(results)
            logger.info(f"Processing completed for image {image_id}")
            
            return results
            
        except Exception as e:
            logger.error(f"Error processing image {image_id}: {str(e)}")
            raise
            
    def save_results(self, results: Dict) -> None:
            """
            Save processed results to upscaled directory.
            
            Args:
                results: Dictionary containing processed data
            """
            image_id = results['image_id']
            
            # Create upscaled directory if it doesn't exist
            upscaled_dir = self.data_dir / 'upscaled'
            upscaled_dir.mkdir(parents=True, exist_ok=True)
            
            # Generate filename based on original format
            base_filename = f"depth_frame_{image_id}"
            
            # Save aligned RGBD data
            np.save(
                upscaled_dir / f"{base_filename}_upscaled.npy",
                results['aligned_rgbd']
            )
            
            # Save metadata including intrinsic parameters and depth scale
            metadata = {
                'intrinsic_params': results['intrinsic_params'],
                'depth_scale': float(results['depth_scale']),
                'processed_objects': {}
            }
            
            # Add object-specific metadata
            for category, obj_data in results['processed_objects'].items():
                metadata['processed_objects'][category] = {
                    'category_id': obj_data['category_id'],
                    'bbox': obj_data['bbox']
                }
                
                # Save object-specific depth and mask
                obj_prefix = f"{base_filename}_{category}"
                np.save(
                    upscaled_dir / f"{obj_prefix}_depth.npy",
                    obj_data['depth']
                )
                np.save(
                    upscaled_dir / f"{obj_prefix}_mask.npy",
                    obj_data['mask']
                )
            
            # Save metadata
            with open(upscaled_dir / f"{base_filename}_metadata.json", 'w') as f:
                json.dump(metadata, f, indent=4)
                
            logger.info(f"Results saved to {upscaled_dir}")
def run_preprocessing(config_path: str):
    """
    Run the complete preprocessing pipeline.
    
    Args:
        config_path: Path to configuration file
    """
    try:
        # Load configuration
        with open(config_path, 'r') as f:
            config = json.load(f)
            
        # Validate configuration
        required_keys = ['data_dir', 'coco_file', 'output_dir', 
                        'rgbd_shape', 'rgb_shape', 'image_ids']
        for key in required_keys:
            if key not in config:
                raise ValueError(f"Missing required config key: {key}")
                
        # Initialize and run pipeline
        pipeline = PreprocessingPipeline(config)
        
        # Process each image
        for image_id in config['image_ids']:
            try:
                pipeline.process_single_image(image_id)
                logger.info(f"Successfully processed image {image_id}")
            except Exception as e:
                logger.error(f"Failed to process image {image_id}: {str(e)}")
                continue
                
        logger.info("Preprocessing pipeline completed")
        
    except Exception as e:
        logger.error(f"Pipeline execution failed: {str(e)}")
        raise

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Run preprocessing pipeline")
    parser.add_argument('--config', required=True, help='Path to config file')
    args = parser.parse_args()
    
    run_preprocessing(args.config)

================
File: src/reconstruction/mesh_generator.py
================
...

================
File: src/reconstruction/point_cloud.py
================
...

================
File: src/reconstruction/volume_calculator.py
================
...

================
File: src/utils/__init__.py
================
from .coco_utils import CocoHandler

================
File: src/utils/coco_utils.py
================
import numpy as np
import cv2
import json
from typing import Dict, List, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CocoHandler:
    """Utility class for handling COCO format annotations"""
    def __init__(self, annotation_file: str):
        self.annotations = self._load_annotations(annotation_file)
        self.categories = {cat['id']: cat['name'] 
                          for cat in self.annotations['categories']}
        
    def _load_annotations(self, file_path: str) -> Dict:
        try:
            with open(file_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading COCO annotations: {str(e)}")
            raise
            
    def get_image_annotations(self, image_id: int) -> List[Dict]:
        return [ann for ann in self.annotations['annotations'] 
                if ann['image_id'] == image_id]
    
    def get_category_id(self, category_name: str) -> int:
        """Get category ID from name"""
        for cat_id, name in self.categories.items():
            if name == category_name:
                return cat_id
        raise ValueError(f"Category {category_name} not found")
    
    def create_mask(self, annotation: Dict, shape: Tuple[int, int]) -> np.ndarray:
        """Create binary mask from single annotation"""
        mask = np.zeros(shape, dtype=np.uint8)
        for segmentation in annotation['segmentation']:
            points = np.array(segmentation).reshape(-1, 2).astype(np.int32)
            cv2.fillPoly(mask, [points], 1)
        return mask

    def create_category_mask(self, image_id: int, 
                           category_name: str, 
                           shape: Tuple[int, int]) -> np.ndarray:
        """Create mask for specific category"""
        category_id = self.get_category_id(category_name)
        mask = np.zeros(shape, dtype=np.uint8)
        
        annotations = [ann for ann in self.get_image_annotations(image_id)
                      if ann['category_id'] == category_id]
        
        for ann in annotations:
            mask = cv2.bitwise_or(mask, self.create_mask(ann, shape))
            
        return mask

================
File: src/utils/io_utils.py
================
...

================
File: src/utils/visualization.py
================
...

================
File: src/__init__.py
================
from .preprocessing import PreprocessingPipeline
from .core import DepthProcessor, ImageAligner
from .utils import CocoHandler

================
File: test/test_preprocessing.py
================
import sys
import os
from pathlib import Path
import logging
import json

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.preprocessing import PreprocessingPipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_first_files():
    try:
        # Get first .raw file from rgbd directory
        rgbd_dir = Path("data/rgbd")
        raw_files = list(rgbd_dir.glob("*.raw"))
        if not raw_files:
            raise FileNotFoundError("No .raw files found in rgbd directory")
        raw_file = raw_files[0]
        
        # Extract frame_id from filename
        frame_id = raw_file.stem.replace("depth_frame_", "")
        
        # Get corresponding files
        segmented_dir = Path("data/segmented")
        coco_file = next(segmented_dir.glob("*_annotations.coco.json"))
        png_file = next(segmented_dir.glob(f"rgb_frame_{frame_id}.png"))
        
        logger.info(f"Found files:")
        logger.info(f"RAW: {raw_file.name}")
        logger.info(f"COCO: {coco_file.name}")
        logger.info(f"PNG: {png_file.name}")
        
        return {
            'frame_id': frame_id,
            'raw_path': str(raw_file),
            'coco_path': str(coco_file),
            'png_path': str(png_file)
        }
        
    except Exception as e:
        logger.error(f"Error finding files: {str(e)}")
        raise

def create_test_config(files):
    config = {
        "data_dir": str(Path("data")),
        "coco_file": files['coco_path'],
        "rgbd_shape": [480, 640],
        "rgb_shape": [1080, 1920],
        "frame_ids": [files['frame_id']],
        "camera_height": 33.0,
        "plate_diameter": 25.5,
        "plate_height": 0.7,
        "raw_file": files['raw_path'],
        "rgb_file": files['png_path']
    }
    
    config_path = Path("test_config.json")
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=4)
    
    return str(config_path)

def test_preprocessing():
    try:
        files = get_first_files()
        
        config_path = create_test_config(files)
        logger.info(f"Created config file: {config_path}")
        
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        pipeline = PreprocessingPipeline(config)
        
        logger.info("Testing data loading...")
        data = pipeline.load_data(files['frame_id'])
        logger.info("Data loading successful")
        
        logger.info("Testing full preprocessing pipeline...")
        result = pipeline.process_single_image(files['frame_id'])
        logger.info("Preprocessing completed successfully")
        
        upscaled_dir = Path("data/upscaled")
        if upscaled_dir.exists():
            output_files = list(upscaled_dir.glob("*"))
            logger.info(f"Files generated in upscaled directory: {[f.name for f in output_files]}")
        
        return True
        
    except Exception as e:
        logger.error(f"Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_preprocessing()
    if success:
        print("All tests passed!")
    else:
        print("Tests failed!")

================
File: test_config.json
================
{
    "data_dir": "data",
    "coco_file": "data/segmented/_annotations.coco.json",
    "rgbd_shape": [
        480,
        640
    ],
    "rgb_shape": [
        1080,
        1920
    ],
    "frame_ids": [
        "20241127_181653"
    ],
    "camera_height": 33.0,
    "plate_diameter": 25.5,
    "plate_height": 0.7,
    "raw_file": "data/rgbd/depth_frame_20241127_181653.raw",
    "rgb_file": "data/segmented/rgb_frame_20241127_181653.png"
}
