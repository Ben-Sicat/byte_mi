This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-12-04T17:30:54.694Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
data/
  rgbd/
    depth_frame_20241204_233746.meta
  segmented/
    _annotations.coco.json
    rgb_frame_20241204_233746.meta
  upscaled/
    depth_frame_20241204_233746_metadata.json
src/
  core/
    __init__.py
    depth_processor.py
    image_alignment.py
  preprocessing/
    __init__.py
    calibration.py
    noise_reduction.py
    preprocessing.py
  reconstruction/
    mesh_generator.py
    point_cloud.py
    volume_calculator.py
  utils/
    __init__.py
    coco_utils.py
    io_utils.py
    visualization.py
  __init__.py
test/
  inspect_data.py
  test_preprocessing.py
.gitignore
requirements.txt
test_config.json

================================================================
Repository Files
================================================================

================
File: data/rgbd/depth_frame_20241204_233746.meta
================
{
    "width": 160,
    "height": 90,
    "format": 5,
    "timestamp": 84927.277066975,
    "centerPixelDepth": 0.33899998664855959,
    "minDepth": 0.3330000042915344,
    "maxDepth": 0.34200000762939455
}

================
File: data/segmented/_annotations.coco.json
================
{"info":{"year":"2024","version":"1","description":"Exported from roboflow.com","contributor":"","url":"https://public.roboflow.com/object-detection/undefined","date_created":"2024-12-04T16:48:42+00:00"},"licenses":[{"id":1,"url":"https://creativecommons.org/licenses/by/4.0/","name":"CC BY 4.0"}],"categories":[{"id":0,"name":"food","supercategory":"none"},{"id":1,"name":"plate","supercategory":"food"},{"id":2,"name":"rice","supercategory":"food"}],"images":[{"id":0,"license":1,"file_name":"rgb_frame_20241204_233746_png","height":480,"width":640,"date_captured":"2024-12-04T16:48:42+00:00"}],"annotations":[{"id":0,"image_id":0,"category_id":1,"bbox":[160,72,340,341],"area":115940,"segmentation":[[161,218,160,261,166,291,174,313,194,346,215,369,243,390,276,405,313,413,340,413,378,406,415,390,433,378,457,356,478,326,490,300,499,265,500,233,492,189,482,165,462,134,432,105,415,94,380,79,347,72,290,76,252,90,228,105,201,130,185,152,170,183]],"iscrowd":0},{"id":1,"image_id":0,"category_id":2,"bbox":[254,142,161,184],"area":29624,"segmentation":[[255,257,254,268,269,279,261,302,276,325,299,326,318,322,322,316,351,315,404,299,415,290,413,197,389,160,360,152,348,142,290,157,279,173,281,189,274,244]],"iscrowd":0}]}

================
File: data/segmented/rgb_frame_20241204_233746.meta
================
{
    "width": 640,
    "height": 480,
    "timestamp": 84927.277066975
}

================
File: data/upscaled/depth_frame_20241204_233746_metadata.json
================
{
    "intrinsic_params": {
        "focal_length": 444.0259902056526,
        "pixel_size": 0.07431997389323068,
        "principal_point": [
            320.0,
            240.0
        ],
        "image_dimensions": [
            480,
            640
        ],
        "camera_height": 33.0,
        "reference_object": {
            "type": "plate",
            "diameter": 25.5,
            "height": 0.7,
            "measured_diameter_pixels": 343.1109924316406,
            "center_pixels": [
                330.0,
                242.5
            ]
        }
    },
    "depth_scale": 0.09300031512975693,
    "processed_objects": {
        "plate": {
            "category_id": 1,
            "bbox": [
                160,
                72,
                340,
                341
            ]
        },
        "rice": {
            "category_id": 2,
            "bbox": [
                254,
                142,
                161,
                184
            ]
        }
    }
}

================
File: src/core/__init__.py
================
from .depth_processor import DepthProcessor
from .image_alignment import ImageAligner

================
File: src/core/depth_processor.py
================
import numpy as np
import cv2
from typing import Tuple, Optional, Dict
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthProcessor:
    """
    Handles depth data processing and image alignment.
    All measurements in centimeters.
    """
    
    def __init__(self, rgbd_meta_path: Path, rgb_meta_path: Path):
        """
        Initialize processor with metadata paths.
        
        Args:
            rgbd_meta_path: Path to depth metadata file
            rgb_meta_path: Path to RGB metadata file
        """
        # Load dimensions from metadata
        self.depth_shape, self.rgb_shape = self._load_dimensions(
            rgbd_meta_path, 
            rgb_meta_path
        )
        self.dtype = np.uint16
        
        # Processing parameters
        self.bilateral_d = 5
        self.bilateral_sigma_color = 50
        self.bilateral_sigma_space = 50
        
        logger.info(
            f"Initialized DepthProcessor with shapes - "
            f"Depth: {self.depth_shape}, RGB: {self.rgb_shape}"
        )
        
    def _load_dimensions(self, depth_meta_path: Path, 
                        rgb_meta_path: Path) -> Tuple[Tuple[int, int], Tuple[int, int]]:
        """Load image dimensions from metadata files"""
        from ..utils.io_utils import get_frame_dimensions
        return get_frame_dimensions(depth_meta_path, rgb_meta_path)
        
    def load_raw_depth(self, file_path: str) -> np.ndarray:
        """
        Load raw depth data from .raw file.
        
        Args:
            file_path: Path to .raw depth file
            
        Returns:
            np.ndarray: Depth data array
        """
        try:
            raw_data = np.fromfile(file_path, dtype=self.dtype)
            
            expected_size = self.depth_shape[0] * self.depth_shape[1]
            if raw_data.size != expected_size:
                raise ValueError(
                    f"Raw data size {raw_data.size} does not match "
                    f"expected size {expected_size}"
                )
            
            depth_data = raw_data.reshape(self.depth_shape)
            
            logger.info(f"Loaded depth data - Shape: {depth_data.shape}, "
                       f"Range: [{depth_data.min()}, {depth_data.max()}]")
            
            return depth_data
            
        except Exception as e:
            logger.error(f"Error loading depth file: {str(e)}")
            raise
            
    def align_to_depth(self, rgb_image: np.ndarray, 
                      mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        Align RGB image and mask to depth resolution.
        
        Args:
            rgb_image: RGB image to align
            mask: Optional segmentation mask
            
        Returns:
            Tuple[np.ndarray, Optional[np.ndarray]]: Aligned RGB and mask
        """
        # Resize RGB image
        aligned_rgb = cv2.resize(
            rgb_image,
            (self.depth_shape[1], self.depth_shape[0]),  # width, height
            interpolation=cv2.INTER_LINEAR
        )
        
        # Resize mask if provided
        aligned_mask = None
        if mask is not None:
            aligned_mask = cv2.resize(
                mask,
                (self.depth_shape[1], self.depth_shape[0]),
                interpolation=cv2.INTER_NEAREST  # Use nearest neighbor for masks
            )
            
        logger.info(f"Aligned images to depth resolution: {self.depth_shape}")
        return aligned_rgb, aligned_mask
        
    def process_depth(self, depth_data: np.ndarray, 
                     mask: Optional[np.ndarray] = None) -> np.ndarray:
        """Process depth data to remove noise and fill holes."""
        if depth_data.shape != self.depth_shape:
            raise ValueError(f"Expected shape {self.depth_shape}, got {depth_data.shape}")
            
        # Convert to float32 for processing
        depth = depth_data.astype(np.float32)
        
        # Remove outliers using statistics from masked region if mask provided
        valid_mask = mask if mask is not None else (depth > 0)
        if np.any(valid_mask):
            mean_depth = np.mean(depth[valid_mask])
            std_depth = np.std(depth[valid_mask])
            
            min_valid = mean_depth - 2 * std_depth
            max_valid = mean_depth + 2 * std_depth
            
            outlier_mask = (depth < min_valid) | (depth > max_valid)
            depth[outlier_mask] = 0
            
            logger.info(f"Removed {np.sum(outlier_mask)} outlier points")
        
        # Fill holes using bilateral filter
        filtered_depth = cv2.bilateralFilter(
            depth,
            d=self.bilateral_d,
            sigmaColor=self.bilateral_sigma_color,
            sigmaSpace=self.bilateral_sigma_space
        )
        
        return filtered_depth
        
    def get_depth_stats(self, depth_data: np.ndarray, 
                       mask: Optional[np.ndarray] = None) -> Dict:
        """Get statistics about depth data."""
        valid_mask = mask if mask is not None else (depth_data > 0)
        
        if not np.any(valid_mask):
            return {
                'min': 0,
                'max': 0,
                'mean': 0,
                'std': 0,
                'valid_points': 0
            }
            
        valid_depths = depth_data[valid_mask]
        return {
            'min': float(np.min(valid_depths)),
            'max': float(np.max(valid_depths)),
            'mean': float(np.mean(valid_depths)),
            'std': float(np.std(valid_depths)),
            'valid_points': int(np.sum(valid_mask))
        }

================
File: src/core/image_alignment.py
================
import numpy as np
import cv2
from typing import Tuple, Dict
import logging
from ..utils.coco_utils import CocoHandler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImageAligner:
    """Handles alignment between RGB images, RGBD data, and segmentation masks"""
    def __init__(self, coco_file: str):
        self.rgb_shape = None
        self.rgbd_shape = None
        self.coco_handler = CocoHandler(coco_file)
        
    def set_reference_sizes(self, rgb_shape: Tuple[int, int], 
                           rgbd_shape: Tuple[int, int]) -> None:
        self.rgb_shape = rgb_shape
        self.rgbd_shape = rgbd_shape
        logger.info(f"Set reference shapes - RGB: {rgb_shape}, RGBD: {rgbd_shape}")
        
    def align_rgbd_to_rgb(self, rgbd_data: np.ndarray) -> np.ndarray:
        """Align RGBD data to RGB dimensions"""
        if not (self.rgb_shape and self.rgbd_shape):
            raise ValueError("Reference sizes not set")
            
        if rgbd_data.shape[2] != 4:
            raise ValueError(f"Expected 4 channels in RGBD data, got {rgbd_data.shape[2]}")
            
        # Split and resize channels
        rgb_channels = rgbd_data[:, :, :3]
        depth_channel = rgbd_data[:, :, 3]
        
        aligned_rgb = cv2.resize(rgb_channels, 
                               (self.rgb_shape[1], self.rgb_shape[0]),
                               interpolation=cv2.INTER_LINEAR)
        
        aligned_depth = cv2.resize(depth_channel,
                                 (self.rgb_shape[1], self.rgb_shape[0]),
                                 interpolation=cv2.INTER_LINEAR)
        
        # Combine channels
        aligned_rgbd = np.zeros((*self.rgb_shape, 4), dtype=rgbd_data.dtype)
        aligned_rgbd[:, :, :3] = aligned_rgb
        aligned_rgbd[:, :, 3] = aligned_depth
        
        return aligned_rgbd
        
    def extract_object_depth(self, rgbd_aligned: np.ndarray, 
                           image_id: int,
                           category_name: str) -> Dict[str, np.ndarray]:
        """Extract depth data for specific object category"""
        # Get object mask using COCO handler
        mask = self.coco_handler.create_category_mask(
            image_id, 
            category_name, 
            self.rgb_shape
        )
        
        # Extract depth data
        depth_data = rgbd_aligned[:, :, 3].copy()
        masked_depth = np.zeros_like(depth_data)
        masked_depth[mask > 0] = depth_data[mask > 0]
        
        return {
            'mask': mask,
            'depth': masked_depth,
            'category': category_name
        }

================
File: src/preprocessing/__init__.py
================
from .preprocessing import PreprocessingPipeline
from .calibration import CameraCalibrator
from .noise_reduction import DepthNoiseReducer

================
File: src/preprocessing/calibration.py
================
import numpy as np
import cv2
from typing import Dict, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CameraCalibrator:
    """
    Calculate intrinsic parameters using known measurements and plate as reference.
    All measurements are in centimeters.
    """
    def __init__(self):
        # Known measurements in cm
        self.camera_height = 33.0  
        self.plate_diameter = 25.5  
        self.plate_height = 0.7  
        
        # Will be calculated
        self.focal_length = None
        self.principal_point = None
        self.pixel_size = None
        
    def calculate_focal_length(self, plate_diameter_pixels: float) -> float:
        """
        Calculate focal length using pinhole model and plate as reference.
        f = (P * H) / W
        where:
        f = focal length in pixels
        P = plate diameter in pixels
        H = camera height in cm
        W = actual plate diameter in cm
        """
        focal_length = (plate_diameter_pixels * self.camera_height) / self.plate_diameter
        logger.info(f"Calculated focal length: {focal_length:.2f} pixels")
        return focal_length
        
    def calculate_pixel_size(self, plate_diameter_pixels: float) -> float:
        """
        Calculate pixel size in cm
        pixel_size = actual_size / pixel_size
        """
        pixel_size = self.plate_diameter / plate_diameter_pixels
        logger.info(f"Calculated pixel size: {pixel_size:.6f} cm/pixel")
        return pixel_size

    def get_plate_measurements(self, plate_mask: np.ndarray) -> Dict:
        """
        Get plate measurements from mask in pixels.
        """
        # Find contours of the plate
        contours, _ = cv2.findContours(
            plate_mask.astype(np.uint8),
            cv2.RETR_EXTERNAL,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        if not contours:
            raise ValueError("No plate contour found in mask")
        
        # Get largest contour (plate)
        plate_contour = max(contours, key=cv2.contourArea)
        
        # Find the minimum enclosing circle
        (center_x, center_y), radius = cv2.minEnclosingCircle(plate_contour)
        diameter_pixels = radius * 2
        
        return {
            'center': (center_x, center_y),
            'radius': radius,
            'diameter_pixels': diameter_pixels
        }

    def calculate_intrinsics(self, plate_mask: np.ndarray) -> Dict:
        """
        Calculate all intrinsic parameters using plate mask.
        """
        try:
            # get plate measurements
            plate_info = self.get_plate_measurements(plate_mask)
            
            # calculate focal length
            self.focal_length = self.calculate_focal_length(
                plate_info['diameter_pixels']
            )
            
            # calculate pixel size
            self.pixel_size = self.calculate_pixel_size(
                plate_info['diameter_pixels']
            )
            
            # principal point 
            height, width = plate_mask.shape
            self.principal_point = (width / 2, height / 2)
            
            intrinsic_params = {
                'focal_length': self.focal_length,  # in pixels
                'pixel_size': self.pixel_size,      # cm/pixel
                'principal_point': self.principal_point,
                'image_dimensions': (height, width),
                'camera_height': self.camera_height,
                'reference_object': {
                    'type': 'plate',
                    'diameter': self.plate_diameter,
                    'height': self.plate_height,
                    'measured_diameter_pixels': plate_info['diameter_pixels'],
                    'center_pixels': plate_info['center']
                }
            }
            
            self._validate_parameters(intrinsic_params)
            
            return intrinsic_params
            
        except Exception as e:
            logger.error(f"Error calculating intrinsic parameters: {str(e)}")
            raise

    def _validate_parameters(self, params: Dict) -> None:
        """
        Validate calculated parameters.
        """
        if params['focal_length'] <= 0:
            raise ValueError(f"Invalid focal length: {params['focal_length']}")
            
        if params['pixel_size'] <= 0 or params['pixel_size'] > 1:
            raise ValueError(f"Invalid pixel size: {params['pixel_size']}")
            
        measured_diameter_cm = (
            params['reference_object']['measured_diameter_pixels'] * 
            params['pixel_size']
        )
        error_margin = abs(measured_diameter_cm - self.plate_diameter)
        if error_margin > 1.7:  # More than 1cm error
            logger.warning(
                f"Large error in plate diameter measurement: "
                f"{error_margin:.2f}cm"
            )

    def get_depth_scale_factor(self, plate_depth_values: np.ndarray) -> float:
        """
        Calculate depth scale factor using plate as reference.
        """
        # Expected plate distance from camera
        expected_plate_distance = self.camera_height - self.plate_height
        
        # Use median of plate depth values
        measured_plate_distance = np.median(plate_depth_values)
        
        # Calculate scale factor
        scale_factor = expected_plate_distance / measured_plate_distance
        
        logger.info(f"Depth scale factor: {scale_factor:.4f}")
        return scale_factor

================
File: src/preprocessing/noise_reduction.py
================
import numpy as np
import cv2
from typing import Dict, Optional, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthNoiseReducer:
    """
    Handles noise reduction and cleaning of depth data from RGBD images.
    """
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize with optional configuration parameters.
        
        Args:
            config: Dictionary containing filter parameters:
                - bilateral_d: Diameter of pixel neighborhood
                - bilateral_sigma_color: Filter sigma in color space
                - bilateral_sigma_space: Filter sigma in coordinate space
                - median_kernel: Median filter kernel size
                - outlier_threshold: Standard deviation threshold for outliers
        """
        self.config = config or {
            'bilateral_d': 5,
            'bilateral_sigma_color': 0.1,
            'bilateral_sigma_space': 5.0,
            'median_kernel': 5,
            'outlier_threshold': 2.0
        }
        
    def remove_outliers(self, depth_data: np.ndarray, 
                       mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Remove outlier depth values using statistical analysis.
        
        Args:
            depth_data: Depth channel data
            mask: Optional mask to process specific regions
            
        Returns:
            np.ndarray: Depth data with outliers removed
        """
        if mask is not None:
            valid_depths = depth_data[mask > 0]
        else:
            valid_depths = depth_data[depth_data > 0]
            
        if len(valid_depths) == 0:
            return depth_data
            
        # Calculate statistics
        mean_depth = np.mean(valid_depths)
        std_depth = np.std(valid_depths)
        threshold = std_depth * self.config['outlier_threshold']
        
        # Create outlier mask
        outliers = np.abs(depth_data - mean_depth) > threshold
        
        # Replace outliers with local median
        cleaned_depth = depth_data.copy()
        if np.any(outliers):
            kernel_size = self.config['median_kernel']
            local_median = cv2.medianBlur(
                depth_data.astype(np.float32),
                kernel_size
            )
            cleaned_depth[outliers] = local_median[outliers]
            
            logger.info(f"Removed {np.sum(outliers)} outlier points")
            
        return cleaned_depth
        
    def fill_missing_values(self, depth_data: np.ndarray) -> np.ndarray:
        """
        Fill missing or invalid depth values using interpolation.
        
        Args:
            depth_data: Depth channel data
            
        Returns:
            np.ndarray: Depth data with filled values
        """
        # Create mask of invalid values
        invalid_mask = (depth_data <= 0) | np.isnan(depth_data)
        
        if not np.any(invalid_mask):
            return depth_data
            
        filled_depth = depth_data.copy()
        
        # Use inpainting to fill holes
        filled_depth = cv2.inpaint(
            filled_depth.astype(np.float32),
            invalid_mask.astype(np.uint8),
            3,
            cv2.INPAINT_NS
        )
        
        logger.info(f"Filled {np.sum(invalid_mask)} missing values")
        return filled_depth
        
    def apply_bilateral_filter(self, depth_data: np.ndarray) -> np.ndarray:
        """
        Apply bilateral filtering to reduce noise while preserving edges.
        
        Args:
            depth_data: Depth channel data
            
        Returns:
            np.ndarray: Filtered depth data
        """
        filtered_depth = cv2.bilateralFilter(
            depth_data.astype(np.float32),
            self.config['bilateral_d'],
            self.config['bilateral_sigma_color'],
            self.config['bilateral_sigma_space']
        )
        
        return filtered_depth
        
    def smooth_edges(self, depth_data: np.ndarray, 
                    mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Smooth depth values at object edges.
        
        Args:
            depth_data: Depth channel data
            mask: Optional segmentation mask
            
        Returns:
            np.ndarray: Depth data with smoothed edges
        """
        if mask is not None:
            # Find edges in mask
            edges = cv2.Canny(mask.astype(np.uint8), 100, 200)
            
            # Dilate edges slightly
            kernel = np.ones((3,3), np.uint8)
            edge_region = cv2.dilate(edges, kernel, iterations=1)
            
            # Apply stronger smoothing only to edge regions
            smoothed = cv2.GaussianBlur(
                depth_data.astype(np.float32),
                (5,5),
                1.0
            )
            
            result = depth_data.copy()
            result[edge_region > 0] = smoothed[edge_region > 0]
            
            return result
        
        return depth_data
        
    def process_depth(self, depth_data: np.ndarray,
                     mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Apply complete noise reduction pipeline to depth data.
        
        Args:
            depth_data: Depth channel data
            mask: Optional segmentation mask
            
        Returns:
            np.ndarray: Processed depth data
        """
        logger.info("Starting depth noise reduction")
        
        # Fill missing values first
        filled_depth = self.fill_missing_values(depth_data)
        
        # Remove statistical outliers
        cleaned_depth = self.remove_outliers(filled_depth, mask)
        
        # Apply bilateral filtering
        filtered_depth = self.apply_bilateral_filter(cleaned_depth)
        
        # Smooth edges if mask provided
        if mask is not None:
            final_depth = self.smooth_edges(filtered_depth, mask)
        else:
            final_depth = filtered_depth
            
        logger.info("Completed depth noise reduction")
        return final_depth

================
File: src/preprocessing/preprocessing.py
================
import cv2
import numpy as np
from typing import Dict, Optional
import logging
from pathlib import Path
import json

from ..core.depth_processor import DepthProcessor
from ..core.image_alignment import ImageAligner
from .calibration import CameraCalibrator
from .noise_reduction import DepthNoiseReducer
from ..utils.coco_utils import CocoHandler

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PreprocessingPipeline:
    def __init__(self, config: Dict):
        """
        Initialize preprocessing pipeline.
        
        Args:
            config: Dict containing:
                - data_dir: Path to data directory
                - output_dir: Path to save processed data
                - coco_file: Path to COCO annotations
                - camera_height: Height of camera in cm
                - plate_diameter: Diameter of plate in cm
                - plate_height: Height of plate in cm
        """
        self.config = config
        self.data_dir = Path(config['data_dir'])
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self.coco_handler = CocoHandler(config['coco_file'])
        self.calibrator = CameraCalibrator()
        self.noise_reducer = DepthNoiseReducer()
        
        logger.info("Initialized preprocessing pipeline")
    def load_data(self, frame_id: str) -> Dict:
        """Load all necessary data for processing"""
        try:
            # Get metadata paths
            rgbd_meta_path = self.data_dir / "rgbd" / f"depth_frame_{frame_id}.meta"
            rgb_meta_path = self.data_dir / "segmented" / f"rgb_frame_{frame_id}.meta"
            
            # Initialize depth processor
            self.depth_processor = DepthProcessor(rgbd_meta_path, rgb_meta_path)
            
            # Load RGB image
            rgb_path = self.data_dir / "segmented" / f"rgb_frame_{frame_id}.png"
            if not rgb_path.exists():
                raise FileNotFoundError(f"RGB image not found: {rgb_path}")
                
            rgb_image = cv2.imread(str(rgb_path))
            rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)
            
            # Load depth data
            depth_path = self.data_dir / "rgbd" / f"depth_frame_{frame_id}.raw"
            depth_meta = load_metadata(rgbd_meta_path)
            
            if not depth_path.exists():
                raise FileNotFoundError(f"Depth data not found: {depth_path}")
                
            # Load and process depth
            raw_depth = self.depth_processor.load_raw_depth(str(depth_path))
            
            # Validate depth data
            if not validate_depth_data(raw_depth, self.depth_processor.depth_shape, depth_meta):
                raise ValueError("Invalid depth data")
                
            # Process depth
            processed_depth = self.depth_processor.process_depth(raw_depth)
            
            # Get plate mask at RGB resolution
            plate_mask = self.coco_handler.create_category_mask(
                frame_id, 
                'plate',
                rgb_image.shape[:2]
            )
            
            # Align RGB and mask to depth resolution
            aligned_rgb, aligned_mask = self.depth_processor.align_to_depth(
                rgb_image, plate_mask
            )
            
            # Validate alignments
            if not validate_image_alignment(rgb_image, aligned_rgb, self.depth_processor.depth_shape):
                raise ValueError("RGB alignment failed validation")
                
            if not validate_image_alignment(plate_mask, aligned_mask, self.depth_processor.depth_shape):
                raise ValueError("Mask alignment failed validation")
                
            return {
                'rgb': aligned_rgb,
                'depth': processed_depth,
                'plate_mask': aligned_mask,
                'frame_id': frame_id,
                'original_rgb': rgb_image,  # Keep original for reference
                'original_mask': plate_mask  # Keep original for reference
            }
            
        except Exception as e:
            logger.error(f"Error loading data for frame {frame_id}: {str(e)}")
            raise
            
    def process_single_image(self, frame_id: str) -> Dict:

        """Process a single image through the pipeline"""
        try:
            logger.info(f"Processing frame {frame_id}")
            
            # Step 1: Load data
            data = self.load_data(frame_id)
            logger.info("Data loaded successfully")
            
            # Step 2: Calculate camera intrinsics using plate
            intrinsic_params = self.calibrator.calculate_intrinsics(data['plate_mask'])
            logger.info("Camera calibration completed")
            
            # Step 3: Clean depth data
            cleaned_depth = self.noise_reducer.process_depth(
                data['depth'],
                data['plate_mask']
            )
            
            # Step 4: Get depth scale factor using plate
            plate_depth = cleaned_depth[data['plate_mask'] > 0]
            depth_scale = self.calibrator.get_depth_scale_factor(plate_depth)
            cleaned_depth *= depth_scale
            logger.info(f"Depth scaling applied (scale factor: {depth_scale:.4f})")
            
            # Step 5: Process each object
            annotations = self.coco_handler.get_image_annotations(frame_id)
            processed_objects = {}
            
            for ann in annotations:
                category_id = ann['category_id']
                category_name = self.coco_handler.categories[category_id]
                
                # Create object mask
                obj_mask = self.coco_handler.create_mask(
                    ann, 
                    cleaned_depth.shape
                )
                
                # Extract and clean object depth
                obj_depth = cleaned_depth.copy()
                obj_depth[obj_mask == 0] = 0
                
                processed_objects[category_name] = {
                    'mask': obj_mask,
                    'depth': obj_depth,
                    'category_id': category_id,
                    'bbox': ann['bbox']
                }
                
            logger.info(f"Processed {len(processed_objects)} objects")
            
            # Prepare results
            results = {
                'frame_id': frame_id,
                'intrinsic_params': intrinsic_params,
                'depth': cleaned_depth,
                'depth_scale': depth_scale,
                'processed_objects': processed_objects,
                'rgb': data['rgb']
            }
            
            # Save results
            self.save_results(results)
            logger.info(f"Processing completed for frame {frame_id}")
            
            return results
            
        except Exception as e:
            logger.error(f"Error processing frame {frame_id}: {str(e)}")
            raise
    def save_results(self, results: Dict) -> None:
        """Save processed results to output directory"""
        frame_id = results['frame_id']
        base_filename = f"depth_frame_{frame_id}"
        
        # Create output directory if it doesn't exist
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save processed depth
        np.save(
            self.output_dir / f"{base_filename}_processed.npy",
            results['depth']
        )
        
        # Save aligned RGB for reference
        cv2.imwrite(
            str(self.output_dir / f"{base_filename}_aligned_rgb.png"),
            cv2.cvtColor(results['rgb'], cv2.COLOR_RGB2BGR)
        )
        
        # Save masks
        for category, obj_data in results['processed_objects'].items():
            mask_filename = f"{base_filename}_{category}_mask.npy"
            np.save(self.output_dir / mask_filename, obj_data['mask'])
        
        # Save metadata
        metadata = {
            'intrinsic_params': results['intrinsic_params'],
            'depth_scale': float(results['depth_scale']),
            'processed_objects': {
                category: {
                    'category_id': obj_data['category_id'],
                    'bbox': obj_data['bbox']
                }
                for category, obj_data in results['processed_objects'].items()
            },
            'alignment_info': {
                'depth_shape': self.depth_processor.depth_shape,
                'original_rgb_shape': results['original_rgb'].shape[:2]
            }
        }
        
        with open(self.output_dir / f"{base_filename}_metadata.json", 'w') as f:
            json.dump(metadata, f, indent=4)
            
        logger.info(
            f"Saved processed results to {self.output_dir}:\n"
            f"- Processed depth map\n"
            f"- Aligned RGB image\n"
            f"- Object masks: {list(results['processed_objects'].keys())}\n"
            f"- Metadata with alignment info"
        )
def run_preprocessing(config_path: str):
    """Run the complete preprocessing pipeline"""
    try:
        # Load configuration
        with open(config_path, 'r') as f:
            config = json.load(f)
            
        # Validate configuration
        required_keys = [
            'data_dir', 'output_dir', 'coco_file',
            'rgb_shape', 'camera_height', 'plate_diameter', 'plate_height'
        ]
        for key in required_keys:
            if key not in config:
                raise ValueError(f"Missing required config key: {key}")
                
        # Initialize and run pipeline
        pipeline = PreprocessingPipeline(config)
        
        # Process each frame
        for frame_id in config['frame_ids']:
            try:
                pipeline.process_single_image(frame_id)
                logger.info(f"Successfully processed frame {frame_id}")
            except Exception as e:
                logger.error(f"Failed to process frame {frame_id}: {str(e)}")
                continue
                
        logger.info("Preprocessing pipeline completed")
        
    except Exception as e:
        logger.error(f"Pipeline execution failed: {str(e)}")
        raise

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Run preprocessing pipeline")
    parser.add_argument('--config', required=True, help='Path to config file')
    args = parser.parse_args()
    
    run_preprocessing(args.config)

================
File: src/reconstruction/mesh_generator.py
================
...

================
File: src/reconstruction/point_cloud.py
================
...

================
File: src/reconstruction/volume_calculator.py
================
...

================
File: src/utils/__init__.py
================
from .coco_utils import CocoHandler

================
File: src/utils/coco_utils.py
================
import numpy as np
import cv2
import json
from typing import Dict, List, Tuple, Optional
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CocoHandler:
    """Utility class for handling COCO format annotations"""
    def __init__(self, annotation_file: str):
        self.annotations = self._load_annotations(annotation_file)
        self.categories = {cat['id']: cat['name'] 
                          for cat in self.annotations['categories']}
        
        # Create filename to image_id mapping
        self.filename_to_id = {}
        for img in self.annotations['images']:
            # Strip extension and any extra suffixes
            base_name = img['file_name'].split('_png')[0]
            self.filename_to_id[base_name] = img['id']
            
        logger.info(f"Loaded categories: {list(self.categories.values())}")
        logger.info(f"Loaded image mappings: {self.filename_to_id}")
        
    def _load_annotations(self, file_path: str) -> Dict:
        try:
            with open(file_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading COCO annotations: {str(e)}")
            raise
            
    def get_image_id(self, frame_id: str) -> int:
        """Get COCO image ID from frame ID"""
        filename = f"rgb_frame_{frame_id}"
        if filename not in self.filename_to_id:
            logger.error(
                f"No image ID found for {filename}. "
                f"Available files: {list(self.filename_to_id.keys())}"
            )
            raise ValueError(f"Image ID not found for frame {frame_id}")
        return self.filename_to_id[filename]
            
    def get_image_annotations(self, frame_id: str) -> List[Dict]:
        """Get annotations for specific image"""
        try:
            image_id = self.get_image_id(frame_id)
            annotations = [ann for ann in self.annotations['annotations'] 
                         if ann['image_id'] == image_id]
            
            if annotations:
                logger.info(f"Found {len(annotations)} annotations for image {frame_id}")
            else:
                logger.warning(f"No annotations found for image {frame_id}")
                
            return annotations
            
        except Exception as e:
            logger.error(f"Error getting annotations: {str(e)}")
            raise
    
    def get_category_id(self, category_name: str) -> int:
        """Get category ID from name"""
        for cat_id, name in self.categories.items():
            if name.lower() == category_name.lower():
                return cat_id
        available_categories = list(self.categories.values())
        raise ValueError(
            f"Category '{category_name}' not found. "
            f"Available categories are: {available_categories}"
        )
    
    def create_mask(self, annotation: Dict, shape: Tuple[int, int]) -> np.ndarray:
        """Create binary mask from single annotation"""
        mask = np.zeros(shape, dtype=np.uint8)
        
        if not annotation.get('segmentation'):
            logger.error(f"No segmentation data in annotation: {annotation}")
            return mask
            
        try:
            for segmentation in annotation['segmentation']:
                points = np.array(segmentation).reshape(-1, 2).astype(np.int32)
                cv2.fillPoly(mask, [points], 1)
                
            if not np.any(mask):
                logger.warning("Created mask is empty")
            else:
                logger.info(f"Created mask with {np.sum(mask)} positive pixels")
                
            return mask
            
        except Exception as e:
            logger.error(f"Error creating mask: {str(e)}")
            return mask

    def create_category_mask(self, frame_id: str, 
                           category_name: str, 
                           shape: Tuple[int, int]) -> np.ndarray:
        """Create mask for specific category"""
        try:
            category_id = self.get_category_id(category_name)
            mask = np.zeros(shape, dtype=np.uint8)
            
            annotations = self.get_image_annotations(frame_id)
            category_annotations = [
                ann for ann in annotations 
                if ann['category_id'] == category_id
            ]
            
            if not category_annotations:
                logger.warning(
                    f"No annotations found for category '{category_name}' "
                    f"in frame {frame_id}"
                )
                return mask
                
            for ann in category_annotations:
                ann_mask = self.create_mask(ann, shape)
                mask = cv2.bitwise_or(mask, ann_mask)
                
            if not np.any(mask):
                logger.warning(f"Final mask for category '{category_name}' is empty")
            else:
                logger.info(
                    f"Created mask for category '{category_name}' with "
                    f"{np.sum(mask)} positive pixels"
                )
                
            return mask
            
        except Exception as e:
            logger.error(f"Error creating category mask: {str(e)}")
            return np.zeros(shape, dtype=np.uint8)
        
    def visualize_mask(self, mask: np.ndarray, save_path: Optional[str] = None) -> np.ndarray:
        """
        Visualize a binary mask and optionally save it.
        
        Args:
            mask: Binary mask to visualize
            save_path: Optional path to save visualization
            
        Returns:
            np.ndarray: Visualization image
        """
        if not np.any(mask):
            logger.warning("Mask is empty - no visualization created")
            return np.zeros((*mask.shape, 3), dtype=np.uint8)
            
        # Create color visualization
        viz = np.zeros((*mask.shape, 3), dtype=np.uint8)
        viz[mask > 0] = [0, 255, 0]  # Green for masked areas
        
        # Add contours
        contours, _ = cv2.findContours(
            mask.astype(np.uint8), 
            cv2.RETR_EXTERNAL, 
            cv2.CHAIN_APPROX_SIMPLE
        )
        cv2.drawContours(viz, contours, -1, (255, 255, 255), 2)
        
        if save_path:
            save_path = Path(save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)
            cv2.imwrite(str(save_path), cv2.cvtColor(viz, cv2.COLOR_RGB2BGR))
            logger.info(f"Saved mask visualization to {save_path}")
            
        return viz

================
File: src/utils/io_utils.py
================
import json
from pathlib import Path
from typing import Dict, Tuple, Optional
import logging
import cv2
import numpy as np

logger = logging.getLogger(__name__)

def load_metadata(file_path: Path) -> Dict:
    """Load metadata from a .meta file."""
    try:
        with open(file_path, 'r') as f:
            metadata = json.load(f)
            
        required_keys = ['width', 'height']
        if not all(key in metadata for key in required_keys):
            raise ValueError(f"Missing required keys in metadata: {required_keys}")
            
        return metadata
    except Exception as e:
        logger.error(f"Error loading metadata from {file_path}: {str(e)}")
        raise

def get_frame_dimensions(rgbd_meta_path: Path, rgb_meta_path: Path) -> Tuple[Tuple[int, int], Tuple[int, int]]:
    """Get frame dimensions from metadata files."""
    try:
        rgbd_meta = load_metadata(rgbd_meta_path)
        rgb_meta = load_metadata(rgb_meta_path)
        
        depth_dims = (rgbd_meta['height'], rgbd_meta['width'])
        rgb_dims = (rgb_meta['height'], rgb_meta['width'])
        
        # Validate dimensions
        if not all(d > 0 for d in depth_dims + rgb_dims):
            raise ValueError("Invalid dimensions: all dimensions must be positive")
            
        logger.info(f"Loaded dimensions - Depth: {depth_dims}, RGB: {rgb_dims}")
        return depth_dims, rgb_dims
    except Exception as e:
        logger.error(f"Error getting frame dimensions: {str(e)}")
        raise

def validate_image_alignment(source: np.ndarray, aligned: np.ndarray, 
                           target_shape: Tuple[int, int], 
                           threshold: float = 0.1) -> bool:
    """
    Validate image alignment by checking dimensions and content.
    
    Args:
        source: Original image
        aligned: Aligned image
        target_shape: Expected shape after alignment
        threshold: Maximum allowed mean absolute difference after normalization
        
    Returns:
        bool: True if alignment is valid
    """
    try:
        # Check dimensions
        if aligned.shape[:2] != target_shape:
            logger.error(f"Invalid aligned shape: {aligned.shape[:2]} != {target_shape}")
            return False
            
        # For masks, check binary values are preserved
        if aligned.dtype == bool or (aligned.dtype == np.uint8 and np.max(aligned) == 1):
            if not np.array_equal(np.unique(aligned), np.unique(source)):
                logger.error("Binary mask values were not preserved during alignment")
                return False
                
        # For RGB images, check content preservation
        else:
            # Resize source to target for comparison
            source_resized = cv2.resize(source, target_shape[::-1])
            
            # Normalize and compare
            source_norm = source_resized.astype(float) / np.max(source_resized)
            aligned_norm = aligned.astype(float) / np.max(aligned)
            
            diff = np.mean(np.abs(source_norm - aligned_norm))
            if diff > threshold:
                logger.error(f"Alignment error too high: {diff:.3f} > {threshold}")
                return False
                
        return True
        
    except Exception as e:
        logger.error(f"Error validating alignment: {str(e)}")
        return False

def validate_depth_data(depth_data: np.ndarray, 
                       expected_shape: Tuple[int, int],
                       metadata: Optional[Dict] = None) -> bool:
    """
    Validate depth data against expected parameters.
    
    Args:
        depth_data: Depth image to validate
        expected_shape: Expected dimensions
        metadata: Optional metadata with min/max depth values
        
    Returns:
        bool: True if depth data is valid
    """
    try:
        # Check dimensions
        if depth_data.shape != expected_shape:
            logger.error(f"Invalid depth shape: {depth_data.shape} != {expected_shape}")
            return False
            
        # Check data type
        if depth_data.dtype not in [np.uint16, np.float32]:
            logger.error(f"Invalid depth dtype: {depth_data.dtype}")
            return False
            
        # Check value range
        if np.all(depth_data == 0):
            logger.error("Depth data is all zeros")
            return False
            
        # If metadata provided, check against expected ranges
        if metadata:
            min_depth = metadata.get('minDepth')
            max_depth = metadata.get('maxDepth')
            
            if min_depth is not None and max_depth is not None:
                actual_min = np.min(depth_data[depth_data > 0])
                actual_max = np.max(depth_data)
                
                if actual_min < min_depth * 0.5 or actual_max > max_depth * 1.5:
                    logger.error(
                        f"Depth values out of expected range: "
                        f"[{actual_min:.3f}, {actual_max:.3f}] vs "
                        f"[{min_depth:.3f}, {max_depth:.3f}]"
                    )
                    return False
                    
        return True
        
    except Exception as e:
        logger.error(f"Error validating depth data: {str(e)}")
        return False

================
File: src/utils/visualization.py
================
import plotly.graph_objects as go
import numpy as np
from typing import Dict, Optional, Tuple
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DepthVisualizer:
    """
    Visualization tools for depth data analysis and validation.
    """
    def __init__(self, camera_height: float, plate_diameter: float, plate_height: float):
        """
        Initialize visualizer with camera parameters.
        
        Args:
            camera_height: Height of camera in cm
            plate_diameter: Diameter of reference plate in cm
            plate_height: Height of plate in cm
        """
        self.camera_height = camera_height
        self.plate_diameter = plate_diameter
        self.plate_height = plate_height
        self.expected_plate_distance = camera_height - plate_height
        
    def create_depth_surface(self, depth_map: np.ndarray, 
                           mask: Optional[np.ndarray] = None,
                           title: str = "Depth Surface Plot") -> go.Figure:
        """
        Create interactive 3D surface plot of depth data.
        
        Args:
            depth_map: 2D depth map array
            mask: Optional binary mask to focus on specific region
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        # Apply mask if provided
        if mask is not None:
            depth_data = depth_map.copy()
            depth_data[mask == 0] = np.nan
        else:
            depth_data = depth_map
            
        # Create coordinate grids
        rows, cols = depth_data.shape
        x = np.linspace(0, cols-1, cols)
        y = np.linspace(0, rows-1, rows)
        X, Y = np.meshgrid(x, y)
        
        # Create surface plot
        fig = go.Figure(data=[
            go.Surface(
                x=X,
                y=Y,
                z=depth_data,
                colorscale='Viridis',
                colorbar=dict(title='Depth (cm)'),
                contours=dict(
                    z=dict(
                        show=True,
                        usecolormap=True,
                        project_z=True
                    )
                )
            )
        ])
        
        # Update layout
        fig.update_layout(
            title=title,
            scene=dict(
                xaxis_title='X (pixels)',
                yaxis_title='Y (pixels)',
                zaxis_title='Depth (cm)',
                camera=dict(
                    eye=dict(x=1.5, y=1.5, z=1.5)
                ),
                aspectratio=dict(x=1, y=1, z=0.5)
            ),
            width=900,
            height=700
        )
        
        return fig
        
    def create_depth_heatmap(self, depth_map: np.ndarray,
                            mask: Optional[np.ndarray] = None,
                            title: str = "Depth Heatmap") -> go.Figure:
        """
        Create 2D heatmap of depth data.
        
        Args:
            depth_map: 2D depth map array
            mask: Optional binary mask
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        if mask is not None:
            depth_data = depth_map.copy()
            depth_data[mask == 0] = np.nan
        else:
            depth_data = depth_map
            
        fig = go.Figure(data=go.Heatmap(
            z=depth_data,
            colorscale='Viridis',
            colorbar=dict(title='Depth (cm)')
        ))
        
        fig.update_layout(
            title=title,
            xaxis_title='X (pixels)',
            yaxis_title='Y (pixels)',
            width=800,
            height=600
        )
        
        return fig
        
    def analyze_plate_depth(self, depth_map: np.ndarray,
                          plate_mask: np.ndarray) -> Dict:
        """
        Analyze depth values in plate region for validation.
        
        Args:
            depth_map: Depth map array
            plate_mask: Binary mask of plate region
            
        Returns:
            Dict containing analysis results
        """
        # Extract plate depth values
        plate_depths = depth_map[plate_mask > 0]
        
        if len(plate_depths) == 0:
            raise ValueError("No valid depth values in plate region")
            
        # Calculate statistics
        stats = {
            'mean_depth': float(np.mean(plate_depths)),
            'std_depth': float(np.std(plate_depths)),
            'min_depth': float(np.min(plate_depths)),
            'max_depth': float(np.max(plate_depths)),
            'num_points': int(len(plate_depths)),
            'expected_depth': self.expected_plate_distance,
            'depth_error': float(np.mean(plate_depths) - self.expected_plate_distance)
        }
        
        # Calculate planarity
        if len(plate_depths) > 3:
            planarity = self._calculate_planarity(depth_map, plate_mask)
            stats['planarity_error'] = float(planarity)
            
        return stats
        
    def _calculate_planarity(self, depth_map: np.ndarray, 
                           mask: np.ndarray) -> float:
        """Calculate RMSE from fitted plane."""
        # Get coordinates of valid points
        ys, xs = np.nonzero(mask)
        depths = depth_map[mask > 0]
        
        # Fit plane using least squares
        A = np.column_stack([xs, ys, np.ones_like(xs)])
        plane_params, _, _, _ = np.linalg.lstsq(A, depths, rcond=None)
        
        # Calculate error from plane
        fitted_depths = A @ plane_params
        rmse = np.sqrt(np.mean((depths - fitted_depths) ** 2))
        
        return rmse
        
    def create_depth_profile(self, depth_map: np.ndarray,
                           start_point: Tuple[int, int],
                           end_point: Tuple[int, int],
                           title: str = "Depth Profile") -> go.Figure:
        """
        Create line plot showing depth values along a line.
        
        Args:
            depth_map: Depth map array
            start_point: (x, y) starting point
            end_point: (x, y) ending point
            title: Plot title
            
        Returns:
            plotly.graph_objects.Figure
        """
        # Extract points along line
        num_points = 100
        x = np.linspace(start_point[0], end_point[0], num_points).astype(int)
        y = np.linspace(start_point[1], end_point[1], num_points).astype(int)
        
        # Get depth values
        depths = depth_map[y, x]
        
        # Create distance array
        distances = np.sqrt(
            (x - start_point[0])**2 + 
            (y - start_point[1])**2
        )
        
        fig = go.Figure(data=go.Scatter(
            x=distances,
            y=depths,
            mode='lines+markers',
            name='Depth Profile'
        ))
        
        fig.update_layout(
            title=title,
            xaxis_title='Distance along line (pixels)',
            yaxis_title='Depth (cm)',
            width=800,
            height=500
        )
        
        return fig
        
    def save_visualization(self, fig: go.Figure, 
                          output_path: Path,
                          filename: str) -> None:
        """Save visualization as HTML file."""
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        fig.write_html(str(output_dir / filename))
        logger.info(f"Saved visualization to {output_dir / filename}")

================
File: src/__init__.py
================
from .preprocessing import PreprocessingPipeline
from .core import DepthProcessor, ImageAligner
from .utils import CocoHandler

================
File: test/inspect_data.py
================
import sys
import os
from pathlib import Path
import numpy as np
import cv2
import json
from typing import Dict
import logging

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def inspect_raw_file(file_path: Path) -> Dict:
    """Analyze raw depth file"""
    try:
        # Try different data types
        data_types = [np.float32, np.uint16, np.uint8]
        results = {}
        
        for dtype in data_types:
            try:
                raw_data = np.fromfile(file_path, dtype=dtype)
                results[dtype.__name__] = {
                    'size': raw_data.size,
                    'min': float(raw_data.min()),
                    'max': float(raw_data.max()),
                    'possible_shapes': [
                        f"{s} x {raw_data.size // s}" 
                        for s in range(1, raw_data.size + 1) 
                        if raw_data.size % s == 0
                    ][:5]  # Show first 5 possible shapes
                }
            except Exception as e:
                logger.warning(f"Could not read as {dtype.__name__}: {str(e)}")
                continue
        
        return results
    except Exception as e:
        logger.error(f"Error analyzing raw file: {str(e)}")
        return {}

def inspect_rgb_file(file_path: Path) -> Dict:
    """Analyze RGB image file"""
    try:
        img = cv2.imread(str(file_path))
        if img is None:
            raise ValueError("Could not read image file")
            
        return {
            'shape': img.shape,
            'dtype': img.dtype,
            'min_val': float(img.min()),
            'max_val': float(img.max())
        }
    except Exception as e:
        logger.error(f"Error analyzing RGB file: {str(e)}")
        return {}

def inspect_coco_file(file_path: Path) -> Dict:
    """Analyze COCO annotation file"""
    try:
        with open(file_path, 'r') as f:
            coco_data = json.load(f)
            
        return {
            'num_images': len(coco_data.get('images', [])),
            'num_annotations': len(coco_data.get('annotations', [])),
            'categories': [cat['name'] for cat in coco_data.get('categories', [])],
            'image_shapes': [
                (img['height'], img['width']) 
                for img in coco_data.get('images', [])
            ]
        }
    except Exception as e:
        logger.error(f"Error analyzing COCO file: {str(e)}")
        return {}

def main():
    try:
        data_dir = Path("data")
        
        # Inspect first raw file
        raw_files = list((data_dir / "rgbd").glob("*.raw"))
        if raw_files:
            logger.info("\nAnalyzing RAW file:")
            raw_results = inspect_raw_file(raw_files[0])
            logger.info(f"File: {raw_files[0].name}")
            for dtype, info in raw_results.items():
                logger.info(f"\nAs {dtype}:")
                logger.info(f"Size: {info['size']}")
                logger.info(f"Range: {info['min']} to {info['max']}")
                logger.info(f"Possible dimensions: {info['possible_shapes']}")
        else:
            logger.warning("No .raw files found")
        
        # Inspect first PNG file
        png_files = list((data_dir / "segmented").glob("*.png"))
        if png_files:
            logger.info("\nAnalyzing RGB file:")
            rgb_results = inspect_rgb_file(png_files[0])
            logger.info(f"File: {png_files[0].name}")
            logger.info(f"Shape: {rgb_results.get('shape')}")
            logger.info(f"Data type: {rgb_results.get('dtype')}")
            logger.info(f"Value range: {rgb_results.get('min_val')} to {rgb_results.get('max_val')}")
        else:
            logger.warning("No .png files found")
        
        # Inspect COCO file
        coco_file = data_dir / "segmented" / "_annotations.coco.json"
        if coco_file.exists():
            logger.info("\nAnalyzing COCO file:")
            coco_results = inspect_coco_file(coco_file)
            logger.info(f"Number of images: {coco_results.get('num_images')}")
            logger.info(f"Number of annotations: {coco_results.get('num_annotations')}")
            logger.info(f"Categories: {coco_results.get('categories')}")
            logger.info(f"Image shapes: {coco_results.get('image_shapes')}")
        else:
            logger.warning("COCO annotation file not found")
            
    except Exception as e:
        logger.error(f"Error in main: {str(e)}")

if __name__ == "__main__":
    main()

================
File: test/test_preprocessing.py
================
import sys
import os
from pathlib import Path
import logging
import json

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.preprocessing import PreprocessingPipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_first_files():
    try:
        # Get first .raw file from rgbd directory
        rgbd_dir = Path("data/rgbd")
        raw_files = list(rgbd_dir.glob("*.raw"))
        if not raw_files:
            raise FileNotFoundError("No .raw files found in rgbd directory")
        raw_file = raw_files[0]
        
        # Extract frame_id from filename
        frame_id = raw_file.stem.replace("depth_frame_", "")
        
        # Get corresponding files
        segmented_dir = Path("data/segmented")
        coco_file = next(segmented_dir.glob("*_annotations.coco.json"))
        png_file = next(segmented_dir.glob(f"rgb_frame_{frame_id}.png"))
        
        logger.info(f"Found files:")
        logger.info(f"RAW: {raw_file.name}")
        logger.info(f"COCO: {coco_file.name}")
        logger.info(f"PNG: {png_file.name}")
        
        return {
            'frame_id': frame_id,
            'raw_path': str(raw_file),
            'coco_path': str(coco_file),
            'png_path': str(png_file)
        }
        
    except Exception as e:
        logger.error(f"Error finding files: {str(e)}")
        raise

def create_test_config(files):
    config = {
        "data_dir": str(Path("data")),
        "output_dir": str(Path("data/upscaled")),  # Added output_dir
        "coco_file": files['coco_path'],
        "rgbd_shape": [480, 640],
        "rgb_shape": [1080, 1920],
        "frame_ids": [files['frame_id']],
        "camera_height": 33.0,
        "plate_diameter": 25.5,
        "plate_height": 0.7,
        "raw_file": files['raw_path'],
        "rgb_file": files['png_path']
    }
    
    config_path = Path("test_config.json")
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=4)
    
    return str(config_path)
def test_preprocessing():
    try:
        files = get_first_files()
        
        config_path = create_test_config(files)
        logger.info(f"Created config file: {config_path}")
        
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        pipeline = PreprocessingPipeline(config)
        
        logger.info("Testing data loading...")
        data = pipeline.load_data(files['frame_id'])
        logger.info("Data loading successful")
        
        logger.info("Testing full preprocessing pipeline...")
        result = pipeline.process_single_image(files['frame_id'])
        logger.info("Preprocessing completed successfully")
        
        upscaled_dir = Path("data/upscaled")
        if upscaled_dir.exists():
            output_files = list(upscaled_dir.glob("*"))
            logger.info(f"Files generated in upscaled directory: {[f.name for f in output_files]}")
        
        return True
        
    except Exception as e:
        logger.error(f"Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_preprocessing()
    if success:
        print("All tests passed!")
    else:
        print("Tests failed!")

================
File: .gitignore
================
/venv
/repopack-output.txt

================
File: requirements.txt
================
numpy==2.1.3
opencv-python==4.10.0.84
pathlib==1.0.1

================
File: test_config.json
================
{
    "data_dir": "data",
    "output_dir": "data/upscaled",
    "coco_file": "data/segmented/_annotations.coco.json",
    "rgbd_shape": [
        480,
        640
    ],
    "rgb_shape": [
        1080,
        1920
    ],
    "frame_ids": [
        "20241204_233746"
    ],
    "camera_height": 33.0,
    "plate_diameter": 25.5,
    "plate_height": 0.7,
    "raw_file": "data/rgbd/depth_frame_20241204_233746.raw",
    "rgb_file": "data/segmented/rgb_frame_20241204_233746.png"
}
